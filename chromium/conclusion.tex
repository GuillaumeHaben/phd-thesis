\section{Conclusion}
\label{sec:chromium-conclusion}


In this paper, we investigated the utility of existing vocabulary-based flaky test prediction methods in the context of a continuous integration pipeline. To do so, we collected data about 23,374 flaky tests and 2,343 fault-revealing tests composing a dataset of 1.8 million test failures representing the actual development process of more than 10,000 builds corresponding to a period of 9 months. Thus, we empirically evaluated the prediction methods and found similar performance compared to previous studies in terms of precision and recall. Despite the (very) high accuracy to detect flaky test failures, we also found that 76.2\% of fault-triggering test failures were misclassified as flaky by the prediction methods, indicating major losses on the fault revelation capabilities of the test suites. Going a step further, we also showed that flaky tests have a strong ability to detect faults, with \nicefrac{1}{3} of all regression faults being revealed by tests that have experienced flaky behaviour at some point in the lifetime of the project under analysis.

These findings motivated the need for failure-focused prediction methods. To this end, we extended our analysis by checking the performance of failure-focused models (trained on failures instead of tests) and found that they result in similar accuracy and fewer false positives. We also found that considering test execution features such as the run duration and the historical flake rate was helpful to increase its ability to discern flaky failures and fault-triggering failures. However, our results still miss a large number of test failures, 42.3\%. Therefore, we believe that the current performance is not actionable and that additional research is needed in order to tackle this vastly ignored problem of flaky test failure prediction over flaky tests.


% In this paper, we conducted a case study on Chromium's CI. To this end we collected a dataset of 1.8 million test failures representing the actual development process for a period of 9 months. We then explored the importance of test flakiness in this project and discovered that it is prevalent and causes issues in all the related builds. The current process includes reruns of test failures, which is often due to 250 flaky tests. Regression faults are also frequent, as about \nicefrac{1}{4} of the builds are failing due to fault-revealing tests. Interestingly, we observed that flaky and fault-revealing tests share similarities (\eg flake rate) and in \nicefrac{1}{3} of the cases, regression faults are revealed by tests that were flaky in previous builds. When further analyzing failing builds, we realised that \nicefrac{3}{4} of them contain only those fault-revealing flaky tests. 

% % Explain why failure detection is the real challenge
% Based on these findings, we report the performance of two models that we trained using code vocabulary features and targeting two distinct tasks. The first one, aimed at replicating what the majority of the research community does, \ie. classifying tests as flaky or non-flaky. The second one, aims at classifying test failures as flaky or fault-triggering in order to account for the faults that are revealed by flaky tests. Overall, we conclude that in Chromium, flaky tests are challenging to detect, with many false positives being reported. Failures are more difficult to classify even when adding dynamic features specific to each test execution. 

 Our future research agenda aims at improving the performance of flaky test failure detection techniques by using additional features and artificial data (augmenting the training data with new positive examples to tackle the class imbalance issues). We also plan to develop failure-cause interpretations for the techniques so that they could be usable by the Chromium developers.  