\section{Introduction}
\label{sec:chromium-introduction}


% Continuous Integration pipeline
Continuous Integration (CI) is a software engineering process that allows developers to frequently merge their changes in a shared repository~\cite{CI}. To ensure a fast and efficient collaboration, the CI automates parts, if not all, of the development life cycle. Regression testing is an important aspect of CI as it ensures that new changes do not break existing functionality. Test suites are executed for every commit and test results signal whether changes should be integrated into the operational codebase or not.

% Flaky tests
Tests are an essential part of the CI as they prevent faults from enterring the codebase, and they ensure smooth code integration and overall good software function. Unfortunately, some tests, named flaky tests, exhibit a non-deterministic behaviour as they both pass and fail for the same version of the codebase. When flaky tests fail, they send false alerts to developers about the state of their applications and the integration of their changes. 

Indeed, developers spend time and effort investigating flaky failures, as they can be difficult to reproduce, only to discover that they are false alerts ~\cite{Eck2019}. These false alerts occur frequently in open-source and industrial projects~\cite{Bell2018,Kowalczyk2020,Lam2019iDFlakies,LeongSPTM19} and make developers lose not only time but also their trust in the test signal. This trust issue in turn introduces the risk of ignoring fault-triggering test failures. This way, false alerts defy the purpose of software testing and hinder the flow of the CI.

% Existing approaches to detect flaky tests 
To deal with test flakiness, many techniques aiming at detecting flaky tests have been introduced. A basic approach is to rerun tests multiple times and observe their outcomes. While to some extent effective, test reruns are extremely expensive \cite{LeongSPTM19,Bell2018} and unsafe. To this end, researchers have proposed several approaches relying on static (the test code) \cite{camara2021use,Pinto2020,fatima2021flakify,King2018, LeongSPTM19} or dynamic (test executions) \cite{Bell2018,Lam2019iDFlakies,ziftci2020flake} information (or both) \cite{FlakeFlagger} to predict whether a given test is flaky. 

Among the many flakiness prediction methods, the vocabulary-based ones~\cite{Haben2021,Pinto2020,Camara2021VocabExtendedReplication,Bertolino2020,olewickiBrown} are the most popular \cite{Parry2021}. They rely on machine learning models that predict test flakiness based on the occurrences of source code tokens of the candidate tests. Interestingly, previous research has found these approaches particularly precise, with current state-of-the-art achieving accuracy values higher than 95\% \cite{Pinto2020,fatima2021flakify,camara2021use,Camara2021VocabExtendedReplication,Haben2021}. 

At the same time, vocabulary-based approaches are static and text-based, thus, they are both portable, \ie limited to a specific language, and interpretable, \ie users may understand the cause of flakiness based on the keywords that impact the model's decisions. All these characteristics (precision, portability and interpretability) make vocabulary-based approaches appealing; they are flexible and easy to use in practice. In view of this, we decided to replicate these techniques on an industrial project (the Chromium project) and evaluated their ability to effectively support the detection of flaky tests during the CI operation cycles.

Perhaps not surprisingly, we found a similar prediction performance (99.2\% precision 98.4\% recall) to the ones reported by previous studies. Surprisingly though, we noticed numerous fault-triggering failures (approximately 76\% of all fault-triggering failures) being marked as flaky by these prediction methods. This means that, in the case where the Chromium teams were to follow the recommendations of these techniques, they would have missed at least 76\% of all regression faults (considering their respective test failures as flaky) that were captured by their test suites. 

The significantly high fault loss experienced (fault-triggering failures considered as flaky) motivated the investigation of the fault-triggering failures. To this end, we made the following three findings: 

\begin{itemize}
    \item \textit{Flaky tests have a strong fault-revealing capability}, \ie more than \nicefrac{1}{3} of all regression faults are triggered by a test that exhibits flaky behaviour at some point in time. This means that methods aiming at detecting flaky tests, inevitably flag as flaky fault-triggering test failures made by these tests. This indicates an inherent limitation of all methods focusing on identifying flaky tests, a fact that is largely ignored by previous studies.
    
    \item \textit{Many fault-revealing tests have characteristics similar to flaky ones} and thus, are mistakenly flagged as flaky by the flakiness prediction methods.
    In our data, the set of fault-triggering failures made by non-flaky tests that are mistakenly predicted as flaky represents 56.2\% of all fault-triggering failures. 

    \item \textit{The majority of the flaky tests fail frequently (87.9\% of the flaky tests have also flaked in the past)}, making prediction methods mark them as flaky based on the test history, instead of the characteristics of their failures. In particular, a dummy method that classifies as flaky any test that flaked at least once in the past achieves precision and recall values of 99.8\% and 87.8\% when predicting flaky test failures.
\end{itemize}

The above findings motivate the need for techniques focusing on flaky test failures, instead of flaky tests, \ie discriminating between fault-triggering and flaky test failures, an essential problem that has largely been ignored by previous research \cite{Parry2021}. Therefore, we adapt the vocabulary-based methods for failure-focused predictions and check their performance. Although they miss fewer fault-triggering test failures than the test-focused methods (20.3\% FPR compared to 76.2\% FPR), their ability to detect fault-triggering failures remains poor, MCC value of 0.25. 

With the hope of improving the methods' performance, we augment their feature set with dynamic features related to (flaky) test executions (\eg run duration and historical flake rate) that we find useful for flakiness diagnosis. We achieve a better --but still not acceptable performance -- MCC value of 0.42, indicating an improvement to detect fault-triggering failures when considering dynamic features. 

Overall, our study demonstrates the need for methods that can effectively predict flaky test failures (instead of flaky tests), to reduce the faults missed, and the need for adopting more thorough experimental methodologies, reflecting the needs of the domain of the actual practice (not just classification metrics), when evaluating flakiness predictions. 

In summary, the contributions of our paper are:
\begin{itemize}
    \item We present \textit{a large empirical study on flakiness prediction, based on the Chromium project} -- one of the biggest open-source industrial projects -- involving 10,000 builds, more than 200,000 unique tests and 1.8 million test failures. Our study is the first to study the suitability of applying flakiness prediction into a CI pipeline by focusing on the potential losses that they introduce: missed fault-triggering failures.  
    
    \item We provide empirical evidence that flaky test prediction methods, despite being very precise, are practically non-actionable since they \textit{flag as flaky a majority, approximately 76.2\%, of all fault-triggering failures} (56.2\% due to misclassifications of fault-revealing tests and 20\% due to correct classification of flaky tests that reveal faults).

    \item We provide empirical evidence that \textit{flaky tests have strong fault-revealing capabilities}, indicating an inherent limitation of existing methods. At the same time, our results motivate the need for failure-focused prediction techniques. Unfortunately, we also show that existing vocabulary-based methods are insufficiently precise, calling for additional research in this area.  

    \item We investigate whether \textit{imbuing the training data with additional dynamic features can enhance the failure prediction effectiveness}. These results show  improvement (MCC values are up to 0.42), but indicate that more work remains to be done to develop deployable flakiness prediction for real-world systems.
   
\end{itemize}

To support future research, we share our dataset, experimental data and related code, in a replication package.\footnote{https://github.com/serval-uni-lu/DiscerningFlakyFailures}