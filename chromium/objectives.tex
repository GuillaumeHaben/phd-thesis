\section{Objectives and Methodologies}
\label{sec:chromium-objectives}

\subsection{Research Questions}

We start our analysis by assessing the effectiveness of the existing flakiness prediction methods in our project by considering the critical cases where fault-revealing test failures are flagged as flaky by the prediction methods in various CI cycles. We thus are interested in investigating the methods' performance under realistic settings, i.e., correctly detected and missed flaky and fault-triggering test failures, when trained with past CI data and evaluated on future ones. In contrast to previous work, this analysis introduces a new dimension in the evaluation of flakiness predictions which is the investigation of what we lose when adopting a prediction method (the fault-triggering failures classified as flaky). Therefore, we ask:

\begin{description}
    \item \textbf{\textsc{RQ1:}} \emph{How well do flaky test prediction methods discern flaky test failures from fault-triggering ones?}
\end{description}

To establish realistic settings, we train the prediction models using the information available (flaky tests and non-flaky ones) at a given point in time, where we have sufficient historical data to train on. We then evaluate the models in subsequent builds with respect to flaky and fault-triggering test failures. 
We replicate the vocabulary-based methods since they are popular, easy to implement and quite effective, and aimed at learning to predict flaky tests, as done by previous studies.

After checking the prediction performance in a realistic setting (test failures), we repeat the entire process but now we train on historical test failures instead of tests. We make this adaptation with the hope of improving further our predictions and perhaps improving our understanding of the impact that such predictions may have on missed fault-triggering test failures (those marked by the models as flaky). Hence, we ask: 

\begin{description}
    \item \textbf{\textsc{RQ2:}} \emph{How well do flaky test failure prediction methods discern flaky test failures from fault-triggering ones?}
\end{description}

Finally, we wish to be comprehensive, so we also optimize and extend the prediction methods with additional features, some of which were suggested by previous studies (the flake rate~\cite{Kowalczyk2020}, the run duration~\cite{FlakeFlagger}) and some dynamic features (test run status, test run tag status, test run duration) that we found by us when experimenting with the flaky tests. Thus, we ask:

\begin{description}
    \item \textbf{\textsc{RQ3:}} \emph{Can we improve the accuracy of the flaky test failure predictions by considering dynamic test execution features?}
\end{description}


To answer RQ3, we repeat the analysis carried out for RQ2 but now we are training and optimising for the additional features that we determined during our analysis and check the performance we achieve with regards to test failures, as performed in RQ2. 


\subsection{Experimental Procedure}

\subsubsection{Selection of a flaky test detection approach} 
\label{sec:evaluationSelection}
Being a recent topic of interest, several techniques have been introduced in the scientific literature. Approaches relying on code coverage such as FlakeFlagger~\cite{FlakeFlagger} or DeFlaker~\cite{Bell2018} are challenging to implement in our case. Chromium's code base consists of several languages and code coverage is both costly and non-trivial to retrieve. Test smells~\cite{camara2021use} approaches are also difficult to extract as tests are written in many different languages and tools do not always exist. 

Having those constraints in mind, we decide to use the vocabulary-based approach introduced by Pinto~\etal ~\cite{Pinto2020}. It received significant attention with several replication studies conducted~\cite{Haben2021,Camara2021VocabExtendedReplication} and follow-up studies and its portability makes it easy to implement regardless of the languages being used.

\subsubsection{Training and validation of the existing approaches (RQ1)} 
We evaluate the ability of the vocabulary-based approach, trained to differentiate flaky from non-flaky tests and used to predict flaky test failures. To do so, we divide our dataset into a training set (containing test information about the first 8,000 builds) and a test set (containing test information about the last 2,000 builds). We train our model following the existing methodologies. The flaky set includes all tests marked as flaky in the training set. 
The non-flaky set includes all fault-revealing tests and all passing tests in the 8,000\textsuperscript{th} build (\ie the last build of the training set) minus the tests that are found as flaky in any of the builds under study (to increase the confidence of being non-flaky). The test set includes all flaky test failures and all fault-triggering failures (reported by fault-revealing tests). The test set is common in all RQs.

\subsubsection{Implementation of a failure classifier (RQ2 and RQ3)}
We select flaky failures in our dataset as all failures produced by flaky tests and fault-triggering failures are all failures produced by fault-revealing tests. There are no duplicated data in the case of test failures, as each test execution is unique. For RQ2, we train our classifier on non-flaky executions (passing and fault-revealing tests execution) and flaky failures. In RQ3, we report the performance of a model using execution features (run duration and flake rate).

\subsubsection{Time-sensitive evaluation}
We split our data in two parts: the first 80\% builds are selected as a training set and the last 20\% as a holdout set. By doing so, we respect the evolution of failures across time and avoid any data leakage that could occur by randomly selecting data. This time-sensitive aspect is very important to consider. We found that not taking this condition into account and training a model on a shuffled dataset would greatly overestimate the performance. Figure~\ref{fig:dataset} shows a representation of our dataset. Flaky tests are present in all builds and Fault-revealing tests are occasional: they happen in \nicefrac{1}{4} of builds (See Section~\ref{sec:chromium-discussion}). To mitigate imbalance, we collected all passing tests for 1 build: $b_{8,000}$ and use them in our set of non-flaky tests, for training.

% Figure dataset
\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{figures/chromium/dataset.png}
\caption{The data collected from Chromium's CI consists of flaky, fault-revealing and passing tests spread across 10,000 builds. The build timeline ranges from build \textit{$b_0$} to \textit{$b_{10,000}$} and depicts the distribution of the collected tests: flaky tests are spread across all builds and fault-revealing tests happen occasionally. Due to a large number of passing tests, we collected them from the \textit{$b_{8,000}$} build (\ie at the end of our training set).}
\label{fig:dataset}
\end{figure*}

\subsubsection{Classifier selection and pipeline description}
% Model 
We use a random forest classifier to perform the predictions. Unfortunately, our dataset is imbalanced with the minority class being 1\% of the data. Using a simple random forest would greatly increase the chance of having few or no elements from our minority class in the different trees, making the overall model poor in predicting the class of interest. To alleviate this issue, we decide to use a Balanced Random Forest classifier\cite{chen2004using} to facilitate the learning. This implementation artificially modifies the class distribution in each tree so that they are equally represented. Furthermore, we use SMOTE in the training phase to augment data for the minority class~\cite{smote}.

% Test source representation
To represent the tests, we use \textit{CountVectorizer} to convert texts as a matrix of token counts. This technique, known as bag-of-words, is used in previous vocabulary-based approaches~\cite{Pinto2020,Haben2021,Camara2021VocabExtendedReplication,Bertolino2020,olewickiBrown}. These vectors initially contain as many features as the words appearing in source code of the tests. As the generated dictionary can become big (in terms of size) we need to use feature selection to reduce it, remove irrelevant features (reducing noise in the data) and select the most informative features. Feature selection is thus, helping to reduce the model training time and to improve the overall performance and interpretability of the model. 

We use SelectKBest\cite{selectkbest} which retains the $k$ highest score features based on the univariate statistical test $\chi^2$.
Hyper-parameters of the machine learning pipeline, \ie the number of trees in the forests, the sampling strategy for SMOTE and the number of features to be retained are tuned using a grid search approach and cross-validation in the training set. Once optimized, we retrain a model fitted on the whole training set and evaluate it on the holdout set.

In addition to the precision, recall and MCC metrics, we also report the false positive rate (FPR), that is in our case, the ratio of fault-triggering failures misclassified as flaky over all fault-triggering failures. It is defined by:
    \[
    \textbf{FPR} = \frac{FP}{FP+TN}
    \]