\chapter{Conclusion \& Future Work}
\label{chap:conclusion}

\setcounter{minitocdepth}{1}
\justifying
\textit{
This chapter presents the overall conclusion of the dissertation and proposes potential research directions. 
}

\chapterPage{}

\section{Summary of Contributions}

 This dissertation aimed at addressing test flakiness, one of the major challenges of modern software testing. It consists of five main scientific contributions: two exploratory studies giving more insights into flakiness in practice, two constructive studies presenting different approaches suggesting new ways of tackling flakiness and a case study evaluating the usefulness of existing prediction techniques in an industrial context. More precisely:\\

 We first performed a grey literature review and interviewed 14 practitioners in order to have a better understanding of the challenges linked with flakiness in the industry. We explore three aspects: the sources of flakiness within the testing ecosystem, the impacts of flakiness and the measures adopted when addressing flakiness. Our analysis showed that, besides the tests and code, flakiness stems from interactions between the system components, the testing infrastructure, and external factors. We also highlighted the impact of flakiness on testing practices and product quality and showed that the adoption of guidelines together with a stable infrastructure are key measures in mitigating the problem. Furthermore, we also identified automation opportunities enabling future research works.\\
    
 While detecting flaky tests using supervised learning is possible, to reach industrial adoption and practice, existing techniques need to be replicated and evaluated extensively on multiple datasets, occasions and settings. In the second contribution, we perform a replication study of a recently proposed method that predicts flaky tests based on their code vocabulary. We thus replicate the original study on three different dimensions. First, we replicate the approach on the same subjects as in the original study but using a different evaluation methodology, \ie we adopt a time-sensitive selection of training and test sets to better reflect the envisioned use case. Second, we consolidate the findings of the original study by checking the generalisability of the results for a different programming language. Finally, we propose an extension to the original approach by experimenting with different features extracted from the code under test.\\
    
 Existing flakiness detection approaches mainly focus on classifying tests as flaky or not and, even when high performances are reported, it remains challenging to understand the cause of flakiness. This part is crucial for researchers and developers that aim to fix it. To help with the comprehension of a given flaky test, the third contribution introduces FlakyCat, the first approach to classify flaky tests based on their root cause category. FlakyCat relies on CodeBERT for code representation and leverages Siamese networks to train a multi-class classifier. We train and evaluate FlakyCat on a set of 451 flaky tests collected from open-source Java projects. Our evaluation shows that FlakyCat categorises flaky tests accurately, with an F1 score of 73\%. We also investigate the performance of FlakyCat for each category. In addition, to facilitate the comprehension of FlakyCat's prediction, we present a new technique for CodeBERT-based model interpretability that highlights code statements influencing the categorization.\\
    
 To mitigate the effects of flakiness, both researchers and industrial experts proposed strategies and tools to detect and isolate flaky tests. However, flaky tests are rarely fixed as developers struggle to localise and understand their causes. Additionally, developers working with large codebases often need to know the sources of nondeterminism to preserve code quality, \ie avoid introducing technical debt linked with non-deterministic behaviour, and avoid introducing new flaky tests. To aid with these tasks, we propose with the fourth contribution re-targeting Fault Localisation techniques to the flaky component localisation problem, \ie pinpointing program classes that cause the non-deterministic behaviour of flaky tests. In particular, we employ Spectrum-Based Fault Localisation (SBFL), a coverage-based fault localisation technique commonly adopted for its simplicity and effectiveness. We also utilise other data sources, such as change history and static code metrics, to further improve the localisation. Our results show that augmenting SBFL with change and code metrics ranks flaky classes in the top-1 and top-5 suggestions, in 26\% and 47\% of the cases. Overall, we successfully reduced the average number of classes inspected to locate the first flaky class to 19\% of the total number of classes covered by flaky tests. Our results also show that localisation methods are effective in major flakiness categories, such as concurrency and asynchronous waits, indicating their general ability to identify flaky components.\\
    
 While promising, the actual utility of the methods predicting flaky tests remains unclear since they have not been evaluated within a continuous integration (CI) process. In particular, it remains unclear what is the impact of missed faults, \ie the consideration of fault-triggering test failures as flaky, at different CI cycles. In the last contribution, we apply state-of-the-art flakiness prediction methods at the Chromium CI and check their performance. Perhaps surprisingly, we find that the application of such methods leads to numerous faults missed, which is approximately \nicefrac{3}{4} of all regression faults. To explain this result, we analyse the fault-triggering failures and find that flaky tests have a strong fault-revealing capability, \ie they reveal more than \nicefrac{1}{3} of all regression faults, indicating inevitable mistakes of methods that focus on identifying flaky tests, instead of flaky test failures. We also find that 56.2\% of fault-triggering failures, made by non-flaky tests, are misclassified as flaky. To deal with these issues, we build failure-focused prediction methods and optimize them by considering new features. Interestingly, we find that these methods perform better than the test-focused ones, with an MCC increasing from 0.20 to 0.42. Overall, our findings suggest that future research should focus on predicting flaky test failures instead of flaky tests (to reduce missed faults) and reveal the need for adopting more thorough experimental methodologies when evaluating flakiness prediction methods (to better reflect the actual practice).


\section{Perspectives}

In the following, we discuss potential future research that follows the contributions and ideas presented in this dissertation:

\begin{itemize}
    \item \textbf{Failure Prediction:} As we saw in Chapter 8, one of the real challenges is to detect flaky from relevant test failures, \ie knowing when the developers should investigate issues. While promising, an approach using a limited set of features does not enable efficient performance to reach industrial adoption. Future research should focus on that problem and investigate the use of other features that can be linked with flakiness such as the time of the day tests were executed or the parameters, state and load of the machine running the tests. Many studies in software testing rely on the history of each test to draw conclusions~\cite{Wen:2019:tse,Azizi2018}. While we used the flake rate as a feature in our approach, we could think of more elaborative ways to leverage it as done in other studies~\cite{gruber2023practical,Kowalczyk2020}.
    
    \item \textbf{Machine Learning Interpretability:} Interpretability and explainability of machine learning models are one of the main challenges in AI at the moment. This often restrains the usage of such approaches by practitioners as reliability is often a prerequisite to performance. Nowadays, more and more tools leverage large language models to attempt to solve software engineering problems. However, it would be interesting to investigate, in parallel, techniques to interpret model outcomes and increase the reliability of their usage. Some approaches already exist, like SHAP~\cite{shaponline} or Lime~\cite{ribeiro2016should} but are unsuitable for large language models. As we saw in Chapter 6, this can be useful for developers to know more about the reasons behind the flakiness of a test to debug it and the lack of ground truth opens doors for future research. 
\end{itemize}

% Practitioner's study on the usage of ICSME and AST
