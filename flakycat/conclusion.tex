\section{Conclusions}
\label{sec:flakycat-conclusion}

Test flakiness is considered as a major issue in software testing as it disrupts CI pipelines and breaks trust in regression testing.

Detecting flaky tests is resourceful, as it can require many reruns to reproduce failures. To facilitate the detection, more and more studies suggest static and dynamic approaches to predict if a test is flaky or not.
However, detecting flaky tests constitutes only a part of the challenge, since it remains difficult for developers to understand the root causes of flakiness. Such understanding is vital for addressing the problem, \ie fixing the cause of flakiness. At the same time, researchers would gain more insights based on this information. So far, only a few automated fixing approaches were suggested and these are focusing on one category of flakiness. Knowing the category of flakiness for a given flaky test is thus a piece of key information. 

With our work, we propose a new approach to this problem that aims at classifying previously identified flaky tests into their corresponding category. We propose FlakyCat, a Siamese network-based multi-class classifier that relies on CodeBERT's code representation. FlakyCat addresses the problem of data scarcity in the field of flakiness by leveraging the Few-Shot Learning capabilities of Siamese networks to allow the learning of flakiness categories from small sets of flaky tests. As part of our evaluation of FlakyCat, we collect and make available a dataset of 451 flaky tests with information about their flakiness categories. 

Our empirical evaluation shows that FlakyCat performs the best compared to other code representations and traditional classification models used by previous flakiness prediction studies. In particular, we reach an F1 score of 73\%. We also analyzed the performances with respect to each category of flakiness, showing that flaky tests belonging to \textit{Async waits}, \textit{Test order dependency}, \textit{Unordered collections}, and \textit{Time} are the easiest to classify, whereas flaky tests from the \textit{Concurrency} category are more challenging. Finally, we present a new technique to explain CodeBERT-based machine learning models. This technique helps in explaining what code elements are learnt by models and give more information to developers who wish to understand flakiness's root causes. 