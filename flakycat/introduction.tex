\section{Introduction}
\label{sec:flakycat-introduction}

Continuous Integration (CI) plays a key role in nowadays software development life cycle~\cite{shahin2017continuous, CI}. 
CI ensures the quick application of changes to a main code base by automatically running a variety of tasks. Those changes are responsible for building the program and its dependencies, performing checks (\eg static analysis), and running test suites to maintain code integrity and correctness. An important assumption for practitioners is that tasks are deterministic, \ie regardless of the execution's context of a same task, results need to remain similar. 

Unfortunately, in practice, this is not always the case. Previous research has identified test flakiness as one of the main issues in the application of automated software testing~\cite{Micco2017,LeongSPTM19, memon2017taming}. A flaky test is a test that passes and fails when executed on the same version of a program. Flakiness hinders CI cycles and prevents automatic builds due to false signals, resulting in undesirable delays. Furthermore, surveys~\cite{Habchi2022Qualitative, Eck2019, gruber2022survey} show that flakiness affects developers' productivity, as they spend a considerable time and effort investigating the nature and causes of flaky tests. 

To alleviate this issue, researchers have proposed tools that help detect flaky tests. In particular, IDFlakies~\cite{Lam2019iDFlakies} and Shaker~\cite{Silva2020} detect flakiness in test suites by running tests in different setups. However, rerunning tests, especially for a large number of times, is resource-intensive and might not be a scalable solution. For this reason, researchers recently suggested alternative approaches to detect flaky tests based on features that do not require any test execution \cite{LeongSPTM19,King2018,Pinto2020,camara2021use}. Although promising, these approaches mainly focus on classifying tests as flaky or not without any additional explanation. Unfortunately, the absence of additional information prevents a proper comprehension of flaky failure causes. Hence, further investigation is required to understand the nature of flakiness and identify the culprit code elements that need to be fixed~\cite{Eck2019}.

Another important line of research in the area regards automated approaches that aim at helping to locate the root causes and suggest potential flakiness fixes~\cite{ziftci2020flake,Lam2019RootCausing,flakyloc}. However, research on automatically fixing flakiness is still at an early stage: tools often focus on one category of flakiness and with few examples. For instance, iFixFlakies~\cite{Shi2019iFix} and ODRepair~\cite{li2022repairing} focus only on dealing with test order dependencies, which is one of the main causes of test flakiness. Flex~\cite{Dutta2020} automatically fixes flakiness due to algorithmic randomness in machine learning algorithms. 

We believe that both developers and researchers would benefit from additional information that could assist them in gaining a better understanding of flaky tests, once they have been detected. Therefore, we propose FlakyCat, a learning-based flakiness categorization approach that identifies the key reason/category of the test failures. 

One limitation of previous work, relying on supervised learning, regards the need for large volumes of available data. Unfortunately, debugged flaky test data is scarce, inhibiting the application of learning-based methods. To deal with this issue, we leverage the Few-Shot learning capabilities of Siamese networks, which we combine with the CodeBERT representations to learn flakiness categories from a limited set of data (flaky tests). 

To evaluate FlakyCat, we gather a set of 451 flaky tests annotated with their category of flakiness issued from previous studies and projects that we mined from GitHub.

Our empirical evaluation aims at answering the following research questions:

\begin{itemize}[label={}]
    \item \textsc{\textbf{RQ1:}} \emph{How effective is FlakyCat compared to approaches based on other combinations of test representation and classifier?} \\
    \textbf{Findings:} Our results show that FlakyCat is capable of predicting flakiness categories with an F1 score of 73\%, outperforming classifiers based on traditional supervised machine learning.

    \item \textsc{\textbf{RQ2:}} \emph{How effective is FlakyCat at predicting each of the considered flakiness categories?}\\
    
    \textbf{Findings:} FlakyCat classifies accurately flaky tests related to \textit{Async waits}, \textit{Test order dependency}, \textit{Unordered collection}, and \textit{Time}, with the best F1 score of 81\% for the \textit{Async waits} category. However, the approach shows difficulty in classifying concurrency-related flaky tests (an F1 score of 39\%), since these cases are related to the interaction of threads and processes and are easily confused with Asynchronous waits. 

    \item \textsc{\textbf{RQ3:}} \emph{How do statements of the test code influence the predictions of FlakyCat?}\\
    \textbf{Findings:} We found that some statement types are specific to certain flakiness categories. This is the case for assert statements in \textit{Unordered collections} and statements using date or time for the \textit{Time} category. We also found that some flaky categories have similar statement types like the presence of thread usages in both \textit{Async waits} and \textit{Concurrency} categories.
\end{itemize}

In summary, our contributions can be summarized as follows:
 \begin{itemize}
    \item \textbf{Dataset} We collected 451 flaky tests alongside their categories. 
     \item \textbf{Model} We present FlakyCat, a new approach using Few-Shot Learning and CodeBERT to classify flaky tests based on their flakiness category. 
     \item \textbf{Interpretability} We introduce a novel technique to explain what information is learnt by models using CodeBERT as code representation.
 \end{itemize}

To enable the reproducibility of our work, we make the dataset used to evaluate FlakyCat and the scripts publicly available in our replication package \footnote{https://github.com/serval-uni-lu/FlakyCat}.

The paper is organized as followed: Section~\ref{sec:flakycat-flakycat} presents the designed implementation of FlakyCat. Section~\ref{sec:flakycat-interpretability} introduces our interpretability technique. Section~\ref{sec:flakycat-evaluation} describes how we collected our dataset and evaluated our study. Section~\ref{sec:flakycat-resuts} presents the results of our study. We further discuss different use cases in Section~\ref{sec:flakycat-discussion}. Finally, Section~\ref{sec:flakycat-conclusion} discusses threats to the validity of this study.