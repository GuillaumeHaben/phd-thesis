\chapter{Introduction}

Today, companies need to produce software faster than ever in order to remain competitive. To do so, they rely mainly on two different strategies: the integration of continuous deployment pipeline and the establishment of agile methodologies within the company. These strategies allow for software producers to increase the release rate while maintaining a high quality for the software that is shipped.

One of the keystones of the quality process is based on the adoption of strategies to assess that both functional and technical requirements. Because of the complex dynamic runtime of most modern applications, static analysis techniques faces limitations in revealing violation of the requirements. Thus, most teams heavily rely on dynamic analysis to reveal fault in their software. 

In this work, we define dynamic analysis or testing as a technical investigation to assess the quality of a system under test (SUT) by executing the latter against a set of input values. Tests can further be classified through the scope they target into three categories: unit, integration and system tests. Unit testing cover the tests that are testing a unit in isolation, be it a class, a function or any other software component,  to validate each unit individually. In integration testing, related modules are grouped together and tested as a group. Finally, system testing covers the test evaluating the system as a whole to evaluate its compliance to its requirements.

Because unit and integration tests target more restricted portions of the application, they offer better control to the tester who can isolate functionalities from their environment and tests specific behaviors of sub-parts of the system. On the other hand, system tests and especially test interacting with the user interface, System User Interactive Tests (SUITs), target the system as whole usually as a black box, thus, are sensitive to any change in the SUT and its interface. This property makes system tests more fragile, \ie\ they may fail due to non functional changes that are not related to the behavior that is being tested.

In this work, we analyze the evolution of SUITs to shed light on the reasons why these tests require costly maintenance. Then, we propose some solutions as how to create more robust test suites, \ie, test suites that are less likely to break following non-functional evolution of the SUT thus exhibiting a more stable behavior.

\section{GUI Testing}
\label{sec:introduction-gui-testing}

User Interfaces are designed to provide a way for the user to interact with an application. Unfortunately, as interfaces become more user-friendly, the underlying technology becomes more complex \cite{Myers1994}. Indeed, as depicted by \textcite{Myers1995}, user interfaces require the system to  interact with complex graphical components, to provide multiple ways to provide the same command, to manage multiple asynchronous input devices through the use of asynchronous event loops or to provide an interactive computer programming environment (\eg\ read–evaluate–print loop). To ease development, different architectural designs are proposed in the literature to decouple the code responsible for the user interaction to the rest of the system such as the Entity-Component-System model \cite{Raffaillac2018}.

Nevertheless, while these architectural solutions offer advantages by providing better isolation when testing individual components, in the case of system testing, the challenges remain open. Indeed, as the complexity of user interfaces and their associated code continues to grow, testing through the user interface for functional correctness remains challenging, but vital to help ensure the safety, robustness and usability of the SUT.

Furthermore, today, Graphical User Interface (GUI) are becoming ubiquitous as a mean for users to interact with a software \cite{Myers1992, Myers1995, Brooks2009, Memon2010}. In this work, we define a GUI as a hierarchical, graphical front-end to a software system that accepts as input user-generated and system-generated events from a fixed set of events and produces deterministic graphical output \cite{Memon2007} and alters the state of the software \cite{Nguyen2014}. A GUI-based application is a software for which the GUI is the main mode of interaction to the detriment of other types of user interfaces. As a consequence of the predominance of GUI-based applications, a lot of effort has been geared towards GUI automation and specifically mobile GUI-based application \cite{Machiry2013, Gomez2013}. However, as we show later in this section, major challenges remain open.

Yet, the question of correctness of a GUI-based application is of utmost importance since prototyping and developing the behavior of a system is harder than prototyping and developing the appearance of the application \cite{Myers2008}. This is in this context that we introduce GUI testing which encompasses the techniques used to test user interface layout and its behavior. More precisely, GUI testing can be defined as ``the process of testing a software application with a graphical front-end to guarantee that it meets its specification by performing a sequence of events against the GUI elements'' \cite{Cunha2010, Banerjee2013, Issa2012}. We define a SUIT as an automated GUI test that exercises the SUT in a black box fashion.

Thus, strategies have been developed to exercise the user interface to ensure that functional requirements are satisfied. Different techniques have been proposed but all rely on the same principle. A test framework executes events belonging to GUI components and monitors the resulting changes to the program state \cite{Nguyen2014}. In other words, each technique generates a sequence of actions that will be executed against the SUT (input) and captures some indication as what is the current state of the system (output). Consequently, a natural classification discriminate each strategy on the way the sequences of input are generated.

In the remainder of this section, we discuss current techniques used to test software systems through their user interfaces. The techniques are ordered in term of their theoretical potential for automation during the test generation process.

\subsection{Random GUI Testing}
\label{sec:introduction-random-gui-testing}

In random GUI testing, also known as monkey testing or random-walk, tests are automatically created by generating sequences of events at random in the hope of exploring the SUT to reveal failures. Because no requirements are passed as input such techniques rely on specific constraints that are application or domain invariant oracles \cite{Mesbah2009} to assess for correctness. Thus, any test case which causes the SUT to violate an invariant is considered as a fault revealing test case \cite{Barr2015}. These properties make random GUI testing cheap to run on large number of version/configuration of applications and effective at finding crashes.

Typically such tools rely on the concept of fuzz testing which inject arbitrary or contextual events to the SUT. Tools like Monkey \cite{Google2020},  ATUSA \cite{Mesbah2012} or Dynodroid \cite{Machiry2013} rely on pseudo random input events which include both user interface event and system events and assess the generated sequences against invariant oracles. 

One of the major challenges encountered when relying on random GUI testing resides in the exploration of the application states. Indeed, with modern applications, the infinite number of combination of user inputs leads to potentially large and sometimes infinite number of reachable states. Consequently, the search space cannot be covered exhaustively leading to potentially low state coverage \cite{Canny2019}. Furthermore, in the case of more complex input such as a chain of character, randomly generated input have often low chances of producing feasible test cases, thus requiring a large number of input generations to find interesting test cases (\eg if a login page exists, only a combination of a valid user name and password would allow the test case to continue its exploration behind the login wall).

To alleviate these limitations, different approaches emerged introducing algorithms to better explore the application state space and consequently generate more fault revealing test cases. For example EvoDroid \cite{Mahmood2014} and Sapienz \cite{Mao2016} propose to use search-based algorithms to explore the space of possible sequences and generate tests for Android applications. The goals of the algorithm being to generate short sequences of input while improving coverage (code and state coverage) to produce fault revealing sequences. More recently, tools such as DIG \cite{Biagiola2019} try to increase the coverage problem by specifically targeting the input generation problem. While these approaches look promising, today, monkey testing typically requires to provide typical input (\eg login and passwords) to allow the algorithms to explore more states.

To conclude this introduction to random GUI testing, we see that it offers a cheap alternative to classical test scripting which can be useful to detect crashes and other invariant. Nevertheless, because of the lack of knowledge of functional requirement and life cycle, exploring the application and assessing more fine-grain behavior might be challenging. Thus, while companies do use random GUI testing extensively, they use it in addition to other techniques that we present below.

\subsection{Model-Based Testing}
\label{sec:introduction-model-based-testing}

Model-Based Testing (MBT) is an approach encompassing the processes and techniques for the automatic derivation of abstract test cases from abstract models \cite{Utting2012}. Thus, functional specifications of a systems, with the assumption that they are precise enough, are used as input to design process models which in turn are used to generate automated test cases \cite{Gupta2011}. This process can be devised in the five below steps \cite{Utting2012}:

\begin{enumerate}
    \item A model of the SUT is build from informal requirements or existing specification documents;
    \item Selection criteria are chosen, to guide the automatic test generation;
    \item Test selection criteria are transformed into test case specifications;
    \item A set of test cases is generated with the aim of satisfying all the test case specifications;
    \item Finally, the test cases are executed against the SUT.
\end{enumerate}

Unfortunately, most of current GUI-based application are not derived from a model, thus, cannot rely on a \emph{a priori} model encompassing all the expected behaviors of the application. Besides, even though advances have been made in Model-Based Software Engineering, complex GUI applications exhibit behaviors that cannot be expressed by models found in GUI testing. This limitation causes MBT to offer limited applicability which explains why practitioners shy away from the model-based solution proposed in the scientific literature.

To tackle this shortcoming, in recent literature, researchers offer method to automatically derive a model of the application using techniques similar as the ones presented in Section~\ref{sec:introduction-random-gui-testing}. For example, \textcite{Memon2007} introduces an event-flow model to formally represent event and event interactions in a GUI-based application. However, because of the large space of events and interactions present in a given application, manually building the model is prohibitive. Thus, the author proposes to rely on reverse-engineering to automatically derive the models by automatically exploring applications (in a similar fashion as random GUI testing). Unfortunately, models built that way do not guarantee completeness anymore, leading to shortcoming such as the difficulty to assess whether or not the coverage offered by the model is sufficient or the choice of inputs during exploration. While recent literature offer partial solutions such as improving the input generation \cite{Biagiola2019}, new ways to derive models from the GUI and its requirements \cite{Canny2020} the question of automatic exploration of modern GUI-based application remains an open problem.

\subsection{Record \& Replay}
\label{sec:introduction-record-and-replay}

This leads us to the third category of GUI testing, Record \& Replay which generates test cases from sequences of user interactions provided by the tester and not by exploring the model in a random fashion (Random GUI Testing) or by relying on a model (Model-Based Testing). As its name suggests, Record \& Replay works in two phases: a record phase and a replay phase:

\begin{enumerate}
    \item During the record phase, the tester manually interacts with the application, thereby generating events on the SUT. The tool records the interactions and a part of the SUT response state as specified by the tester.
    \item During the replay phase, the recorded test cases can be replayed on subsequent versions of the SUT and the captured state of the application are used as test oracles.
\end{enumerate}

Thus, the generated test cases require human intervention during the capture phase and can be rerun, reducing the overall effort during regression testing. Furthermore, because the test cases are generated by the framework, no particular skills are needed from the tester. \textcite{DiMartino2021} show that even when human testers have limited information about the system, they achieve better coverage metrics than automatic test generation techniques when using Record \& Replay.

Despite its advantages when generating test cases, Record \& Replay tend to be brittle \cite{Hammoudi2016}. Indeed, whenever the application naturally evolves, the generated test cases might break, requiring testers to regenerate the test. This limitation can be exacerbated by agile and continuous delivery practices where applications and their user interfaces is under constant evolution.

To address this drawback, the research community proposes two ways to tackle the problem of brittleness. On the one hand, some attempts have been geared towards generating more robust test cases \cite{Hammoudi2016b}. On the other hand, other researchers focus their attention on the generation of more robust tests \cite{Ronsse1999}.

Therefore, we see that while generation of test cases using Capture \& Replay is efficient, the approach suffers from drawbacks in terms of maintainability of the generated test suite. This issue is all the more relevant as the generated tests are often obscure, lacking any type of internal hierarchy, thus, compromising any attempt to manually fix such tests. 

\subsection{Test Scripting}
\label{sec:introduction-test-scripting}

Finally, the last technique covered in the section covers tests that are manually generated by testers themselves. Following the general definition, a test script can be define as a piece of executable code that execute a test case against the SUT and generates a verdict. In this work, we focus on test scripts interacting with the SUT through their GUI, namely, SUITs. Thus, we include in this category software testing techniques that aims at separating test design from the technical implementation of the tests.

This type of test scripts are typically used by teams performing acceptance testing. Acceptance tests ensure that a specific acceptance criteria, which can be functional or non-functional is met \cite{Pandit2015}. Conforming to the acceptance criteria both verifies that the SUT delivers the business value expected by the customer and guards against regressions or defects that break preexisting functions of the SUT \cite{Humble2010}. Hence, acceptance tests are not concerned with the internal implementation of the SUT, but with its overall behavior. Additionally, because such test are business facing, they are involved in the discussion between testers, developers and business analyst and as such should be readable by all stakeholders.

Thus, to ensure better maintainability of such these tests, practitioners propose design pattern to separate different concerns into different abstraction layers. For example, \textcite{Humble2010} propose the three following layers adopted by open source and commercial tools like Cucumber, JBehave, Finess, or TestComplete:

\begin{itemize}
    \item \textbf{Acceptance Criteria.} It contains functional behavior, quality characteristics, scenarios or business rules that the test is targeting. These are usually written in form close to natural language. The goal of this layer is to express for all the stakeholders the requirements that is being automatically executed.
    
    \item \textbf{Test Implementation Layer.} It contains all the underlying implementation of the test called by the acceptance criteria layer. Because test implementations that refer directly to the application GUI elements are brittle, it is not uncommon to large portions of acceptance test suites break when a single GUI element changes. While deferring the interactions with the application to the application driver layer does not make tests more robust to changes, it will make their maintenance easier with a better separation of concerns.
    
    \item \textbf{Application Driver Layer.} It is the layer that understands how to interact with the SUT. While the two other layers use only the domain language of the business, the application driver layer is expressed in the domain language used by the drivers that communicate with the SUT. One of the consequences of a well-designed application driver layer is to improved test reliability. Moreover, because of the high degree of reuse that is implicit when relying on an application driver layer, complex interactions and operations can be written once and used in many tests.

\end{itemize}

One approach promoting this architecture and commonly used in industry and facilitate all the previous points is Keyword-Driven testing (KDT). Indeed, KDT advocates that a separation of concerns allows tests to be written easier, to create more maintainable tests and enables experts from different fields and backgrounds to work together, at different levels of abstraction, for the creation of the tests.

\begin{figure}
\caption{Example of Robot Framework test}
\label{fig:robot-script}
\begin{minipage}{\linewidth}
\begin{lstlisting}[]
    <@\textcolor{block}{*** Settings ***}@>
    Test Teardown     Close All Browsers
    Library           Selenium2Library
    
    <@\textcolor{block}{*** Test Cases ***}@>
    <@\textcolor{keyword}{A user logs in with his username and password}@>
        Given browser is opened to login page
        When user "demo" logs in with password "mode"
        Then welcome page should be open
    
    <@\textcolor{block}{*** Keywords ***}@>
    <@\textcolor{keyword}{Browser is opened to login page}@>
        Open browser to login page
    
    <@\textcolor{keyword}{User "}@><@\textcolor{variable}{\$\{username\}}@><@\textcolor{keyword}{" logs in with password "}@><@\textcolor{variable}{\$\{password\}}@><@\textcolor{keyword}{"}@>
        Input Text    username_id    <@\textcolor{variable}{\$\{username\}}@>
        Input Text    password_id    <@\textcolor{variable}{\$\{password\}}@>
        Click Button    validate_id
    
    <@\textcolor{keyword}{Open Browser To Login Page}@>
        Open Browser    <@\textcolor{variable}{\$\{LOGIN URL\}}@>    <@\textcolor{variable}{\$\{BROWSER\}}@>
        Maximize Browser Window
        Set Selenium Speed    0
        Login Page Should Be Open        
    
    <@\textcolor{keyword}{Go To Login Page}@>
        Go To    <@\textcolor{variable}{\$\{LOGIN URL\}}@>
        Login Page Should Be Open
        
    <@\textcolor{keyword}{Welcome Page Should Be Open}@>
        Title Should Be    Welcome Page
        
    <@\textcolor{keyword}{Login Page Should Be Open}@>
        Title Should Be    Login Page
    
    <@\textcolor{block}{*** Variables ***}@>
        <@\textcolor{variable}{\$\{SERVER\}}@>           localhost:7272
        <@\textcolor{variable}{\$\{BROWSER\}}@>          Chrome        
        <@\textcolor{variable}{\$\{LOGIN URL\}}@>        http://<@\textcolor{variable}{\$\{SERVER\}}@>
\end{lstlisting}
\end{minipage}
\end{figure}

To do so, KDT\cite{Tang2008, Hametner2012} aims at separating test design from technical implementation. Its goal is to limit the exposure to unnecessary details and avoiding duplication. Figure~\ref{fig:robot-script} shows an example of a KDT test. This test, named ``Valid Login'' (line 5, adopted from the official documentation of Robot Framework), is responsible for validating the correct behaviour of the login form in an imaginary SUT. Lines 6--8 present the ``steps'' of the tests and, in KDT parlance, they are calls to \emph{keywords}. In turn, these keywords are defined in the respective definition blocks between lines 10 and 28. Each keyword is itself decomposed in a series of steps. Keywords can have \emph{arguments}. For instance, keyword ``Open browser'' (line 12) takes two arguments, ``\$\{LOGIN URL\}'' and ``\$\{BROWSER\}''. The use of arguments to call keywords allows to further extend the reuse of keywords.

As can be seen from the figure, most part of this fully automated test is written in plain English. This enables the unobstructed collaboration in the creation of the tests between different experts. For instance, a business analyst can write the high-level part of the test (lines 4--8) and an automation expert can implement the remaining part of the test (lines 10-35), adding the technical details to automate the steps.

While manually generated test scripts present advantages in term of communication, compared to the other techniques presented, they present a major drawback: they are expensive to create and present also a high maintenance cost as the system they are testing naturally evolve, especially when good practices are not respected.

\section{Challenges of System User Interactive Tests}

As depicted in Section~\ref{sec:introduction-gui-testing} while different GUI testing exist, they all come with advantages and limitations. When addressing the case of fully automated techniques such as random GUI testing and to a certain extend model-based testing, exploring the GUI space remains an open challenge. Thus, practitioners while relying to a certain extend to these approaches, tend to complete the test suites with manual test generation.

Unfortunately, manual test generation comes with its challenges as well. Indeed, because SUITs by design are decoupled from the application they are testing, they tend to be more fragile, \ie breaking following non-functional changes resulting from the natural evolution of the SUT. Thus, test cases fails or requires maintenance where the specific functionalities it tests remain unchanged \cite{Coppola2019, DiMartino2021}.

Better understanding why GUI test break and which invariant exist in the SUT on to which they rely is a crucial step to understand how to limit GUI test case fragility.

\section{Overview of the Contribution and Organization of the Dissertation}

This section presents the contributions of this dissertation to address the aforementioned challenges encountered when writing GUI tests as well as the organisation of this dissertation.

\subsection{Contributions}

Following are the contribution of this dissertation:

\begin{itemize}
    \item \textbf{An empirical study on the evolution of Keyword-Driven Tests in an industrial context (Chapter~\ref{chap:evolution-system-user-interactive-test}).} We conduct an empirical study on the evolution of Keyword-Driven test suites. Our aim is to demonstrate the problem of test maintenance, identify the benefits of Keyword-Driven Testing and overall improve the understanding of test code evolution (at the acceptance testing level). This information will support the development of automatic techniques, such as test refactoring and repair, and will motivate future research. To this end, we identify, collect and analyze test code changes across the evolution of industrial KDT test suites for a period of eight months. We show that the problem of test maintenance is largely due to test fragility (most commonly-performed changes are due to locator and synchronization issues) and test clones (over 30\% of keywords are duplicated). We also show that the better test design of KDT test suites has the potential for drastically reducing (approximately 70\%) the number of test code changes required to support software evolution. To further validate our results, we interview testers from BGL BNP Paribas and report their perceptions on the advantages and challenges of keyword-driven testing. 
    
    \item \textbf{An empirical study on the diffusion and refactoring of test smells in Keyword-Driven Test (Chapter~\ref{chap:smells-system-user-interactive-test}).} We conduct an empirical study on the diffusion and refactoring action observed in Keyword-Driven test suites. Following our analysis on test maintenance, we focus on bad testing scripting practices, also known as SUIT smells and their refactoring. To this end, we collect and analyze SUIT smells and their refactoring across different version of a large industrial project and 12 open-source projects. We show that the same type of smells tend to appear in industrial and open-source projects, but the symptoms are not addressed in the same way. Some smells tend to appear in most tests with a diffusion of up than 90\% of the tests for some specific smell. Yet refactoring actions are much less frequent with less than 50\% of the test ever going under refactoring. Interestingly,  while refactoring actions are rare, some smells tend to disappear through the removal of old symptomatic tests and the introduction of new tests not presenting such symptoms.
    
    \item \textbf{An approach to generate more flexible locators (Chapter~\ref{chap:flexible-locators-system-user-interactive-tests})} One of the main source of test fragility is the evolution of GUI elements tests are interacting with. To alleviate the impact of such evolution to a certain extent, we propose a novel approach, HPath, to provide more flexibility to DOM-based locators by exploiting the semantics offered in the HTML5 standard. We evaluate our approach on web elements mined from the test suites of two large open source projects. Our results suggest that, when HTML5 semantics are present, HPath can exploit rendered properties of web elements to generate expressive locators for 73.35\% of them reducing locator breakages from 64.99\% when relying on attribute properties to 0.49\% with rendered properties.
    
    \item \textbf{Ikora: Continuous Inspection for Keyword-Driven Testing (Chapter~\ref{chap:ikora-framework})} We introduce Ikora, an automated tool that statically analyzes Robot Framework test suites, enabling the continuous inspection of the test code base. Ikora targets code written in the Robot Framework syntax, a popular framework for writing Keyword-Driven tests. Ikora has been successfully deployed at BGL BNP Paribas, detecting issues otherwise unknown to the automation testers, such as the presence of duplicated test code, dead test code and dependency issues among the tests.
\end{itemize}

\subsection{Organization of the Dissertation}

In the remaining of this dissertation, Chapter~\ref{chap:related-work} presents the previous work that are related to the contributions presented in this dissertation. Chapter~\ref{chap:evolution-system-user-interactive-test} presents an empirical study that evaluates the evolution of GUI test scripts written using Keyword-Driven Testing in an industrial setting. Chapter~\ref{chap:smells-system-user-interactive-test} presents an empirical study on the diffusion and refactoring of smells present in Keyword-Driven Test scripts both in industrial settings and in open-source software. Chapter~\ref{chap:ikora-framework} describes the tools and frameworks built as the result of this work, to contribute to the advances in good testing practices when developing GUI test cases. Finally, Chapter~\ref{chap:conclusion} conclude this dissertation and presents the future work.