@online{GZoltarA61:online,
author = {},
title = {GZoltar - Automatic Testing \& Debugging using Spectrum-based Fault Localization (SFL)},
howpublished = {\url{https://gzoltar.com/}},
month = {},
year = {2020},
note = {(Accessed on 01/11/2021)}
}
@online{damorimR19:online,
author = {Marcelo d'Amorim},
title = {damorimRG/msr4flakiness},
howpublished = {\url{https://github.com/damorimRG/msr4flakiness/}},
month = {4},
year = {2020},
note = {(Accessed on 01/11/2021)}
}
@online{boxflaky83:online,
author = {Box, Inc.},
title = {box/flaky: Plugin for nose or pytest that automatically reruns flaky tests.},
howpublished = {\url{https://github.com/box/flaky}},
month = {8},
year = {2020},
note = {(Accessed on 01/11/2021)}
}
@article{Jimenez2019,
abstract = {Previous work on vulnerability prediction assume that predictive models are trained with respect to perfect labelling information (includes labels from future, as yet undiscovered vulnerabilities). In this paper we present results from a comprehensive empirical study of 1,898 real-world vulnerabilities reported in 74 releases of three security-critical open source systems (Linux Kernel, OpenSSL and Wiresark). Our study investigates the effectiveness of three previously proposed vulnerability prediction approaches, in two settings: with and without the unrealistic labelling assumption. The results reveal that the unrealistic labelling assumption can profoundly mislead the scientific conclusions drawn; suggesting highly effective and deployable prediction results vanish when we fully account for realistically available labelling in the experimental methodology. More precisely, MCC mean values of predictive effectiveness drop from 0.77, 0.65 and 0.43 to 0.08, 0.22, 0.10 for Linux Kernel, OpenSSL and Wiresark, respectively. Similar results are also obtained for precision, recall and other assessments of predictive efficacy. The community therefore needs to upgrade experimental and empirical methodology for vulnerability prediction evaluation and development to ensure robust and actionable scientific findings. CCS CONCEPTS • Software and its engineering → Software defect analysis.},
author = {Jimenez, Matthieu and Rwemalika, Renaud and Papadakis, Mike and Sarro, Federica and {Le Traon}, Yves and Harman, Mark},
doi = {10.1145/3338906.3338941},
file = {:Users/guillaume.haben/Documents/Work/papers/vulnerabilities/The Importance of Accounting for Real-World Labelling When Predicting Software Vulnerabilities.pdf:pdf},
isbn = {9781450355728},
keywords = {acm reference format,federica sarro,machine learning,matthieu jimenez,mike papadakis,prediction modelling,renaud rwemalika,software vulnerabilities},
pages = {695--705},
title = {{The importance of accounting for real-world labelling when predicting software vulnerabilities}},
year = {2019}
}
@inproceedings{Shi2019,
abstract = {Mutation testing is widely used in research as a metric for evaluat- ing the quality of test suites. Mutation testing runs the test suite on generated mutants (variants of the code under test) where a test suite kills a mutant if any of the tests fail when run on the mutant. Mutation testing implicitly assumes that tests exhibit deterministic behavior, in terms of their coverage and the outcome of a test (not) killing a certain mutant. Such an assumption does not hold in the presence of flaky tests, whose outcomes can non-deterministically differ even when run on the same code under test. Without reliable test outcomes, mutation testing can result in unreliable results, e.g., in our experiments, mutation scores vary by four percentage points on average between repeated executions, and 9{\%} of mutant-test pairs have an unknown status. Many modern software projects suffer from flaky tests. We propose techniques that manage flak- iness throughout the mutation testing process, largely based on strategically re-running tests. We implement our techniques by modifying the open-source mutation testing tool, PIT. Our evalua- tion on 30 projects shows that our techniques reduce the number of {\l}unknown{\v{z}} (flaky) mutants by 79.4{\%}.},
address = {New York, New York, USA},
author = {Shi, August and Bell, Jonathan and Marinov, Darko},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis - ISSTA 2019},
doi = {10.1145/3293882.3330568},
file = {:Users/guillaume.haben/Documents/Work/papers/Flakiness/Mitigating the Effects of Flaky Tests on Mutation Testing.pdf:pdf},
isbn = {9781450362245},
keywords = {2019,Flaky tests,acm reference format,and darko marinov,august shi,flaky tests,jonathan bell,mitigating the effects,mutation testing,non-deterministic coverage},
pages = {112--122},
publisher = {ACM Press},
title = {{Mitigating the effects of flaky tests on mutation testing}},
url = {http://dl.acm.org/citation.cfm?doid=3293882.3330568},
year = {2019}
}
@inproceedings{Lam2019RootCausing,
abstract = {In today's agile world, developers often rely on continuous integra- tion pipelines to help build and validate their changes by executing tests in an efficient manner. One of the significant factors that hinder developers' productivity is flaky tests—tests that may pass and fail with the same version of code. Since flaky test failures are not deterministically reproducible, developers often have to spend hours only to discover that the occasional failures have nothing to do with their changes. However, ignoring failures of flaky tests can be dangerous, since those failures may represent real faults in the production code. Furthermore, identifying the root cause of flakiness is tedious and cumbersome, since they are often a con- sequence of unexpected and non-deterministic behavior due to various factors, such as concurrency and external dependencies. As developers in a large-scale industrial setting, we first describe our experience with flaky tests by conducting a study on them. Our results show that although the number of distinct flaky tests may be low, the percentage of failing builds due to flaky tests can be substantial. To reduce the burden of flaky tests on developers, we describe our end-to-end framework that helps identify flaky tests and understand their root causes. Our framework instruments flaky tests and all relevant code to log various runtime properties, and then uses a preliminary tool, called RootFinder, to find differences in the logs of passing and failing runs. Using our framework, we collect and publicize a dataset of real-world, anonymized execution logs of flaky tests. By sharing the findings from our study, our framework and tool, and a dataset of logs, we hope to encourage more research on this important problem.},
address = {Beijing, China},
author = {Lam, Wing and Godefroid, Patrice and Nath, Suman and Santhiar, Anirudh and Thummalapenta, Suresh},
booktitle = {Proceedings ofthe 28th ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA '19)},
doi = {10.1145/3293882.3330570},
file = {:Users/guillaume.haben/Documents/Work/papers/Flakiness/Root Causing Flaky Tests in a Large-Scale Industrial Setting.pdf:pdf},
isbn = {9781450362245},
keywords = {debugging,flaky tests,regression testing ACM},
pages = {101--111},
publisher = {ACM Press},
title = {{Root Causing Flaky Tests in a Large-Scale Industrial Setting}},
year = {2019}
}
@article{King2018,
author = {King, Tariq M and Santiago, Dionny and Phillips, Justin and Clarke, Peter J},
doi = {10.1109/QRS-C.2018.00031},
file = {:Users/guillaume.haben/Documents/Work/papers/Flakiness/Towards a Bayesian Net Model for Predicting Flaky Tests.pdf:pdf},
isbn = {9781538678398},
journal = {2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
pages = {100--107},
publisher = {IEEE},
title = {{Towards a Bayesian Network Model for Predicting Flaky Automated Tests}},
year = {2018}
}
@article{Palomba2017a,
abstract = {{\textcopyright} 2017 IEEE. Regression testing is a core activity that allows developers to ensure that source code changes do not introduce bugs. An important prerequisite then is that test cases are deterministic. However, this is not always the case as some tests suffer from socalled flakiness. Flaky tests have serious consequences, as they can hide real bugs and increase software inspection costs. Existing research has focused on understanding the root causes of test flakiness and devising techniques to automatically fix flaky tests; a key area of investigation being concurrency. In this paper, we investigate the relationship between flaky tests and three previously defined test smells, namely Resource Optimism, Indirect Testing and Test Run War. We have set up a study involving 19, 532 JUnit test methods belonging to 18 software systems. A key result of our investigation is that 54{\%} of tests that are flaky contain a test code smell that can cause the flakiness. Moreover, we found that refactoring the test smells not only removed the design flaws, but also fixed all 54{\%} of flaky tests causally co-occurring with test smells.},
author = {Palomba, Fabio and Zaidman, Andy},
doi = {10.1109/ICSME.2017.12},
file = {:Users/guillaume.haben/Documents/Work/papers/testSmells/Does Refactoring of Test Smells Induce Fixing Flaky Tests? .pdf:pdf},
isbn = {9781538609927},
journal = {Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017},
keywords = {Flaky tests,Refactoring,Test smells},
pages = {1--12},
title = {{Does refactoring of test smells induce fixing flaky tests?}},
year = {2017}
}
@inproceedings{Luo2014,
abstract = {Regression testing is a crucial part of software development. It checks that software changes do not break existing func-tionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodified test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests— often called flaky tests—have non-deterministic outcomes. Such tests undermine the regression testing as they make it difficult to rely on test results. We present the first extensive study of flaky tests. We study in detail a total of 201 commits that likely fix flaky tests in 51 open-source projects. We classify the most com-mon root causes of flaky tests, identify approaches that could manifest flaky behavior, and describe common strategies that developers use to fix flaky tests. We believe that our insights and implications can help guide future research on the important topic of (avoiding) flaky tests.},
author = {Luo, Qingzhou and Hariri, Farah and Eloussi, Lamyaa and Marinov, Darko},
booktitle = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
doi = {10.1145/2635868.2635920},
file = {:Users/guillaume.haben/Library/Application Support/Mendeley Desktop/Downloaded/Luo et al. - 2014 - An empirical analysis of flaky tests.pdf:pdf},
isbn = {9781450330565},
keywords = {Empirical study,Flaky tests,Non-determinism},
month = {11},
pages = {643--653},
publisher = {Association for Computing Machinery},
title = {{An empirical analysis of flaky tests}},
volume = {16-21-November-2014},
year = {2014}
}
@inproceedings{Bell2018,
abstract = {Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle. We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96 Java projects on TravisCI, and found 87 previously unknown flaky tests in the 10 of these projects. We also ran experiments on project histories, where DeFlaker detected 1, 874 flaky tests from 4, 846 failures, with a low false alarm rate (1.5{\%}). DeFlaker had a higher recall (95.5{\%} vs. 23{\%}) of confirmed flaky tests than Maven's default flaky test detector. CCS CONCEPTS • Software and its engineering → Software testing and de-bugging;},
address = {New York, New York, USA},
author = {Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko},
booktitle = {Proceedings of the 40th International Conference on Software Engineering - ICSE '18},
doi = {10.1145/3180155.3180164},
file = {:Users/guillaume.haben/Documents/Work/papers/Flakiness/Deflaker.pdf:pdf},
isbn = {9781450356381},
keywords = {acm reference format,code coverage,flaky tests,jonathan bell,lamyaa eloussi,michael hilton,owolabi legunsen,software testing,tifany},
pages = {433--444},
publisher = {ACM Press},
title = {{DeFlaker: Automatically Detecting Flaky Tests}},
url = {http://dl.acm.org/citation.cfm?doid=3180155.3180164},
year = {2018}
}
@article{Cordy2019,
abstract = {Much research on software testing makes an implicit assumption that test failures are deterministic such that they always witness the presence of the same defects. However, this assumption is not always true because some test failures are due to so-called flaky tests, i.e., tests with non-deterministic outcomes. Unfortunately, flaky tests have major implications for testing and test-dependent activities such as mutation testing and automated program repair. To deal with this issue, we introduce a test flakiness assessment and experimentation platform, called FlakiMe, that supports the seeding of a (controllable) degree of flakiness into the behaviour of a given test suite. Thereby, FlakiMe equips researchers with ways to investigate the impact of test flakiness on their techniques under laboratory-controlled conditions. We use FlakiME to report results and insights from case studies that assesses the impact of flakiness on mutation testing and program repair. These results indicate that a 5{\%} of flakiness failures is enough to affect the mutation score, but the effect size is modest (2{\%} - 4{\%}), while it completely annihilates the ability of program repair to patch 50{\%} of the subject programs. We also observe that flakiness has case-specific effects, which mainly disrupts the repair of bugs that are covered by many tests. Moreover, we find that a minimal amount of user feedback is sufficient for alleviating the effects of flakiness.},
archivePrefix = {arXiv},
arxivId = {1912.03197},
author = {Cordy, Maxime and Papadakis, Mike and Rwemalika, Renaud and Harman, Mark},
eprint = {1912.03197},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/1912.03197.pdf:pdf},
journal = {arXiv},
title = {{FlakiMe: Laboratory-controlled test flakiness impact assessment. A case study on mutation testing and automated program repair}},
year = {2019}
}
@misc{Micco2017,
author = {Micco, John},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/45880.pdf:pdf},
journal = {2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)},
title = {{The State of Continuous Integration Testing Google}},
year = {2017}
}
@article{Shi2016,
abstract = {Some commonly used methods have nondeterministicspecifications, e.g., iterating through a set canreturn the elements in any order. However, non-deterministicspecifications typically have deterministic implementations, e.g.,iterating through two sets constructed in the same way mayreturn their elements in the same order. We use the termADINS code to refer to code that Assumes a DeterministicImplementation of a method with a Non-deterministic Specification. Such ADINS code can behave unexpectedly whenthe implementation changes, even if the specification remainsthe same. Further, ADINS code can lead to flaky tests -- teststhat pass or fail seemingly non-deterministically. We present a simple technique, called NONDEX, for detectingflaky tests due to ADINS code. We implemented NONDEX forJava: we found 31 methods with non-deterministic specificationsin the Java Standard Library, manually built non-deterministicmodels for these methods, and used a modified Java VirtualMachine to explore various non-deterministic choices. We evaluatedNONDEX on 195 open-source projects from GitHub and 72student submissions from a programming homework assignment.NONDEX detected 60 flaky tests in 21 open-source projects and110 flaky tests in 34 student submissions.},
author = {Shi, August and Gyori, Alex and Legunsen, Owolabi and Marinov, Darko},
doi = {10.1109/ICST.2016.40},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/Detecting Assumptions on Deterministic Implementations of Non-deterministic Specifications.pdf:pdf},
isbn = {9781509018260},
journal = {Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation, ICST 2016},
keywords = {Flaky Tests,Regression Testing,Specification},
pages = {80--90},
publisher = {IEEE},
title = {{Detecting Assumptions on Deterministic Implementations of Non-deterministic Specifications}},
year = {2016}
}
@article{Eck2019,
abstract = {Flaky tests are software tests that exhibit a seemingly random outcome (pass or fail) despite exercising unchanged code. In this work, we examine the perceptions of software developers about the nature, relevance, and challenges of flaky tests. We asked 21 professional developers to classify 200 flaky tests they previously fixed, in terms of the nature and the origin of the flakiness, as well as of the fixing effort. We also examined developers' fixing strategies. Subsequently, we conducted an online survey with 121 developers with a median industrial programming experience of five years. Our research shows that: The flakiness is due to several different causes, four of which have never been reported before, despite being the most costly to fix; flakiness is perceived as significant by the vast majority of developers, regardless of their team's size and project's domain, and it can have effects on resource allocation, scheduling, and the perceived reliability of the test suite; and the challenges developers report to face regard mostly the reproduction of the flaky behavior and the identification of the cause for the flakiness. Data and materials [https://doi.org/10.5281/zenodo.3265785].},
author = {Eck, Moritz and Castelluccio, Marco and Palomba, Fabio and Bacchelli, Alberto},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/eck2019.pdf:pdf},
isbn = {9781450355728},
journal = {arXiv},
keywords = {Empirical Studies,Flaky Tests,Mixed-Method Research},
pages = {830--840},
title = {{Understanding Flaky Tests: The Developer's Perspective}},
year = {2019}
}
@article{Dutta2020,
abstract = {Probabilistic programming systems and machine learning frameworks like Pyro, PyMC3, TensorFlow, and PyTorch provide scalable and efficient primitives for inference and training. However, such operations are non-deterministic. Hence, it is challenging for developers to write tests for applications that depend on such frameworks, often resulting in flaky tests - tests which fail non-deterministically when run on the same version of code. In this paper, we conduct the first extensive study of flaky tests in this domain. In particular, we study the projects that depend on four frameworks: Pyro, PyMC3, TensorFlow-Probability, and PyTorch. We identify 75 bug reports/commits that deal with flaky tests, and we categorize the common causes and fixes for them. This study provides developers with useful insights on dealing with flaky tests in this domain. Motivated by our study, we develop a technique, FLASH, to systematically detect flaky tests due to assertions passing and failing in different runs on the same code. These assertions fail due to differences in the sequence of random numbers in different runs of the same test. FLASH exposes such failures, and our evaluation on 20 projects results in 11 previously-unknown flaky tests that we reported to developers.},
author = {Dutta, Saikat and Shi, August and Choudhary, Rutvik and Zhang, Zhekun and Jain, Aryaman and Misailovic, Sasa},
doi = {10.1145/3395363.3397366},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/ISSTA2020-flash.pdf:pdf},
isbn = {9781450380089},
journal = {ISSTA 2020 - Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
keywords = {Flaky tests,Machine Learning,Non-Determinism,Probabilistic Programming,Randomness},
pages = {211--224},
title = {{Detecting flaky tests in probabilistic and machine learning applications}},
year = {2020}
}
@article{Lam2019iDFlakies,
abstract = {Regression testing is increasingly important with the wide use of continuous integration. A desirable requirement for regression testing is that a test failure reliably indicates a problem in the code under test and not a false alarm from the test code or the testing infrastructure. However, some test failures are unreliable, stemming from flaky tests that can nondeterministically pass or fail for the same code under test. There are many types of flaky tests, with order-dependent tests being a prominent type. To help advance research on flaky tests, we present (1) a framework, iDFlakies, to detect and partially classify flaky tests; (2) a dataset of flaky tests in open-source projects; and (3) a study with our dataset. iDFlakies automates experimentation with our tool for Maven-based Java projects. Using iDFlakies, we build a dataset of 422 flaky tests, with 50.5{\%} order-dependent and 49.5{\%} not. Our study of these flaky tests finds the prevalence of two types of flaky tests, probability of a test-suite run to have at least one failure due to flaky tests, and how different test reorderings affect the number of detected flaky tests. We envision that our work can spur research to alleviate the problem of flaky tests.},
author = {Lam, Wing and Oei, Reed and Shi, August and Marinov, Darko and Xie, Tao},
doi = {10.1109/ICST.2019.00038},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/lam2019 (1).pdf:pdf},
isbn = {9781728117355},
journal = {Proceedings - 2019 IEEE 12th International Conference on Software Testing, Verification and Validation, ICST 2019},
keywords = {Flaky tests,Order dependent tests,Regression testing},
pages = {312--322},
publisher = {IEEE},
title = {{IDFlakies: A framework for detecting and partially classifying flaky tests}},
year = {2019}
}
@article{Lam2020,
abstract = {Flaky tests are tests that can non-deterministically pass and fail. They pose a major impediment to regression testing, because they provide an inconclusive assessment on whether recent code changes contain faults or not. Prior studies of flaky tests have proposed tools to detect flaky tests and identified various sources of flakiness in tests, e.g., order-dependent (OD) tests that deterministically fail for some order of tests in a test suite but deterministically pass for some other orders. Several of these studies have focused on OD tests. We focus on an important and under-explored source of flakiness in tests: non-order-dependent tests that can non-deterministically pass and fail even for the same order of tests. Instead of using specialized tools that aim to detect flaky tests, we run tests using the tool configured by the developers. Specifically, we perform our empirical evaluation on Java projects that rely on the Maven Surefire plugin to run tests. We re-execute each test suite 4000 times, potentially in different test-class orders, and we label tests as flaky if our runs have both pass and fail outcomes across these reruns. We obtain a dataset of 107 flaky tests and study various characteristics of these tests. We find that many tests previously called "non-order-dependent" actually do depend on the order and can fail with very different failure rates for different orders.},
author = {Lam, Wing and Winter, Stefan and Astorga, Angello and Stodden, Victoria and Marinov, Darko},
doi = {10.1109/issre5003.2020.00045},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/lam2020.pdf:pdf},
isbn = {9781728198705},
issn = {10719458},
pages = {403--413},
title = {{Understanding Reproducibility and Characteristics of Flaky Tests Through Test Reruns in Java Projects}},
year = {2020}
}
@article{Lam2020a,
abstract = {During regression testing, developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality. Thus, flaky tests, which nondeterministically pass or fail on the same code, are problematic because they provide misleading signals during regression testing. Although flaky tests are the focus of several existing studies, none of them study (1) the reoccurrence, runtimes, and time-before-fix of flaky tests, and (2) flaky tests indepth on proprietary projects. This paper fills this knowledge gap about flaky tests and investigates whether prior categorization work on flaky tests also apply to proprietary projects. Specifically, we study the lifecycle of flaky tests in six large-scale proprietary projects at Microsoft.We find, as in prior work, that asynchronous calls are the leading cause of flaky tests in these Microsoft projects. Therefore, we propose the first automated solution, called Flakiness and Time Balancer (FaTB), to reduce the frequency of flaky-test failures caused by asynchronous calls. Our evaluation of five such flaky tests shows that FaTB can reduce the running times of these tests by up to 78{\%} without empirically affecting the frequency of their flaky-test failures. Lastly, our study finds several cases where developers claim they fixed a flaky test but our empirical experiments show that their changes do not fix or reduce these tests' frequency of flaky-test failures. Future studies should be more cautious when basing their results on changes that developers claim to be fixes .},
author = {Lam, Wing and Muslu, Kivanc and Sajnani, Hitesh and Thummalapenta, Suresh},
doi = {10.1145/3377811.3381749},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/LamETAL20FaTB.pdf:pdf},
isbn = {9781450371216},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Empirical study,Flaky test,Lifecycle},
pages = {1471--1482},
title = {{A study on the lifecycle of flaky tests}},
year = {2020}
}
@article{Lam2020b,
abstract = {Flaky tests are tests that can non-deterministically pass or fail for the same code version. These tests undermine regression testing efficiency, because developers cannot easily identify whether a test fails due to their recent changes or due to flakiness. Ideally, one would detect flaky tests right when flakiness is introduced, so that developers can then immediately remove the flakiness. Some software organizations, e.g., Mozilla and Netflix, run some tools-detectors-to detect flaky tests as soon as possible. However, detecting flaky tests is costly due to their inherent non-determinism, so even state-of-the-art detectors are often impractical to be used on all tests for each project change. To combat the high cost of applying detectors, these organizations typically run a detector solely on newly added or directly modified tests, i.e., not on unmodified tests or when other changes occur (including changes to the test suite, the code under test, and library dependencies). However, it is unclear how many flaky tests can be detected or missed by applying detectors in only these limited circumstances. To better understand this problem, we conduct a large-scale longitudinal study of flaky tests to determine when flaky tests become flaky and what changes cause them to become flaky. We apply two state-of-the-art detectors to 55 Java projects, identifying a total of 245 flaky tests that can be compiled and run in the code version where each test was added. We find that 75{\%} of flaky tests (184 out of 245) are flaky when added, indicating substantial potential value for developers to run detectors specifically on newly added tests. However, running detectors solely on newly added tests would still miss detecting 25{\%} of flaky tests. The percentage of flaky tests that can be detected does increase to 85{\%} when detectors are run on newly added or directly modified tests. The remaining 15{\%} of flaky tests become flaky due to other changes and can be detected only when detectors are always applied to all tests. Our study is the first to empirically evaluate when tests become flaky and to recommend guidelines for applying detectors in the future. 1 INTRODUCTION To find bugs and regressions during software development, developers run tests that execute the software under test (SUT) and check the results. If all tests pass, the new changes are allowed to be merged to the SUT. However, if some test fails, then the developers typically debug their changes to locate and remove the cause of the failure. Unfortunately, not all tests provide a consistent result, and may fail for reasons other than a recent change. A test that can both pass and fail in repeated runs, on the same SUT (even without new changes), is known as a flaky test [Luo et al. 2014]. When a flaky test fails, the developers, unaware of the flakiness at first, can start debugging for the cause of the failure within their changes. In particular, during regression testing, if a test that passed on a previous code revision fails on the new code revision, the developers would typically debug for the cause in the changes between the two revisions. Unfortunately, this debugging can take substantial time and effort, because the new changes are not the (sole) cause of failure for flaky tests, essentially (mis)leading developers to debug the failure in the wrong place. Many software organizations report that flaky tests are one of their biggest problems in software development. For example, Facebook released a position paper on the importance of flaky tests [Har-man and O'Hearn 2018] and recently ran a call for research projects focused on flaky tests [Face-bookFlakyTestCall 2019]. Several papers and blog posts from Google [AvoidingFlakeyTests 2019; FlakinessDashboardHOWTO 2020; Micco 2020; Ziftci and Reardon 2017] and Microsoft [Harry 2019; Lam et al. 2019a, 2020a] have reported several challenges with flaky tests, even estimating the monetary cost of flaky tests on developer productivity [Herzig et al. 2015]. Huawei test logs were analyzed for flaky test failures by Jiang et al. [2017]. Other organizations, including Mozilla [Eck et al. 2019; Rahman and Rigby 2018], Netflix [NetflixAutomationTalk 2017], Salesforce [Salesforce-FlakyTests 2016], and ThoughtWorks [Sudarshan 2012], also publicly report their problems with flaky tests. Much existing work [Coverity 2014; Infer 2020; Larus et al. 2004; Memon et al. 2017; Muşlu et al. 2015; Sadowski et al. 2015; Saff and Ernst 2003] has found that providing developers with feedback regarding problematic code is most helpful if provided as soon as possible in the development process. For example, for the static bug-detection tool Infer [2020] at Facebook, Harman and O'Hearn [2018] found that bugs reported to developers at "post land" (after code has been merged into the SUT) had a close to 0{\%} fix rate, yet bugs reported at "diff time" (when the developer submits the code change) had a fix rate of over 70{\%}. In the case of flaky tests, it is likely far more useful to notify developers that a change they are in the process of making introduces test flakiness, rather than to notify developers weeks or months after they made the change that a test is flaky. In fact, the standard testing practice at organizations, such as Mozilla [MozillaChaosMode 2019] and Netflix [NetflixAutomationTalk 2017], already aims to find whether newly added tests are flaky as soon as possible. Due to the non-deterministic nature of flaky tests, detecting them is quite challenging, and has become an increasingly active area of research. The simplest approach to detect flaky tests is to just rerun tests many times: if different outcomes are observed on two runs, then the test is flaky. However, if we can guess the underlying source of non-determinism that causes a test to be flaky, it can be easier to detect the test by purposefully modifying some relevant environmental properties. For instance, some flaky tests might be order-dependent, where some test(s) may write to a shared state (e.g., memory or disk), and this state "pollution" can cause another test to fail depending on the order in which the tests run. Other flaky tests may be implementation-dependent, where a test is flaky due to an assumption that an API is deterministic, when that API is not (e.g., the order of iteration over a HashSet). Several tools have been proposed for detecting these and other categories of flaky tests [Gyori et al.},
author = {Lam, Wing and Winter, Stefan and Wei, Anjiang and Xie, Tao and Marinov, Darko and Bell, Jonathan},
doi = {10.1145/3428270},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/oopsla20flaky.pdf:pdf},
issn = {2475-1421},
journal = {Proceedings of the ACM on Programming Languages},
number = {OOPSLA},
pages = {1--29},
title = {{A large-scale longitudinal study of flaky tests}},
volume = {4},
year = {2020}
}
@article{Shi2019Mitigating,
abstract = {Mutation testing is widely used in research as a metric for evaluating the quality of test suites. Mutation testing runs the test suite on generated mutants (variants of the code under test) where a test suite kills a mutant if any of the tests fail when run on the mutant. Mutation testing implicitly assumes that tests exhibit deterministic behavior, in terms of their coverage and the outcome of a test (not) killing a certain mutant. Such an assumption does not hold in the presence of flaky tests, whose outcomes can non-deterministically differ even when run on the same code under test. Without reliable test outcomes, mutation testing can result in unreliable results, e.g., in our experiments, mutation scores vary by four percentage points on average between repeated executions, and 9{\%} of mutant-test pairs have an unknown status. Many modern software projects suffer from flaky tests. We propose techniques that manage flakiness throughout the mutation testing process, largely based on strategically re-running tests. We implement our techniques by modifying the open-source mutation testing tool, PIT. Our evaluation on 30 projects shows that our techniques reduce the number of {\l}unknown{\v{z}} (flaky) mutants by 79.4{\%}.},
author = {Shi, August and Bell, Jonathan and Marinov, Darko},
doi = {10.1145/3293882.3330568},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/shi2019 (1).pdf:pdf},
isbn = {9781450362245},
journal = {ISSTA 2019 - Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
keywords = {Flaky tests,Mutation testing,Non-deterministic coverage},
pages = {296--306},
title = {{Mitigating the effects of flaky tests on mutation testing}},
year = {2019}
}
@inproceedings{Shi2019iFix,
abstract = {Regression testing provides important pass or fail signals that devel- opers use to make decisions after code changes. However, flaky tests, which pass or fail even when the code has not changed, can mislead developers. A common kind of flaky tests are order-dependent tests, which pass or fail depending on the order in which the tests are run. Fixing order-dependent tests is often tedious and time-consuming. We propose iFixFlakies, a framework for automatically fixing order-dependent tests. The key insight in iFixFlakies is that test suites often already have tests, which we call helpers, whose logic resets or sets the states for order-dependent tests to pass. iFixFlakies searches a test suite for helpers that make the order-dependent tests pass and then recommends patches for the order-dependent tests using code from these helpers. Our evaluation on 110 truly order- dependent tests from a public dataset shows that 58 of them have helpers, and iFixFlakies can fix all 58.We opened pull requests for 56 order-dependent tests (2 of 58 were already fixed), and developers have already accepted pull requests for 21 of them, with all the remaining ones still pending. CCS},
author = {Shi, August and Lam, Wing and Oei, Reed and Xie, Tao and Marinov, Darko},
booktitle = {27th ACM Joint European Software Engineering Conference and Symposium on the Foundations ofSoftware Engineering (ESEC/FSE '19)},
doi = {10.1145/3338906.3338925},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/shi2019.pdf:pdf},
isbn = {9781450355728},
keywords = {2019,acm reference format,and darko marinov,august shi,automated fixing,flaky test,ifixflakies,order-dependent test,patch generation,reed oei,tao xie,wing lam},
title = {{iFixFlakies : A Framework for Automatically Fixing Order-Dependent Flaky Tests}},
year = {2019}
}
@article{Thorve2018,
abstract = {A flaky test is a test that may fail or pass for thesame code under testing (CUT). Flaky tests could be harmfulto developers because the non-deterministic test outcome is notreliable and developers cannot easily debug the code. A priorstudy characterized the root causes and fixing strategies of flakytests by analyzing commits of 51 Apache open source projects,without analyzing any Android app. Due to the popular usageof Android devices and the multitude of interactions of Androidapps with third-party software libraries, hardware, network, andusers, we were curious to find if the Android apps manifestedunique flakiness patterns and called for any special resolutionfor flaky tests as compared to the existing literature. For this paper, we conducted an empirical study to characterizethe flaky tests in Android apps. By classifying the rootcauses and fixing strategies of flakiness, we aimed to investigatehow our proposed characterization for flakiness in Android appsvaries from prior findings, and whether there are domain-specificflakiness patterns. After mining GitHub, we found 29 Androidprojects containing 77 commits that were relevant to flakiness.We identified five root causes of Android apps' flakiness. Werevealed three novel causes - Dependency, Program Logic, and UI.Five types of resolution strategies were observed to address theflaky behavior. Many of the examined commits show developers' attempt to fix flakiness by changing software implementation invarious ways. However, there are still 13{\%} commits that simplyskipped or removed the flaky tests. Our observations provideuseful insights for future research on flaky tests of Android apps.},
author = {Thorve, Swapna and Sreshtha, Chandani and Meng, Na},
doi = {10.1109/ICSME.2018.00062},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/thorve2018.pdf:pdf},
isbn = {9781538678701},
journal = {Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
keywords = {Android,Empirical,Flaky Tests},
pages = {534--538},
publisher = {IEEE},
title = {{An empirical study of flaky tests in android apps}},
year = {2018}
}
@inproceedings{Harman2018,
abstract = {This paper 1 describes some of the challenges and opportunities when deploying static and dynamic analysis at scale, drawing on the authors' experience with the Infer and Sapienz Technologies at Facebook, each of which started life as a research-led start-up that was subsequently deployed at scale, impacting billions of people worldwide. The paper identifies open problems that have yet to receive significant attention from the scientific community, yet which have potential for profound real world impact, formulating these as research questions that, we believe, are ripe for exploration and that would make excellent topics for research projects.},
author = {Harman, Mark and O'Hearn, Peter},
booktitle = {2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)},
doi = {10.1109/SCAM.2018.00009},
file = {:Users/guillaume.haben/Documents/Work/papers/general/From Start-ups to Scale-ups- Opportuni- ties and Open Problems for Static and Dynamic Program Analysis..pdf:pdf},
isbn = {978-1-5386-8290-6},
keywords = {Compositional reasoning,Flaky test,Infer,SBSE,Sapienz,Separation Logic,Testing,Verification},
month = {9},
pages = {1--23},
publisher = {IEEE},
title = {{From Start-ups to Scale-ups: Opportunities and Open Problems for Static and Dynamic Program Analysis}},
url = {https://ieeexplore.ieee.org/document/8530713/},
year = {2018}
}
@article{Azizi2018,
abstract = {Regression test selection offers cost savings by selecting a subset of existing tests when testers validate the modified version of the application. The majority of test selection approaches utilize static or dynamic analyses to decide which test cases should be selected, and these analyses are often very time consuming. In this paper, we propose a novel language-independent Regression TEst SelecTion (ReTEST) technique that facilitates a lightweight analysis by using information retrieval. ReTEST uses fault history, test case diversity, and program change history information to select test cases that should be rerun. Our empirical evaluation with four open source programs shows that our approach can be effective and efficient by selecting a far smaller subset of tests compared to the existing techniques.},
author = {Azizi, Maral and Do, Hyunsook},
doi = {10.1109/ISSRE.2018.00025},
file = {:Users/guillaume.haben/Documents/Work/papers/informationRetrieval/ReTEST- A Cost Effective Test Case Selection Technique for Modern Software Development .pdf:pdf},
isbn = {9781538683217},
issn = {10719458},
journal = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
keywords = {Regression},
pages = {144--154},
publisher = {IEEE},
title = {{ReTEST: A Cost Effective Test Case Selection Technique for Modern Software Development}},
volume = {2018-October},
year = {2018}
}
@article{Palomba2018,
abstract = {Software testing is a key activity to control the reliability of production code. Unfortunately, the effectiveness of test cases can be threatened by the presence of faults. Recent work showed that static indicators can be exploited to identify test-related issues. In particular test smells, i.e., sub-optimal design choices applied by developers when implementing test cases, have been shown to be related to test case effectiveness. While some approaches for the automatic detection of test smells have been proposed so far, they generally suffer of poor performance: As a consequence, current detectors cannot properly provide support to developers when diagnosing the quality of test cases. In this paper, we aim at making a step ahead toward the automated detection of test smells by devising a novel textual-based detector, coined TASTE (Textual AnalySis for Test smEll detection), with the aim of evaluating the usefulness of textual analysis for detecting three test smell types, General Fixture, Eager Test, and Lack of Cohesion of Methods. We evaluate TASTE in an empirical study that involves a manually-built dataset composed of 494 test smell instances belonging to 12 software projects, comparing the capabilities of our detector with those of two code metrics-based techniques proposed by Van Rompaey et al. and Greiler et al. Our results show that the structural-based detection applied by existing approaches cannot identify most of the test smells in our dataset, while TASTE is up to 44{\%} more effective. Finally, we find that textual and structural approaches can identify different sets of test smells, thereby indicating complementarity.},
author = {Palomba, Fabio and Zaidman, Andy and {De Lucia}, Andrea},
doi = {10.1109/ICSME.2018.00040},
file = {:Users/guillaume.haben/Documents/Work/papers/testSmells/Automatic Test Smell Detection Using Information Retrieval Techniques.pdf:pdf},
isbn = {9781538678701},
journal = {Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
keywords = {Empirical Studies,Mining Software Repositories,Test smells},
pages = {311--322},
publisher = {IEEE},
title = {{Automatic test smell detection using information retrieval techniques}},
year = {2018}
}
@article{Pinto2020,
abstract = {Flaky tests are tests whose outcomes are non-deterministic. Despite the recent research activity on this topic, no effort has been made on understanding the vocabulary of flaky tests. This work proposes to automatically classify tests as flaky or not based on their vocabulary. Static classification of flaky tests is important, for example, to detect the introduction of flaky tests and to search for flaky tests after they are introduced in regression test suites. We evaluated performance of various machine learning algorithms to solve this problem. We constructed a data set of flaky and non-flaky tests by running every test case, in a set of 64k tests, 100 times (6.4 million test executions). We then used machine learning techniques on the resulting data set to predict which tests are flaky from their source code. Based on features, such as counting stemmed tokens extracted from source code identifiers, we achieved an F-measure of 0.95 for the identification of flaky tests. The best prediction performance was obtained when using Random Forest and Support Vector Machines. In terms of the code identifiers that are most strongly associated with test flakiness, we noted that job, action, and services are commonly associated with flaky tests. Overall, our results provides initial yet strong evidence that static detection of flaky tests is effective.},
author = {Pinto, Gustavo and Miranda, Breno and Dissanayake, Supun and D'Amorim, Marcelo and Treude, Christoph and Bertolino, Antonia},
doi = {10.1145/3379597.3387482},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/795700a492.pdf:pdf},
isbn = {9781450379571},
journal = {Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020},
keywords = {Regression testing,Test flakiness,Text classification},
pages = {492--502},
title = {{What is the Vocabulary of Flaky Tests?}},
year = {2020}
}
@article{Gomez2014,
abstract = {Context Replication plays an important role in experimental disciplines. There are still many uncertainties about how to proceed with replications of SE experiments. Should replicators reuse the baseline experiment materials? How much liaison should there be among the original and replicating experimenters, if any? What elements of the experimental configuration can be changed for the experiment to be considered a replication rather than a new experiment? Objective To improve our understanding of SE experiment replication, in this work we propose a classification which is intend to provide experimenters with guidance about what types of replication they can perform. Method The research approach followed is structured according to the following activities: (1) a literature review of experiment replication in SE and in other disciplines, (2) identification of typical elements that compose an experimental configuration, (3) identification of different replications purposes and (4) development of a classification of experiment replications for SE. Results We propose a classification of replications which provides experimenters in SE with guidance about what changes can they make in a replication and, based on these, what verification purposes such a replication can serve. The proposed classification helped to accommodate opposing views within a broader framework, it is capable of accounting for less similar replications to more similar ones regarding the baseline experiment. Conclusion The aim of replication is to verify results, but different types of replication serve special verification purposes and afford different degrees of change. Each replication type helps to discover particular experimental conditions that might influence the results. The proposed classification can be used to identify changes in a replication and, based on these, understand the level of verification. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
author = {G{\'{o}}mez, Omar S. and Juristo, Natalia and Vegas, Sira},
doi = {10.1016/j.infsof.2014.04.004},
file = {:Users/guillaume.haben/Documents/Work/papers/general/10.1016@j.infsof.2014.04.004.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Experimental software engineering,Experimentation,Replication,Software engineering},
number = {8},
pages = {1033--1048},
title = {{Understanding replication of experiments in software engineering: A classification}},
volume = {56},
year = {2014}
}
@article{Shull2008,
abstract = {Replications play a key role in Empirical Software Engineering by allowing the community to build knowledge about which results or observations hold under which conditions. Therefore, not only can a replication that produces similar results as the original experiment be viewed as successful, but a replication that produce results different from those of the original experiment can also be viewed as successful. In this paper we identify two types of replications: exact replications, in which the procedures of an experiment are followed as closely as possible; and conceptual replications, in which the same research question is evaluated by using a different experimental procedure. The focus of this paper is on exact replications. We further explore them to identify two sub-categories: dependent replications, where researchers attempt to keep all the conditions of the experiment the same or very similar and independent replications, where researchers deliberately vary one or more major aspects of the conditions of the experiment. We then discuss the role played by each type of replication in terms of its goals, benefits, and limitations. Finally, we highlight the importance of producing adequate documentation for an experiment (original or replication) to allow for replication. A properly documented replication provides the details necessary to gain a sufficient understanding of the study being replicated without requiring the replicator to slavishly follow the given procedures. {\textcopyright} 2008 Springer Science+Business Media, LLC.},
author = {Shull, Forrest J. and Carver, Jeffrey C. and Vegas, Sira and Juristo, Natalia},
doi = {10.1007/s10664-008-9060-1},
file = {:Users/guillaume.haben/Documents/Work/papers/general/10.1.1.460.1082.pdf:pdf},
issn = {13823256},
journal = {Empirical Software Engineering},
keywords = {Experimental infrastructure,Experimental replications,Lab package},
number = {2},
pages = {211--218},
title = {{The role of replications in Empirical Software Engineering}},
volume = {13},
year = {2008}
}
@inproceedings{Kowalczyk2020,
author = {Kowalczyk, Emily and Nair, Karan and Gao, Zebao and Silberstein, Leo and Long, Teng and Memon, Atif},
title = {Modeling and Ranking Flaky Tests at Apple},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381370},
doi = {10.1145/3377813.3381370},
abstract = {Test flakiness---inability to reliably repeat a test's Pass/Fail outcome---continues to be a significant problem in Industry, adversely impacting continuous integration and test pipelines. Completely eliminating flaky tests is not a realistic option as a significant fraction of system tests (typically non-hermetic) for services-based implementations exhibit some level of flakiness. In this paper, we view the flakiness of a test as a rankable value, which we quantify, track and assign a confidence. We develop two ways to model flakiness, capturing the randomness of test results via entropy, and the temporal variation via flipRate, and aggregating these over time. We have implemented our flakiness scoring service and discuss how its adoption has impacted test suites of two large services at Apple. We show how flakiness is distributed across the tests in these services, including typical score ranges and outliers. The flakiness scores are used to monitor and detect changes in flakiness trends. Evaluation results demonstrate near perfect accuracy in ranking, identification and alignment with human interpretation. The scores were used to identify 2 causes of flakiness in the dataset evaluated, which have been confirmed, and where fixes have been implemented or are underway. Our models reduced flakiness by 44\% with less than 1\% loss in fault detection.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {110–119},
numpages = {10},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}
@article{Silva2020,
abstract = {A test is said to be flaky when it non-deterministically passes or fails. Test flakiness negatively affects the effectiveness of regression testing and, consequently, impacts software evolution. Detecting test flakiness is an important and challenging problem. ReRun is the most popular approach in industry to detect test flakiness. It re-executes a test suite on a fixed code version multiple times, looking for inconsistent outputs across executions. Unfortunately, ReRun is costly and unreliable. This paper proposes SHAKER, a lightweight technique to improve the ability of ReRun to detect flaky tests. SHAKER adds noise in the execution environment (e.g., it adds stressor tasks to compete for the CPU or memory). It builds on the observations that concurrency is an important source of flakiness and that adding noise in the environment can interfere in the ordering of events and, consequently, influence on the test outputs. We conducted experiments on a data set with 11 Android apps. Results are very encouraging. SHAKER discovered many more flaky tests than ReRun (95{\%} and 37.5{\%} of the total, respectively) and discovered these flaky tests much faster. In addition, SHAKER was able to reveal 61 new flaky tests that went undetected in 50 re-executions with ReRun.},
author = {Silva, Denini and Teixeira, Leopoldo and D'Amorim, Marcelo},
doi = {10.1109/ICSME46990.2020.00037},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/Silva{\_}ETAL{\_}ICSME20.pdf:pdf},
isbn = {9781728156194},
journal = {Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020},
keywords = {noise,regression testing,test flakiness},
pages = {301--311},
title = {{Shake It! Detecting Flaky Tests Caused by Concurrency with Shaker}},
year = {2020}
}
@article{Qin2021,
author = {Qin, Yihao and Wang, Shangwen and Liu, Kui and Mao, Xiaoguang},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/2021{\_}SANER{\_}camera-ready.pdf:pdf},
number = {2},
title = {{On the Impact of Flaky Tests in Automated Program Repair}},
year = {2021}
}
@inproceedings{JiangHuawei,
author = {Jiang, He and Li, Xiaochen and Yang, Zijiang and Xuan, Jifeng},
title = {What Causes My Test Alarm? Automatic Cause Analysis for Test Alarms in System and Integration Testing},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.71},
doi = {10.1109/ICSE.2017.71},
abstract = {Driven by new software development processes and testing in clouds, system and integration testing nowadays tends to produce enormous number of alarms. Such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms. The causes are critical because they decide which stakeholders are responsible to fix the bugs detected during the testing. In this paper, we present a novel approach that aims to relieve the burden by automating the procedure. Our approach, called Cause Analysis Model, exploits information retrieval techniques to efficiently infer test alarm causes based on test logs. We have developed a prototype and evaluated our tool on two industrial datasets with more than 14,000 test alarms. Experiments on the two datasets show that our tool achieves an accuracy of 58.3\% and 65.8\%, respectively, which outperforms the baseline algorithms by up to 13.3\%. Our algorithm is also extremely efficient, spending about 0.1s per cause analysis. Due to the attractive experimental results, our industrial partner, a leading information and communication technology company in the world, has deployed the tool and it achieves an average accuracy of 72\% after two months of running, nearly three times more accurate than a previous strategy based on regular expressions.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {712–723},
numpages = {12},
keywords = {system and integration testing, software testing, multiclass classification, test alarm analysis},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}


@misc{pytestdocumentation,
author = {},
title = {Pytest documentation},
howpublished = {\url{https://docs.pytest.org/en/stable/}},
month = {},
year = {},
note = {(Accessed on 03/18/2021)}
}

@inproceedings{alshammari2021flakeflagger,
  title={Flakeflagger: Predicting flakiness without rerunning tests},
  author={Alshammari, Abdulrahman and Morris, Christopher and Hilton, Michael and Bell, Jonathan},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages={1572--1584},
  year={2021},
  organization={IEEE}
}
@inproceedings{shi_ifixflakies_2019,
	address = {Tallinn, Estonia},
	series = {{ESEC}/{FSE} 2019},
	title = {{iFixFlakies}: a framework for automatically fixing order-dependent flaky tests},
	isbn = {978-1-4503-5572-8},
	shorttitle = {{iFixFlakies}},
	url = {https://doi.org/10.1145/3338906.3338925},
	doi = {10.1145/3338906.3338925},
	abstract = {Regression testing provides important pass or fail signals that developers use to make decisions after code changes. However, flaky tests, which pass or fail even when the code has not changed, can mislead developers. A common kind of flaky tests are order-dependent tests, which pass or fail depending on the order in which the tests are run. Fixing order-dependent tests is often tedious and time-consuming. We propose iFixFlakies, a framework for automatically fixing order-dependent tests. The key insight in iFixFlakies is that test suites often already have tests, which we call helpers, whose logic resets or sets the states for order-dependent tests to pass. iFixFlakies searches a test suite for helpers that make the order-dependent tests pass and then recommends patches for the order-dependent tests using code from these helpers. Our evaluation on 110 truly orderdependent tests from a public dataset shows that 58 of them have helpers, and iFixFlakies can fix all 58. We opened pull requests for 56 order-dependent tests (2 of 58 were already fixed), and developers have already accepted pull requests for 21 of them, with all the remaining ones still pending.},
	urldate = {2020-07-02},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shi, August and Lam, Wing and Oei, Reed and Xie, Tao and Marinov, Darko},
	month = aug,
	year = {2019},
	keywords = {automated fixing, flaky test, order-dependent test, patch generation},
	pages = {545--555},
	file = {Full Text PDF:/Users/sarra.habchi/Zotero/storage/S2NW2FYM/Shi et al. - 2019 - iFixFlakies a framework for automatically fixing .pdf:application/pdf}
}

@techreport{kit_cha_2007,
  abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis. test chapter},
  added-at = {2010-10-18T14:00:32.000+0200},
  at = {2009-01-27 02:42:01},
  author = {Kitchenham, Barbara and Charters, Stuart},
  biburl = {https://www.bibsonomy.org/bibtex/227b256010a48688388374cf83b619b54/msn},
  id = {3955888},
  institution = {Keele University and Durham University Joint Report},
  interhash = {aed0229656ada843d3e3f24e5e5c9eb9},
  intrahash = {27b256010a48688388374cf83b619b54},
  keywords = {research.cs.softeng research.support},
  number = {EBSE 2007-001},
  priority = {2},
  timestamp = {2010-10-18T14:00:32.000+0200},
  title = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
  url = {http://www.dur.ac.uk/ebse/resources/Systematic-reviews-5-8.pdf},
  year = 2007
}




@inproceedings{luo_empirical_2014,
	address = {Hong Kong, China},
	series = {{FSE} 2014},
	title = {An empirical analysis of flaky tests},
	isbn = {978-1-4503-3056-5},
	url = {https://doi.org/10.1145/2635868.2635920},
	doi = {10.1145/2635868.2635920},
	abstract = {Regression testing is a crucial part of software development. It checks that software changes do not break existing functionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodified test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests often called flaky tests—have non-deterministic outcomes. Such tests undermine the regression testing as they make it difficult to rely on test results. We present the first extensive study of flaky tests. We study in detail a total of 201 commits that likely fix flaky tests in 51 open-source projects. We classify the most common root causes of flaky tests, identify approaches that could manifest flaky behavior, and describe common strategies that developers use to fix flaky tests. We believe that our insights and implications can help guide future research on the important topic of (avoiding) flaky tests.},
	urldate = {2020-07-02},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Luo, Qingzhou and Hariri, Farah and Eloussi, Lamyaa and Marinov, Darko},
	month = nov,
	year = {2014},
	keywords = {Empirical study, flaky tests, non-determinism},
	pages = {643--653},
	file = {Full Text PDF:/Users/sarra.habchi/Zotero/storage/68UJ7GKY/Luo et al. - 2014 - An empirical analysis of flaky tests.pdf:application/pdf}
}
@misc{yasin2012quality,
  title={On the quality of grey literature and its use in information synthesis during systematic literature reviews},
  author={Yasin, Affan and Hasnain, Muhammad Ijlal},
  year={2012}
}
@article{garousi2019guidelines,
  title={Guidelines for including grey literature and conducting multivocal literature reviews in software engineering},
  author={Garousi, Vahid and Felderer, Michael and M{\"a}ntyl{\"a}, Mika V},
  journal={Information and Software Technology},
  volume={106},
  pages={101--121},
  year={2019},
  publisher={Elsevier}
}
@inproceedings{gao_making_2015,
	title = {Making {System} {User} {Interactive} {Tests} {Repeatable}: {When} and {What} {Should} {We} {Control}?},
	volume = {1},
	shorttitle = {Making {System} {User} {Interactive} {Tests} {Repeatable}},
	doi = {10.1109/ICSE.2015.28},
	abstract = {System testing and invariant detection is usually conducted from the user interface perspective when the goal is to evaluate the behavior of an application as a whole. A large number of tools and techniques have been developed to generate and automate this process, many of which have been evaluated in the literature or internally within companies. Typical metrics for determining effectiveness of these techniques include code coverage and fault detection, however, with the assumption that there is determinism in the resulting outputs. In this paper we examine the extent to which a common set of factors such as the system platform, Java version, application starting state and tool harness configurations impact these metrics. We examine three layers of testing outputs: the code layer, the behavioral (or invariant) layer and the external (or user interaction) layer. In a study using five open source applications across three operating system platforms, manipulating several factors, we observe as many as 184 lines of code coverage difference between runs using the same test cases, and up to 96 percent false positives with respect to fault detection. We also see some a small variation among the invariants inferred. Despite our best efforts, we can reduce, but not completely eliminate all possible variation in the output. We use our findings to provide a set of best practices that should lead to better consistency and smaller differences in test outcomes, allowing more repeatable and reliable testing and experimentation.},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	author = {Gao, Zebao and Liang, Yalan and Cohen, Myra B. and Memon, Atif M. and Wang, Zhen},
	month = may,
	year = {2015},
	note = {ISSN: 1558-1225},
	keywords = {Testing, public domain software, software fault tolerance, Java, operating systems (computers), Entropy, Graphical user interfaces, program testing, benchmarking, code coverage, Delays, experimentation, fault detection, graphical user interface, graphical user interfaces, GUI, open source application, operating system platform, Operating systems, software testing, user interactive application testing, ToRead},
	pages = {55--65},
	file = {IEEE Xplore Full Text PDF:/Users/sarra.habchi/Zotero/storage/CLVP5M2V/Gao et al. - 2015 - Making System User Interactive Tests Repeatable W.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/sarra.habchi/Zotero/storage/J2PR6FIX/7194561.html:text/html}
}

@inproceedings{gambi_practical_2018,
	title = {Practical {Test} {Dependency} {Detection}},
	doi = {10.1109/ICST.2018.00011},
	abstract = {Regression tests should consistently produce the same outcome when executed against the same version of the system under test. Recent studies, however, show a different picture: in many cases simply changing the order in which tests execute is enough to produce different test outcomes. These studies also identify the presence of dependencies between tests as one likely cause of this behavior. Test dependencies affect the quality of tests and of the correlated development activities, like regression test selection, prioritization, and parallelization, which assume that tests are independent. Therefore, developers must promptly identify and resolve problematic test dependencies. This paper presents PRADET, a novel approach for detecting problematic dependencies that is both effective and efficient. PRADET uses a systematic, data-driven process to detect problematic test dependencies significantly faster and more precisely than prior work. PRADET scales to analyze large projects with thousands of tests that existing tools cannot analyze in reasonable amount of time, and found 27 previously unknown dependencies.},
	booktitle = {2018 {IEEE} 11th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Gambi, Alessio and Bell, Jonathan and Zeller, Andreas},
	month = apr,
	year = {2018},
	keywords = {Testing, regression analysis, Computer bugs, Tools, empirical study, program testing, flaky tests, data-flow, detection algorithm, Minimization, Out of order, Pollution, practical test dependency detection, PRADET, problematic test dependencies, regression test selection, regression tests, Test dependence},
	pages = {1--11},
	file = {IEEE Xplore Full Text PDF:/Users/sarra.habchi/Zotero/storage/IKYMCY2B/Gambi et al. - 2018 - Practical Test Dependency Detection.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/sarra.habchi/Zotero/storage/6NATPTRA/8367031.html:text/html}
}

@inproceedings{lam_idflakies_2019,
	title = {{iDFlakies}: {A} {Framework} for {Detecting} and {Partially} {Classifying} {Flaky} {Tests}},
	shorttitle = {{iDFlakies}},
	doi = {10.1109/ICST.2019.00038},
	abstract = {Regression testing is increasingly important with the wide use of continuous integration. A desirable requirement for regression testing is that a test failure reliably indicates a problem in the code under test and not a false alarm from the test code or the testing infrastructure. However, some test failures are unreliable, stemming from flaky tests that can nondeterministically pass or fail for the same code under test. There are many types of flaky tests, with order-dependent tests being a prominent type. To help advance research on flaky tests, we present (1) a framework, iDFlakies, to detect and partially classify flaky tests; (2) a dataset of flaky tests in open-source projects; and (3) a study with our dataset. iDFlakies automates experimentation with our tool for Maven-based Java projects. Using iDFlakies, we build a dataset of 422 flaky tests, with 50.5\% order-dependent and 49.5\% not. Our study of these flaky tests finds the prevalence of two types of flaky tests, probability of a test-suite run to have at least one failure due to flaky tests, and how different test reorderings affect the number of detected flaky tests. We envision that our work can spur research to alleviate the problem of flaky tests.},
	booktitle = {2019 12th {IEEE} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	author = {Lam, Wing and Oei, Reed and Shi, August and Marinov, Darko and Xie, Tao},
	month = apr,
	year = {2019},
	note = {ISSN: 2159-4848},
	keywords = {Testing, regression analysis, Tools, Open source software, Java, program testing, flaky tests, Concurrent computing, iDFlakies, Maven-based Java projects, order dependent tests, probability, regression testing, Reliability, Servers, test failure, testing infrastructure},
	pages = {312--322},
	file = {IEEE Xplore Full Text PDF:/Users/sarra.habchi/Zotero/storage/U3R4L6YR/Lam et al. - 2019 - iDFlakies A Framework for Detecting and Partially.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/sarra.habchi/Zotero/storage/IBAGTYYT/8730188.html:text/html}
}

@inproceedings{zhang_empirically_2014,
	address = {San Jose, CA, USA},
	series = {{ISSTA} 2014},
	title = {Empirically revisiting the test independence assumption},
	isbn = {978-1-4503-2645-2},
	url = {https://doi.org/10.1145/2610384.2610404},
	doi = {10.1145/2610384.2610404},
	abstract = {In a test suite, all the test cases should be independent: no test should affect any other test’s result, and running the tests in any order should produce the same test results. Techniques such as test prioritization generally assume that the tests in a suite are independent. Test dependence is a little-studied phenomenon. This paper presents five results related to test dependence. First, we characterize the test dependence that arises in practice. We studied 96 real-world dependent tests from 5 issue tracking systems. Our study shows that test dependence can be hard for programmers to identify. It also shows that test dependence can cause non-trivial consequences, such as masking program faults and leading to spurious bug reports. Second, we formally define test dependence in terms of test suites as ordered sequences of tests along with explicit environments in which these tests are executed. We formulate the problem of detecting dependent tests and prove that a useful special case is NP-complete. Third, guided by the study of real-world dependent tests, we propose and compare four algorithms to detect dependent tests in a test suite. Fourth, we applied our dependent test detection algorithms to 4 real-world programs and found dependent tests in each human-written and automatically-generated test suite. Fifth, we empirically assessed the impact of dependent tests on five test prioritization techniques. Dependent tests affect the output of all five techniques; that is, the reordered suite fails even though the original suite did not.},
	urldate = {2020-07-02},
	booktitle = {Proceedings of the 2014 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Sai and Jalali, Darioush and Wuttke, Jochen and Muşlu, Kıvanç and Lam, Wing and Ernst, Michael D. and Notkin, David},
	month = jul,
	year = {2014},
	keywords = {detection algorithms, empirical studies, Test dependence},
	pages = {385--396},
	file = {Full Text PDF:/Users/sarra.habchi/Zotero/storage/ARWP2DLH/Zhang et al. - 2014 - Empirically revisiting the test independence assum.pdf:application/pdf}
}

@inproceedings{huo_improving_2014,
	address = {Hong Kong, China},
	series = {{FSE} 2014},
	title = {Improving oracle quality by detecting brittle assertions and unused inputs in tests},
	isbn = {978-1-4503-3056-5},
	url = {https://doi.org/10.1145/2635868.2635917},
	doi = {10.1145/2635868.2635917},
	abstract = {Writing oracles is challenging. As a result, developers often create oracles that check too little, resulting in tests that are unable to detect failures, or check too much, resulting in tests that are brittle and difficult to maintain. In this paper we present a new technique for automatically analyzing test oracles. The technique is based on dynamic tainting and detects both brittle assertions—assertions that depend on values that are derived from uncontrolled inputs—and unused inputs—inputs provided by the test that are not checked by an assertion. We also presented OraclePolish, an implementation of the technique that can analyze tests that are written in Java and use the JUnit testing framework. Using OraclePolish, we conducted an empirical evaluation of more than 4000 real test cases. The results of the evaluation show that OraclePolish is effective; it detected 164 tests that contain brittle assertions and 1618 tests that have unused inputs. In addition, the results also demonstrate that the costs associated with using the technique are reasonable.},
	urldate = {2020-07-02},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Huo, Chen and Clause, James},
	month = nov,
	year = {2014},
	keywords = {Brittleness, Dynamic tainting, Improving oracles, Mutation, Unit testing, Unused inputs},
	pages = {621--631},
	file = {Full Text PDF:/Users/sarra.habchi/Zotero/storage/DXQXRW55/Huo and Clause - 2014 - Improving oracle quality by detecting brittle asse.pdf:application/pdf}
}

@inproceedings{gyori_reliable_2015,
	address = {Baltimore, MD, USA},
	series = {{ISSTA} 2015},
	title = {Reliable testing: detecting state-polluting tests to prevent test dependency},
	isbn = {978-1-4503-3620-8},
	shorttitle = {Reliable testing},
	url = {https://doi.org/10.1145/2771783.2771793},
	doi = {10.1145/2771783.2771793},
	abstract = {Writing reliable test suites for large object-oriented systems is complex and time consuming. One common cause of unreliable test suites are test dependencies that can cause tests to fail unexpectedly, not exposing bugs in the code under test but in the test code itself. Prior research has shown that the main reason for test dependencies is the ``pollution'' of state shared across tests. We propose a technique, called , for finding tests that pollute the shared state. In a nutshell, finds tests that modify some location on the heap shared across tests or on the file system; a subsequent test could fail if it assumes the shared location to have the initial value before the state was modified. To aid in inspecting the pollutions, provides an access path through the heap that leads to the polluted value or the name of the file that was modified. We implemented a prototype tool for Java and evaluated it on NumOfProjects projects, with a total of NumOfTests tests. Diaper reported PollutingTests , and our inspection found that NumOfTPsSpace of those are relevant pollutions that can easily affect other tests.},
	urldate = {2020-07-02},
	booktitle = {Proceedings of the 2015 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Gyori, Alex and Shi, August and Hariri, Farah and Marinov, Darko},
	month = jul,
	year = {2015},
	keywords = {Flaky Tests, State Pollution, Test Dependency},
	pages = {223--233},
	file = {Full Text PDF:/Users/sarra.habchi/Zotero/storage/7XYRBP47/Gyori et al. - 2015 - Reliable testing detecting state-polluting tests .pdf:application/pdf}
}

@misc{noauthor_unit_nodate,
	title = {Unit test virtualization with {VMVM} {\textbar} {Proceedings} of the 36th {International} {Conference} on {Software} {Engineering}},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568248},
	urldate = {2020-07-02},
	file = {Unit test virtualization with VMVM | Proceedings of the 36th International Conference on Software Engineering:/Users/sarra.habchi/Zotero/storage/DY87A5NG/2568225.html:text/html}
}

@inproceedings{muslu_finding_2011,
	address = {Szeged, Hungary},
	series = {{ESEC}/{FSE} '11},
	title = {Finding bugs by isolating unit tests},
	isbn = {978-1-4503-0443-6},
	url = {https://doi.org/10.1145/2025113.2025202},
	doi = {10.1145/2025113.2025202},
	abstract = {Even in simple programs there are hidden assumptions and dependencies between units that are not immediately visible in each involved unit. These dependencies are generally hard to identify and locate, and can lead to subtle faults that are often missed, even by extensive test suites. We propose to leverage existing test suites to identify faults due to hidden dependencies and to identify inadequate test suite design. Rather than just executing entire test suites within frameworks such as JUnit, we execute each test in isolation, thus removing masking effects that might be present in the test suites. We hypothesize that this can reveal previously hidden dependencies between program units or tests. A preliminary study shows that this technique is capable of identifying subtle faults that have lived in a system for 120 revisions, despite failures being reported and despite attempts to fix the fault.},
	urldate = {2020-07-02},
	booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th {European} conference on {Foundations} of software engineering},
	publisher = {Association for Computing Machinery},
	author = {Muşlu, Kivanç and Soran, Bilge and Wuttke, Jochen},
	month = sep,
	year = {2011},
	keywords = {data dependency, test isolation, test reuse, testing},
	pages = {496--499},
	file = {Full Text PDF:/Users/sarra.habchi/Zotero/storage/PDWJIPF6/Muşlu et al. - 2011 - Finding bugs by isolating unit tests.pdf:application/pdf}
}

@inproceedings{bell_deflaker_2018,
	title = {{DeFlaker}: {Automatically} {Detecting} {Flaky} {Tests}},
	shorttitle = {{DeFlaker}},
	doi = {10.1145/3180155.3180164},
	abstract = {Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle. We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96 Java projects on TravisCI, and found 87 previously unknown flaky tests in 10 of these projects. We also ran experiments on project histories, where DeFlaker detected 1,874 flaky tests from 4,846 failures, with a low false alarm rate (1.5\%). DeFlaker had a higher recall (95.5\% vs. 23\%) of confirmed flaky tests than Maven's default flaky test detector.},
	booktitle = {2018 {IEEE}/{ACM} 40th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko},
	month = may,
	year = {2018},
	note = {ISSN: 1558-1225},
	keywords = {Testing, Software, Monitoring, regression analysis, public domain software, Tools, Detectors, Java, program testing, flaky tests, code coverage, software testing, test failure, DeFlaker, Syntactics, unknown flaky tests},
	pages = {433--444},
	file = {IEEE Xplore Abstract Record:/Users/sarra.habchi/Zotero/storage/F8IRT667/8453104.html:text/html}
}

@article{parry_flake_nodate,
	title = {Flake {It} ‘{Till} {You} {Make} {It}: {Using} {Automated} {Repair} to {Induce} and {Fix} {Latent} {Test} {Flakiness}},
	abstract = {Since flaky tests pass or fail nondeterministically, without any code changes, they are an unreliable indicator of program quality. Developers may quarantine or delete flaky tests because it is often too time consuming to repair them. Yet, since decommissioning too many tests may ultimately degrade a test suite’s effectiveness, developers may eventually want to fix them, a process that is challenging because the nondeterminism may have been introduced previously. We contend that the best time to discover and repair a flaky test is when a developer first creates and best understands it. We refer to tests that are not currently flaky, but that could become so, as having latent flakiness. We further argue that efforts to expose and repair latent flakiness are valuable in ensuring the future-reliability of the test suite, and that the testing cost is greater if latent flakiness is left to manifest itself later. Using concrete examples from a real-world program, this paper posits that automated program repair techniques will prove useful for surfacing latent flakiness.},
	language = {en},
	author = {Parry, Owain and Kapfhammer, Gregory M and Hilton, Michael and McMinn, Phil},
	keywords = {ToRead},
	pages = {2},
	file = {Parry et al. - Flake It ‘Till You Make It Using Automated Repair.pdf:/Users/sarra.habchi/Zotero/storage/I66RTD7C/Parry et al. - Flake It ‘Till You Make It Using Automated Repair.pdf:application/pdf}
}

@article{lam_study_2020,
	title = {A study on the lifecycle of flaky tests},
	abstract = {During regression testing, developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality. Thus, flaky tests, which nondeterministically pass or fail on the same code, are problematic because they provide misleading signals during regression testing. Although flaky tests are the focus of several existing studies, none of them study (1) the reoccurrence, runtimes, and time-before-fix of flaky tests, and (2) flaky tests indepth on proprietary projects.},
	language = {en},
	author = {Lam, Wing and Muşlu, Kıvanç},
	year = {2020},
	pages = {12},
	file = {Lam and Muşlu - 2020 - A Study on the Lifecycle of Flaky Tests.pdf:/Users/sarra.habchi/Zotero/storage/6TE776M3/Lam and Muşlu - 2020 - A Study on the Lifecycle of Flaky Tests.pdf:application/pdf}
}

@article{moran_flakyloc_2020,
	title = {{FlakyLoc}: {Flakiness} {Localization} for {Reliable} {Test} {Suites} in {Web} {Applications}},
	copyright = {Copyright (c) 2020},
	issn = {1544-5976},
	shorttitle = {{FlakyLoc}},
	url = {https://journals.riverpublishers.com/index.php/JWE/},
	doi = {10.13052/jwe1540-9589.1927},
	abstract = {Web application testing is a great challenge due to the management of complex asynchronous communications, the concurrency between the clients-servers, and the heterogeneity of resources employed. It is difficult to ensure that a test case is re-running in the same conditions because it can be executed in undesirable ways according to several environmental factors that are not easy to fine-grain control such as network bottlenecks, memory issues or screen resolution. These environmental factors can cause flakiness, which occurs when the same test case sometimes obtains one test outcome and other times another outcome in the same application due to the execution of environmental factors. The tester usually stops relying on flaky test cases because their outcome varies during the re-executions. To fix and reduce the flakiness it is very important to locate and understand which environmental factors cause the flakiness. This paper is focused on the localization of the root cause of flakiness in web applications based on the characterization of the different environmental factors that are not controlled during testing. The root cause of flakiness is located by means of spectrum-based localization techniques that analyse the test execution under different combinations of the environmental factors that can trigger the flakiness. This technique is evaluated with an educational web platform called FullTeaching. As a result, our technique was able to locate automatically the root cause of flakiness and provide enough information to both understand it and fix it.},
	language = {en},
	urldate = {2020-07-02},
	journal = {Journal of Web Engineering},
	author = {Morán, Jesús and Augusto, Cristian and Bertolino, Antonia and Riva, Claudio De La and Tuya, Javier},
	month = jun,
	year = {2020},
	keywords = {Software testing and debugging, spectrum-based localization, test flakiness, web applications},
	pages = {267--296.--267--296.},
	file = {Snapshot:/Users/sarra.habchi/Zotero/storage/AI68FTNG/3361.html:text/html}
}

@article{dong_concurrency-related_2020,
	title = {Concurrency-related {Flaky} {Test} {Detection} in {Android} apps},
	url = {http://arxiv.org/abs/2005.10762},
	abstract = {Validation of Android apps via testing is difficult owing to the presence of flaky tests. Due to non-deterministic execution environments, a sequence of events (a test) may lead to success or failure in unpredictable ways. In this work, we present an approach and tool FlakeShovel for detecting flaky tests through systematic exploration of event orders. Our key observation is that for a test in a mobile app, there is a testing framework thread which creates the test events, a main User-Interface (UI) thread processing these events, and there may be several other background threads running asynchronously. For any event e whose execution involves potential non-determinism, we localize the earliest (latest) event after (before) which e must happen.We then efficiently explore the schedules between the upper/lower bound events while grouping events within a single statement, to find whether the test outcome is flaky. We also create a suite of subject programs called DroidFlaker to study flaky tests in Android apps. Our experiments on subject-suite DroidFlaker demonstrate the efficacy of our flaky test detection. Our work is complementary to existing flaky test detection tools like Deflaker which check only failing tests. FlakeShovel can detect flaky tests among passing tests, as shown by our approach and experiments.},
	urldate = {2020-07-02},
	journal = {arXiv:2005.10762 [cs]},
	author = {Dong, Zhen and Tiwari, Abhishek and Yu, Xiao Liang and Roychoudhury, Abhik},
	month = may,
	year = {2020},
	note = {arXiv: 2005.10762},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/sarra.habchi/Zotero/storage/8W2U6V2D/Dong et al. - 2020 - Concurrency-related Flaky Test Detection in Androi.pdf:application/pdf;arXiv.org Snapshot:/Users/sarra.habchi/Zotero/storage/J3Z7FRVH/2005.html:text/html}
}

@inproceedings{eck_understanding_2019,
	address = {Tallinn, Estonia},
	series = {{ESEC}/{FSE} 2019},
	title = {Understanding flaky tests: the developer’s perspective},
	isbn = {978-1-4503-5572-8},
	shorttitle = {Understanding flaky tests},
	url = {https://doi.org/10.1145/3338906.3338945},
	doi = {10.1145/3338906.3338945},
	abstract = {Flaky tests are software tests that exhibit a seemingly random outcome (pass or fail) despite exercising unchanged code. In this work, we examine the perceptions of software developers about the nature, relevance, and challenges of flaky tests. We asked 21 professional developers to classify 200 flaky tests they previously fixed, in terms of the nature and the origin of the flakiness, as well as of the fixing effort. We also examined developers' fixing strategies. Subsequently, we conducted an online survey with 121 developers with a median industrial programming experience of five years. Our research shows that: The flakiness is due to several different causes, four of which have never been reported before, despite being the most costly to fix; flakiness is perceived as significant by the vast majority of developers, regardless of their team's size and project's domain, and it can have effects on resource allocation, scheduling, and the perceived reliability of the test suite; and the challenges developers report to face regard mostly the reproduction of the flaky behavior and the identification of the cause for the flakiness. Public preprint [http://arxiv.org/abs/1907.01466], data and materials [https://doi.org/10.5281/zenodo.3265785].},
	urldate = {2020-07-02},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Eck, Moritz and Palomba, Fabio and Castelluccio, Marco and Bacchelli, Alberto},
	month = aug,
	year = {2019},
	keywords = {Empirical Studies, Flaky Tests, Mixed-Method Research},
	pages = {830--840},
	file = {Full Text PDF:/Users/sarra.habchi/Zotero/storage/F8PICAV8/Eck et al. - 2019 - Understanding flaky tests the developer’s perspec.pdf:application/pdf}
}

@misc{noauthor_eradicating_nodate,
	title = {Eradicating {Non}-{Determinism} in {Tests}},
	url = {https://martinfowler.com/articles/nonDeterminism.html},
	abstract = {Flaky tests can ruin your test suite, so you must quarantine them now. But then go on to solve their non-deterministic behavior},
	urldate = {2020-07-02},
	journal = {martinfowler.com},
	note = {Library Catalog: martinfowler.com},
	file = {Snapshot:/Users/sarra.habchi/Zotero/storage/3VILGP2B/nonDeterminism.html:text/html}
}

@inproceedings{dutta_detecting_2020,
	address = {Virtual Event USA},
	title = {Detecting flaky tests in probabilistic and machine learning applications},
	isbn = {978-1-4503-8008-9},
	url = {https://dl.acm.org/doi/10.1145/3395363.3397366},
	doi = {10.1145/3395363.3397366},
	abstract = {Probabilistic programming systems and machine learning frameworks like Pyro, PyMC3, TensorFlow, and PyTorch provide scalable and efficient primitives for inference and training. However, such operations are non-deterministic. Hence, it is challenging for developers to write tests for applications that depend on such frameworks, often resulting in flaky tests – tests which fail nondeterministically when run on the same version of code. In this paper, we conduct the first extensive study of flaky tests in this domain. In particular, we study the projects that depend on four frameworks: Pyro, PyMC3, TensorFlow-Probability, and PyTorch. We identify 75 bug reports/commits that deal with flaky tests, and we categorize the common causes and fixes for them. This study provides developers with useful insights on dealing with flaky tests in this domain. Motivated by our study, we develop a technique, FLASH, to systematically detect flaky tests due to assertions passing and failing in different runs on the same code. These assertions fail due to differences in the sequence of random numbers in different runs of the same test. FLASH exposes such failures, and our evaluation on 20 projects results in 11 previously-unknown flaky tests that we reported to developers.},
	language = {en},
	urldate = {2020-07-14},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Dutta, Saikat and Shi, August and Choudhary, Rutvik and Zhang, Zhekun and Jain, Aryaman and Misailovic, Sasa},
	month = jul,
	year = {2020},
	pages = {211--224},
	file = {Dutta et al. - 2020 - Detecting flaky tests in probabilistic and machine.pdf:/Users/sarra.habchi/Zotero/storage/S72PPFIP/Dutta et al. - 2020 - Detecting flaky tests in probabilistic and machine.pdf:application/pdf}
}

@article{vaidhyam_subramanian_quantifying_2020,
	title = {Quantifying, {Characterizing}, and {Mitigating} {Flakily} {Covered} {Program} {Elements}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9143477/},
	doi = {10.1109/TSE.2020.3010045},
	abstract = {Code coverage measures the degree to which source code elements (e.g., statements, branches) are invoked during testing. Despite growing evidence that coverage is a problematic measurement, it is often used to make decisions about where testing effort should be invested. For example, using coverage as a guide, tests should be written to invoke the non-covered program elements. At their core, coverage measurements assume that invocation of a program element during any test is equally valuable. Yet in reality, some tests are more robust than others. As a concrete instance of this, we posit in this paper that program elements that are only covered by ﬂaky tests, i.e., tests with non-deterministic behaviour, are also worthy of investment of additional testing effort. In this paper, we set out to quantify, characterize, and mitigate “ﬂakily covered” program elements (i.e., those elements that are only covered by ﬂaky tests). To that end, we perform an empirical study of three large software systems from the OpenStack community. In terms of quantiﬁcation, we ﬁnd that systems are disproportionately impacted by ﬂakily covered statements with 5\% and 10\% of the covered statements in Nova and Neutron being ﬂakily covered, respectively, while {\textless} 1\% of Cinder statements are ﬂakily covered. In terms of characterization, we ﬁnd that incidences of ﬂakily covered statements could not be well explained by solely using code characteristics, such as dispersion, ownership, and development activity. In terms of mitigation, we propose GreedyFlake – a test effort prioritization algorithm to maximize return on investment when tackling the problem of ﬂakily covered program elements. We ﬁnd that GreedyFlake outperforms baseline approaches by at least eight percentage points of Area Under the Cost Effectiveness Curve.},
	language = {en},
	urldate = {2020-07-21},
	journal = {IEEE Transactions on Software Engineering},
	author = {Vaidhyam Subramanian, Shivashree Vysali and McIntosh, Shane and Adams, Bram},
	year = {2020},
	pages = {1--1},
	file = {Vaidhyam Subramanian et al. - 2020 - Quantifying, Characterizing, and Mitigating Flakil.pdf:/Users/sarra.habchi/Zotero/storage/WKRVSRDT/Vaidhyam Subramanian et al. - 2020 - Quantifying, Characterizing, and Mitigating Flakil.pdf:application/pdf}
}

@inproceedings{lam_dependent-test-aware_2020,
	address = {Virtual Event USA},
	title = {Dependent-test-aware regression testing techniques},
	isbn = {978-1-4503-8008-9},
	url = {https://dl.acm.org/doi/10.1145/3395363.3397364},
	doi = {10.1145/3395363.3397364},
	abstract = {Developers typically rely on regression testing techniques to ensure that their changes do not break existing functionality. Unfortunately, regression testing techniques suffer from flaky tests, which can both pass and fail when run multiple times on the same version of code and tests. One prominent type of flaky tests is order-dependent (OD) tests, which are tests that pass when run in one order but fail when run in another order. Although OD tests may cause flaky-test failures, OD tests can help developers run their tests faster by allowing them to share resources. We propose to make regression testing techniques dependent-test-aware to reduce flaky-test failures. To understand the necessity of dependent-test-aware regression testing techniques, we conduct the first study on the impact of OD tests on three regression testing techniques: test prioritization, test selection, and test parallelization. In particular, we implement 4 test prioritization, 6 test selection, and 2 test parallelization algorithms, and we evaluate them on 11 Java modules with OD tests. When we run the orders produced by the traditional, dependent-test-unaware regression testing algorithms, 82\% of human-written test suites and 100\% of automatically-generated test suites with OD tests have at least one flaky-test failure.},
	language = {en},
	urldate = {2020-07-28},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Lam, Wing and Shi, August and Oei, Reed and Zhang, Sai and Ernst, Michael D. and Xie, Tao},
	month = jul,
	year = {2020},
	pages = {298--311},
	file = {Lam et al. - 2020 - Dependent-test-aware regression testing techniques.pdf:/Users/sarra.habchi/Zotero/storage/SPL6LKZW/Lam et al. - 2020 - Dependent-test-aware regression testing techniques.pdf:application/pdf}
}

@misc{liviu_machine_2019,
	title = {A machine learning solution for detecting and mitigating flaky tests},
	url = {https://medium.com/fitbit-tech-blog/a-machine-learning-solution-for-detecting-and-mitigating-flaky-tests-c5626ca7e853},
	abstract = {If you are a developer you’ve probably experienced flaky tests or flakes and the first thing that comes to your mind is either frustration…},
	language = {en},
	urldate = {2020-07-30},
	journal = {Medium},
	author = {Liviu, Serban},
	month = oct,
	year = {2019},
	note = {Library Catalog: medium.com},
	file = {Snapshot:/Users/sarra.habchi/Zotero/storage/3J5XX7J2/a-machine-learning-solution-for-detecting-and-mitigating-flaky-tests-c5626ca7e853.html:text/html}
}

@misc{noauthor_predicting_nodate,
	title = {Predicting {Failing} {Tests} with {Machine} {Learning}},
	url = {https://www.infoq.com/news/2020/05/predicting-failing-tests/},
	abstract = {Machine learning can be used to predict how tests behave on changes in the code. These predictions reduce the feedback time to developers by providing information at check-in time. Marco Achtziger \& Dr. Gregor Endler presented how they are using machine learning to learn from failures at OOP 2020.},
	language = {en},
	urldate = {2020-07-30},
	journal = {InfoQ},
	note = {Library Catalog: www.infoq.com},
	file = {Snapshot:/Users/sarra.habchi/Zotero/storage/QJJCVJQY/predicting-failing-tests.html:text/html}
}

@article{machalica_predictive_2019,
	title = {Predictive {Test} {Selection}},
	url = {http://arxiv.org/abs/1810.05286},
	abstract = {Change-based testing is a key component of continuous integration at Facebook. However, a large number of tests coupled with a high rate of changes committed to our monolithic repository make it infeasible to run all potentially-impacted tests on each change. We propose a new predictive test selection strategy which selects a subset of tests to exercise for each change submitted to the continuous integration system. The strategy is learned from a large dataset of historical test outcomes using basic machine learning techniques. Deployed in production, the strategy reduces the total infrastructure cost of testing code changes by a factor of two, while guaranteeing that over 95\% of individual test failures and over 99.9\% of faulty changes are still reported back to developers. The method we present here also accounts for the non-determinism of test outcomes, also known as test flakiness.},
	urldate = {2020-07-30},
	journal = {arXiv:1810.05286 [cs]},
	author = {Machalica, Mateusz and Samylkin, Alex and Porth, Meredith and Chandra, Satish},
	month = may,
	year = {2019},
	note = {arXiv: 1810.05286},
	keywords = {Computer Science - Software Engineering},
	annote = {Comment: Camera-ready},
	file = {arXiv Fulltext PDF:/Users/sarra.habchi/Zotero/storage/CAG8BD46/Machalica et al. - 2019 - Predictive Test Selection.pdf:application/pdf;arXiv.org Snapshot:/Users/sarra.habchi/Zotero/storage/ZN63FERZ/1810.html:text/html}
}

@misc{noauthor_testing_nodate,
	title = {Testing {Firefox} more efficiently with machine learning – {Mozilla} {Hacks} - the {Web} developer blog},
	url = {https://hacks.mozilla.org/2020/07/testing-firefox-more-efficiently-with-machine-learning},
	abstract = {A browser is an enormously complex piece of software, and it's always in development. About a year ago, we asked ourselves: how could we do better? Our CI relied heavily ...},
	language = {en-US},
	urldate = {2020-07-30},
	journal = {Mozilla Hacks – the Web developer blog},
	note = {Library Catalog: hacks.mozilla.org},
	file = {Snapshot:/Users/sarra.habchi/Zotero/storage/WK79639D/testing-firefox-more-efficiently-with-machine-learning.html:text/html}
}

@inproceedings{king_towards_2018,
	title = {Towards a {Bayesian} {Network} {Model} for {Predicting} {Flaky} {Automated} {Tests}},
	doi = {10.1109/QRS-C.2018.00031},
	abstract = {Artificial intelligence and machine learning are making it possible for computers to diagnose some medical diseases more accurately than doctors. Such systems analyze millions of patient records and make generalizations to diagnose new patients. A key challenge is determining whether a patient's symptoms are attributed to a known disease or other factors. Software testers face a similar problem when troubleshooting automation failures. They investigate questions like: Is a given failure due to a defect, environmental issue, or flaky test? Flaky tests exhibit both passing and failing results although neither the code nor test has changed. Maintaining flaky tests is costly, especially in large-scale software projects. In this paper, we present an approach that leverages Bayesian networks for classifying and predicting flaky tests. Our approach views the test flakiness problem as a disease by specifying its symptoms and possible causes. Preliminary results from a case study suggest the approach is feasible.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {King, Tariq M. and Santiago, Dionny and Phillips, Justin and Clarke, Peter J.},
	month = jul,
	year = {2018},
	keywords = {flaky test, Measurement, Software, software maintenance, learning (artificial intelligence), machine learning, program testing, belief networks, Bayes methods, flaky tests, Artificial intelligence, automation failures, Bayesian network model, diseases, Diseases, known disease, medical diagnostic computing, medical diseases, patient records, predicting flaky automated tests, Software testing, test flakiness problem, testing, automation, machine learning, flaky, model, troubleshooting automation failures},
	pages = {100--107},
	file = {IEEE Xplore Abstract Record:/Users/sarra.habchi/Zotero/storage/HQDL65C9/8431959.html:text/html}
}

@article{ahmad_empirical_2019,
	title = {Empirical {Analysis} of {Factors} and their {Effect} on {Test} {Flakiness} - {Practitioners}' {Perceptions}},
	url = {http://arxiv.org/abs/1906.00673},
	abstract = {Developers always wish to ensure that their latest changes to the code base do not break existing functionality. If test cases fail, they expect these failures to be connected to the submitted changes. Unfortunately, a flaky test can be the reason for a test failure. Developers spend time to relate possible test failures to the submitted changes only to find out that the cause for these failures is test flakiness. The dilemma of an identification of the real failures or flaky test failures affects developers' perceptions about what is test flakiness. Prior research on test flakiness has been limited to test smells and tools to detect test flakiness. In this paper, we have conducted a multiple case study with four different industries in Scandinavia to understand practitioners' perceptions about test flakiness and how this varies between industries. We observed that there are little differences in how the practitioners perceive test flakiness. We identified 23 factors that are perceived to affect test flakiness. These perceived factors are categorized as 1) Software test quality, 2) Software Quality, 3) Actual Flaky test and 4) Company-specific factors. We have studied the nature of effects such as whether factors increase, decrease or affect the ability to detect test flakiness. We validated our findings with different participants of the 4 companies to avoid biases. The average agreement rate of the identified factors and their effects are 86\% and 86\% respectively, among participants.},
	urldate = {2020-07-30},
	journal = {arXiv:1906.00673 [cs]},
	author = {Ahmad, Azeem and Leifler, Ola and Sandahl, Kristian},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.00673},
	keywords = {Computer Science - Software Engineering},
	annote = {Comment: 12 Pages},
	file = {arXiv Fulltext PDF:/Users/sarra.habchi/Zotero/storage/7SKU6PWK/Ahmad et al. - 2019 - Empirical Analysis of Factors and their Effect on .pdf:application/pdf;arXiv.org Snapshot:/Users/sarra.habchi/Zotero/storage/H7FPN6BA/1906.html:text/html}
}

@misc{noauthor_test_nodate,
	title = {Test {Armada}},
	url = {http://testarmada.io/},
	urldate = {2020-07-30},
	file = {Test Armada:/Users/sarra.habchi/Zotero/storage/8W5ZPKJ5/testarmada.io.html:text/html}
}

@article{parry_flake_nodate-1,
	title = {Flake {It} ‘{Till} {You} {Make} {It}: {Using} {Automated} {Repair} to {Induce} and {Fix} {Latent} {Test} {Flakiness}},
	abstract = {Since flaky tests pass or fail nondeterministically, without any code changes, they are an unreliable indicator of program quality. Developers may quarantine or delete flaky tests because it is often too time consuming to repair them. Yet, since decommissioning too many tests may ultimately degrade a test suite’s effectiveness, developers may eventually want to fix them, a process that is challenging because the nondeterminism may have been introduced previously. We contend that the best time to discover and repair a flaky test is when a developer first creates and best understands it. We refer to tests that are not currently flaky, but that could become so, as having latent flakiness. We further argue that efforts to expose and repair latent flakiness are valuable in ensuring the future-reliability of the test suite, and that the testing cost is greater if latent flakiness is left to manifest itself later. Using concrete examples from a real-world program, this paper posits that automated program repair techniques will prove useful for surfacing latent flakiness.},
	language = {en},
	author = {Parry, Owain and Kapfhammer, Gregory M and Hilton, Michael and McMinn, Phil},
	pages = {2},
	file = {Parry et al. - Flake It ‘Till You Make It Using Automated Repair.pdf:/Users/sarra.habchi/Zotero/storage/LN4UL3ST/Parry et al. - Flake It ‘Till You Make It Using Automated Repair.pdf:application/pdf}
}

@article{moran_flakyloc_2020-1,
	title = {{FlakyLoc}: {Flakiness} {Localization} for {Reliable} {Test} {Suites} in {Web} {Applications}},
	issn = {1544-5976, 1540-9589},
	shorttitle = {{FlakyLoc}},
	url = {https://journals.riverpublishers.com/index.php/JWE/article/view/3361},
	doi = {10.13052/jwe1540-9589.1927},
	abstract = {Web application testing is a great challenge due to the management of complex asynchronous communications, the concurrency between the clients-servers, and the heterogeneity of resources employed. It is difficult to ensure that a test case is re-running in the same conditions because it can be executed in undesirable ways according to several environmental factors that are not easy to fine-grain control such as network bottlenecks, memory issues or screen resolution. These environmental factors can cause flakiness, which occurs when the same test case sometimes obtains one test outcome and other times another outcome in the same application due to the execution of environmental factors. The tester usually stops relying on flaky test cases because their outcome varies during the re-executions. To fix and reduce the flakiness it is very important to locate and understand which environmental factors cause the flakiness. This paper is focused on the localization of the root cause of flakiness in web applications based on the characterization of the different environmental factors that are not controlled during testing. The root cause of flakiness is located by means of spectrum-based localization techniques that analyse the test execution under different combinations of the environmental factors that can trigger the flakiness. This technique is evaluated with an educational web platform called FullTeaching. As a result, our technique was able to locate automatically the root cause of flakiness and provide enough information to both understand it and fix it.},
	language = {en},
	urldate = {2020-12-15},
	journal = {Journal of Web Engineering},
	author = {Morán, Jesús and Augusto, Cristian and Bertolino, Antonia and Riva, Claudio De La and Tuya, Javier},
	month = jun,
	year = {2020},
	file = {Morán et al. - 2020 - FlakyLoc Flakiness Localization for Reliable Test.pdf:/Users/sarra.habchi/Zotero/storage/GQCCIZEA/Morán et al. - 2020 - FlakyLoc Flakiness Localization for Reliable Test.pdf:application/pdf}
}

@book{tzoref-brill_instrumenting_2007,
	title = {Instrumenting where it hurts: {An} automatic concurrent debugging technique},
	shorttitle = {Instrumenting where it hurts},
	abstract = {As concurrent and distributive applications are becoming more common and debugging such applications is very dif- ficult, practical tools for automatic debugging of concurrent applications are in demand. In previous work, we applied automatic debugging to noise-based testing of concurrent programs. The idea of noise-based testing is to increase the probability of observing the bugs by adding, using instru- mentation, timing "noise" to the execution of the program. The technique of finding a small subset of points that causes the bug to manifest can be used as an automatic debugging technique. Previously, we showed that Delta Debugging can be used to pinpoint the bug location on some small pro- grams. In the work reported in this paper, we create and evaluate two algorithms for automatically pinpointing program loca- tions that are in the vicinity of the bugs on a number of in- dustrial programs. We discovered that the Delta Debugging algorithms do not scale due to the non-monotonic nature of the concurrent debugging problem. Instead we decided to try a machine learning feature selection algorithm. The idea is to consider each instrumentation point as a feature, execute the program many times with dierent instrumen- tations, and correlate the features (instrumentation points) with the executions in which the bug was revealed. This idea works very well when the bug is very hard to reveal using instrumentation, correlating to the case when a very specific timing window is needed to reveal the bug. However, in the more common case, when the bugs are easy to find using in- strumentation (i.e., instrumentation on many subsets finds the bugs), the correlation between the bug location and in- This work is partially supported by the European Com- munity under the Information Society Technologies (IST) programme of the 6th FP for RTD - project SHADOWS contract IST-035157. The authors are solely responsible for the content of this paper. It does not represent the opinion of the European Community, and the European Community is not responsible for any use that might be made of data appearing therein. strumentation points ranked high by the feature selection algorithm is not high enough. We show that for these cases, the important value is not the absolute value of the evalua- tion of the feature but the derivative of that value along the program execution path. As a number of groups expressed interest in this research, we built an open infrastructure for automatic debugging al- gorithms for concurrent applications, based on noise injec- tion based concurrent testing using instrumentation. The infrastructure is described in this paper.},
	author = {Tzoref-Brill, Rachel and Ur, Shmuel and Yom-Tov, Elad},
	month = jan,
	year = {2007},
	doi = {10.1145/1273463.1273469},
	note = {Journal Abbreviation: 2007 ACM International Symposium on Software Testing and Analysis, ISSTA'07
Pages: 38
Publication Title: 2007 ACM International Symposium on Software Testing and Analysis, ISSTA'07},
	file = {Full Text PDF:/Users/sarra.habchi/Zotero/storage/M37B72XI/Tzoref-Brill et al. - 2007 - Instrumenting where it hurts An automatic concurr.pdf:application/pdf}
}

@book{farchi_concurrent_2003,
	title = {Concurrent {Bug} {Patterns} and {How} to {Test} {Them}.},
	abstract = {We present and categorize a taxonomy of concurrent bug patterns. We then use the taxonomy to create new timing heuristics for ConTest. Initial industrial experience indi- cates that these heuristics improve the bug finding ability of ConTest. We also show how concurrent bug patterns can be derived from concurrent design patterns. Further research is required to complete the concurrent bug taxonomy and formal experiments are needed to show that heurisitics de- rived from the taxonomy improve the bug finding ability of ConTest.},
	author = {Farchi, Eitan and Nir, Yarden and Ur, Shmuel},
	month = jan,
	year = {2003},
	doi = {10.1109/IPDPS.2003.1213511},
	file = {Full Text PDF:/Users/sarra.habchi/Zotero/storage/4E46SZR8/Farchi et al. - 2003 - Concurrent Bug Patterns and How to Test Them..pdf:application/pdf}
}

@INPROCEEDINGS{haben-msr,  
author={Haben, Guillaume and Habchi, Sarra and Papadakis, Mike and Cordy, Maxime and Le Traon, Yves},  
booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},   
title={A Replication Study on the Usability of Code Vocabulary in Predicting Flaky Tests},
year={2021},  
volume={},  
number={},  
pages={219-229},  
doi={10.1109/MSR52588.2021.00034}}

@misc{Mozilla,
author = {MDN contributors},
title = {Test Verification - Mozilla | MDN},
howpublished = {\url{https://developer.mozilla.org/en-US/docs/Mozilla/QA/Test\_Verification}},
month = {3},
year = {2019},
note = {(Accessed on 01/12/2021)}
}
@article{Saha2014,
author = {Saha, Ripon K and Zhang, Lingming and Khurshid, Sarfraz and Perry, Dewayne E},
file = {:Users/guillaume.haben/Documents/Work/papers/informationRetrieval/icse2015.pdf:pdf},
journal = {FSE '14, Hongkong},
keywords = {information retrieval,regression testing,test prioritization},
title = {{REPiR : An Information Retrieval based Approach for Regression Test Prioritization}},
year = {2014}
}
@misc{Permutat82:online,
author = {},
title = {Permutation Importance vs Random Forest Feature Importance (MDI) - scikit-learn 0.24.0 documentation},
howpublished = {\url{https://scikit-learn.org/stable/auto\_examples//inspection/plot\_permutation\_importance.html}},
month = {},
year = {},
note = {(Accessed on 01/12/2021)}
}
@misc{GTAC2016,
author = {Micco, John \& Memon, Atif},
title = {GTAC 2016: How Flaky Tests in Continuous Integration - YouTube},
howpublished = {\url{https://www.youtube.com/watch?v=CrzpkF1-VsA}},
month = {12},
year = {2016},
note = {(Accessed on 01/12/2021)}
}
@misc{CI,
author = {Rehkopf, Max},
title = {What is Continuous Integration | Atlassian},
howpublished = {\url{https://www.atlassian.com/continuous-delivery/continuous-integration}},
month = {},
year = {},
note = {(Accessed on 01/12/2021)}
}
@misc{FlakinessSpotify,
author = {Palmer, Jason},
title = {Test Flakiness – Methods for identifying and dealing with flaky tests : Spotify Engineering},
howpublished = {\url{https://engineering.atspotify.com/2019/11/18/test-flakiness-methods-for-identifying-and-dealing-with-flaky-tests/}},
month = {11},
year = {2019},
note = {(Accessed on 01/12/2021)}
}
@misc{FlakinessGoogle,
author = {Listfield, Jeff},
title = {Google Testing Blog: Where do our flaky tests come from?},
howpublished = {\url{https://testing.googleblog.com/2017/04/where-do-our-flaky-tests-come-from.html}},
month = {4},
year = {2017},
note = {(Accessed on 01/12/2021)}
}
@misc{unorderedCollectionsStackOverflow,
author = {Barnert, Andrew},
title = {python - How should I test that dictionaries will always be in the same order? - Stack Overflow},
howpublished = {\url{https://stackoverflow.com/questions/50475966/how-should-i-test-that-dictionaries-will-always-be-in-the-same-order/50476093\#50476093}},
month = {5},
year = {2018},
note = {(Accessed on 01/12/2021)}
}
@misc{PythonDoc,
author = {docs.python.org},
title = {What’s New In Python 3.1 - Python 3.9.1 documentation},
howpublished = {\url{https://docs.python.org/3/whatsnew/3.1.html\#pep-372-ordered-dictionaries}},
month = {2},
year = {2021},
note = {(Accessed on 01/12/2021)}
}

@inproceedings{LeongSPTM19,
  author    = {Claire Leong and
               Abhayendra Singh and
               Mike Papadakis and
               Yves Le Traon and
               John Micco},
  editor    = {Helen Sharp and
               Mike Whalen},
  title     = {Assessing transition-based test selection algorithms at Google},
  booktitle = {Proceedings of the 41st International Conference on Software Engineering:
               Software Engineering in Practice, {ICSE} {(SEIP)} 2019, Montreal,
               QC, Canada, May 25-31, 2019},
  pages     = {101--110},
  publisher = {{IEEE} / {ACM}},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICSE-SEIP.2019.00019},
  doi       = {10.1109/ICSE-SEIP.2019.00019},
}

@misc{Integrat56:online,
author = {Fowler, Martin},
title = {IntegrationTest},
howpublished = {\url{https://martinfowler.com/bliki/IntegrationTest.html}},
month = {January},
year = {2018},
note = {(Accessed on 02/22/2021)}
}
@article{schmidt2004analysis,
	title={The analysis of semi-structured interviews},
	author={Schmidt, Christiane},
	journal={A companion to qualitative research},
	pages={253--258},
	year={2004}
}
@article{glaser2007remodeling,
	title={Remodeling grounded theory},
	author={Glaser, Barney G and Holton, Judith},
	journal={Historical Social Research/Historische Sozialforschung. Supplement},
	pages={47--68},
	year={2007},
	publisher={JSTOR}
}

@inproceedings{hove2005experiences,
	title={Experiences from conducting semi-structured interviews in empirical software engineering research},
	author={Hove, Siw Elisabeth and Anda, Bente},
	booktitle={Software metrics, 2005. 11th ieee international symposium},
	pages={10--pp},
	year={2005},
	organization={IEEE}
}

@article{oliver2005constraints,
	title={Constraints and opportunities with interview transcription: Towards reflection in qualitative research},
	author={Oliver, Daniel G and Serovich, Julianne M and Mason, Tina L},
	journal={Social forces},
	volume={84},
	number={2},
	pages={1273--1289},
	year={2005},
	publisher={Oxford University Press}
}
@book{creswell2017research,
	title={Research design: Qualitative, quantitative, and mixed methods approaches},
	author={Creswell, John W and Creswell, J David},
	year={2017},
	publisher={Sage publications}
}

@article{adolph2011using,
	title={Using grounded theory to study the experience of software development},
	author={Adolph, Steve and Hall, Wendy and Kruchten, Philippe},
	journal={Empirical Software Engineering},
	volume={16},
	number={4},
	pages={487--513},
	year={2011},
	publisher={Springer}
}
@misc{Welcomet41:online,
author = {Thomas, John},
title = {Welcome to the Google Engineering Tools blog! | Google Engineering Tools},
howpublished = {\url{http://google-engtools.blogspot.com/2011/05/welcome-to-google-engineering-tools.html}},
month = {5},
year = {2011},
note = {(Accessed on 02/22/2021)}
}

@inproceedings{tomasdottir2017and,
	title={Why and how JavaScript developers use linters},
	author={T{\'o}masd{\'o}ttir, Krist{\'\i}n Fj{\'o}la and Aniche, Maur{\'\i}cio and Deursen, Arie van},
	booktitle={Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
	pages={578--589},
	year={2017},
	organization={IEEE Press}
}
@INPROCEEDINGS{8999994,
  author={S. {Habchi} and X. {Blanc} and R. {Rouvoy}},
  booktitle={2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={On Adopting Linters to Deal with Performance Concerns in Android Apps}, 
  year={2018},
  volume={},
  number={},
  pages={6-16},
  doi={10.1145/3238147.3238197}}
Copy


@inproceedings{harman2018start,
  title={From start-ups to scale-ups: Opportunities and open problems for static and dynamic program analysis},
  author={Harman, Mark and O'Hearn, Peter},
  booktitle={2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)},
  pages={1--23},
  year={2018},
  organization={IEEE}
}

@article{Bertolino2020,
abstract = {Flaky tests plague regression testing in Continuous Integration environments by slowing down change releases, wasting development e{\"{i}}¿¿ort, and also eroding testers trust in the test process. We present FLAST, the {\"{i}}¿¿rst static approach to {\"{i}}¿¿akiness detection using test code similarity. Our extensive evaluation on 24 projects taken from repositories used in three previous studies showed that FLAST can identify {\"{i}}¿¿aky tests with up to 0.98 Median and 0.92 Mean precision. For six of those projects it could already yield ⇠0.98 average precision values with a training set containing less than 100 tests. Besides, where known {\"{i}}¿¿aky tests are classi{\"{i}}¿¿ed according to their causes, the same approach can also predict a {\"{i}}¿¿aky test category with alike precision values. The cost of the approach is negligible: the average train time over a dataset of ⇠1,700 test methods is less than one second, while the average prediction time for a new test is less than one millisecond.},
author = {Bertolino, Antonia and Cruciani, Emilio and Miranda, Breno and Verdecchia, Roberto},
doi = {10.32079/ISTI-TR-2020/001.Istituto},
journal = {Proceedings of the International Conference on Software Engineering (ICSE)},
keywords = {flaky test,regression testing,software testing},
title = {{Know Your Neighbor: Fast Static Prediction of Test Flakiness}},
url = {https://ieeexplore.ieee.org},
year = {2020}
}

@article{Lam2020UnderstandingReproducibility,
abstract = {Flaky tests are tests that can non-deterministically pass and fail. They pose a major impediment to regression testing, because they provide an inconclusive assessment on whether recent code changes contain faults or not. Prior studies of flaky tests have proposed tools to detect flaky tests and identified various sources of flakiness in tests, e.g., order-dependent (OD) tests that deterministically fail for some order of tests in a test suite but deterministically pass for some other orders. Several of these studies have focused on OD tests. We focus on an important and under-explored source of flakiness in tests: non-order-dependent tests that can non-deterministically pass and fail even for the same order of tests. Instead of using specialized tools that aim to detect flaky tests, we run tests using the tool configured by the developers. Specifically, we perform our empirical evaluation on Java projects that rely on the Maven Surefire plugin to run tests. We re-execute each test suite 4000 times, potentially in different test-class orders, and we label tests as flaky if our runs have both pass and fail outcomes across these reruns. We obtain a dataset of 107 flaky tests and study various characteristics of these tests. We find that many tests previously called "non-order-dependent" actually do depend on the order and can fail with very different failure rates for different orders.},
author = {Lam, Wing and Winter, Stefan and Astorga, Angello and Stodden, Victoria and Marinov, Darko},
doi = {10.1109/issre5003.2020.00045},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/lam2020.pdf:pdf},
isbn = {9781728198705},
issn = {10719458},
pages = {403--413},
title = {{Understanding Reproducibility and Characteristics of Flaky Tests Through Test Reruns in Java Projects}},
year = {2020}
}


@misc{Effectiv86:online,
author = {Vimberg, Juan},
title = {Effective Testing - Reducing Non-determinism to Avoid Flaky Tests - Coding Forest},
howpublished = {\url{https://jivimberg.io/blog/2020/07/27/effective-testing-reducing-non-determinism/}},
month = {7},
year = {2020},
note = {(Accessed on 02/24/2021)}
}

@misc{TestStab71:online,
author = {Hu, Keqiu},
title = {Test Stability - How We Make UI Tests Stable | LinkedIn Engineering},
howpublished = {\url{https://engineering.linkedin.com/blog/2015/12/test-stability---how-we-make-ui-tests-stable}},
month = {12},
year = {2015},
note = {(Accessed on 02/24/2021)}
}

@misc{Flakytes0:online,
author = {Solntsev, Andrei},
title = {Flaky tests 2 - JVM Advent},
howpublished = {\url{https://www.javaadvent.com/2017/12/flaky-tests-2.html}},
month = {12},
year = {2017},
note = {(Accessed on 02/24/2021)}
}

@misc{Howtofix72:online,
author = {Essential Developer},
title = {How to fix flaky tests in your iOS/Swift codebases - YouTube},
howpublished = {\url{https://www.youtube.com/watch?v=_BOp6WYbq38&ab_channel=EssentialDeveloper}},
month = {12},
year = {2019},
note = {(Accessed on 02/24/2021)}
}

@misc{FlakyTes87:online,
author = {Testinium},
title = {Flaky Tests and How to Reduce them - Testinium},
howpublished = {\url{https://testinium.com/blog/flaky-tests-and-how-to-reduce-them/}},
month = {},
year = {},
note = {(Accessed on 02/24/2021)}
}

@misc{Managing72:online,
author = {Smartbear},
title = {Managing Test Flakiness | TestComplete},
howpublished = {\url{https://smartbear.com/resources/ebooks/managing-ui-test-flakiness/}},
month = {6},
year = {2018},
note = {(Accessed on 02/24/2021)}
}

@misc{Introduc67:online,
author = {Fabio, Pereira},
title = {Introducing the Software Testing Cupcake (Anti-Pattern) | ThoughtWorks},
howpublished = {\url{https://www.thoughtworks.com/insights/blog/introducing-software-testing-cupcake-anti-pattern}},
month = {6},
year = {2014},
note = {(Accessed on 02/24/2021)}
}


@misc{Amachine72:online,
author = {Liviu, Serban},
title = {A machine learning solution for detecting and mitigating flaky tests - Engineering Fitness},
howpublished = {\url{https://eng.fitbit.com/a-machine-learning-solution-for-detecting-and-mitigating-flaky-tests/}},
month = {},
year = {},
note = {(Accessed on 02/25/2021)}
}

@misc{Flakytes54:online,
author = {Fuchsia},
title = {Flaky test policy},
howpublished = {\url{https://fuchsia.dev/fuchsia-src/concepts/testing/test_flake_policy}},
month = {2},
year = {2021},
note = {(Accessed on 02/25/2021)}
}



@misc{TestFlak61:online,
author = {Palmer, Jason},
title = {Test Flakiness – Methods for identifying and dealing with flaky tests : Spotify Engineering},
howpublished = {\url{https://engineering.atspotify.com/2019/11/18/test-flakiness-methods-for-identifying-and-dealing-with-flaky-tests/}},
month = {},
year = {},
note = {(Accessed on 02/25/2021)}
}

@misc{Nomorefl8:online,
author = {Pavan, Sudarshan},
title = {No more flaky tests on the Go team | ThoughtWorks},
howpublished = {\url{https://www.thoughtworks.com/insights/blog/no-more-flaky-tests-go-team}},
month = {9},
year = {2012},
note = {(Accessed on 02/24/2021)}
}

@misc{flakytes70:online,
author = {McPeak, Alex},
title = {flaky tests Archives | CrossBrowserTesting.com},
howpublished = {\url{https://crossbrowsertesting.com/blog/tag/flaky-tests/}},
month = {2},
year = {2018},
note = {(Accessed on 02/24/2021)}
}

@misc{FlakyTes82:online,
author = {Micco, John},
title = {Flaky Tests at Google and How We Mitigate Them | googblogs.com},
howpublished = {\url{https://www.googblogs.com/flaky-tests-at-google-and-how-we-mitigate-them/}},
month = {5},
year = {2016},
note = {(Accessed on 02/24/2021)}
}

@misc{ThinkLik1:online,
author = {Thethinkingtester},
title = {Think Like a Tester: Your Flaky Tests Are Destroying Trust},
howpublished = {\url{http://thethinkingtester.blogspot.com/2019/10/your-flaky-tests-are-destroying-trust.html}},
month = {10},
year = {2019},
note = {(Accessed on 02/24/2021)}
}

@misc{Preventi83:online,
author = {Welter, Dennis},
title = {Preventing Flaky Tests from Ruining your Test Suite | Gradle Enterprise},
howpublished = {\url{https://gradle.com/blog/prevent-flaky-tests/}},
month = {},
year = {},
note = {(Accessed on 02/24/2021)}
}

@misc{FlakyTes55:online,
author = {The Code Gang},
title = {Flaky Tests - A War that Never Ends | Hacker Noon},
howpublished = {\url{https://hackernoon.com/flaky-tests-a-war-that-never-ends-9aa32fdef359}},
month = {12},
year = {2017},
note = {(Accessed on 02/24/2021)}
}

@misc{Selenium34:online,
author = {Attas, Zachary},
title = {Selenium Conf 2018 - How to Un-Flake Flaky Tests- a New Hire's Toolkit | ConfEngine - Conference Platform},
howpublished = {\url{https://confengine.com/conferences/selenium-conf-2018/proposal/6157/how-to-un-flake-flaky-tests-a-new-hires-toolkit}},
month = {6},
year = {2018},
note = {(Accessed on 02/24/2021)}
}

@misc{WeHaveAF52:online,
author = {Lee, Bryan},
title = {We Have A Flaky Test Problem. Flaky tests are insidious. Fighting… | by Bryan Lee | Scope | Medium},
howpublished = {\url{https://medium.com/scopedev/how-can-we-peacefully-co-exist-with-flaky-tests-3c8f94fba166}},
month = {1},
year = {2019},
note = {(Accessed on 02/24/2021)}
}

@misc{artefacts,
author = {Authors},
title = {Summary of the qualitative results},
howpublished = {\url{https://figshare.com/s/5b252c442fc36e8823cb}},
month = {2},
year = {2021},
note = {(Accessed on 02/24/2021)}
}


@article{WongMLK18,
  author    = {Chu{-}Pan Wong and
               Jens Meinicke and
               Lukas Lazarek and
               Christian K{\"{a}}stner},
  title     = {Faster variational execution with transparent bytecode transformation},
  journal   = {Proc. {ACM} Program. Lang.},
  volume    = {2},
  number    = {{OOPSLA}},
  pages     = {117:1--117:30},
  year      = {2018},
  url       = {https://doi.org/10.1145/3276487},
  doi       = {10.1145/3276487}
}

@inproceedings{pinto2020vocabulary,
  title={What is the vocabulary of flaky tests?},
  author={Pinto, Gustavo and Miranda, Breno and Dissanayake, Supun and d'Amorim, Marcelo and Treude, Christoph and Bertolino, Antonia},
  booktitle={Proceedings of the 17th International Conference on Mining Software Repositories},
  pages={492--502},
  year={2020}
}

@misc{testPyramid,
author = {Ham Vocke},
title = {The Practical Test Pyramid},
howpublished = {\url{https://martinfowler.com/articles/practical-test-pyramid.html}},
month = {February},
year = {2018},
note = {(Accessed on 06/17/2022)}
}

@inproceedings{Lam,
author = {Lam, Wing and Mu\c{s}lu, K\i{}van\c{c} and Sajnani, Hitesh and Thummalapenta, Suresh},
title = {A Study on the Lifecycle of Flaky Tests},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3381749},
doi = {10.1145/3377811.3381749},
abstract = {During regression testing, developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality. Thus, flaky tests, which nondeterministically pass or fail on the same code, are problematic because they provide misleading signals during regression testing. Although flaky tests are the focus of several existing studies, none of them study (1) the reoccurrence, runtimes, and time-before-fix of flaky tests, and (2) flaky tests in-depth on proprietary projects.This paper fills this knowledge gap about flaky tests and investigates whether prior categorization work on flaky tests also apply to proprietary projects. Specifically, we study the lifecycle of flaky tests in six large-scale proprietary projects at Microsoft. We find, as in prior work, that asynchronous calls are the leading cause of flaky tests in these Microsoft projects. Therefore, we propose the first automated solution, called Flakiness and Time Balancer (FaTB), to reduce the frequency of flaky-test failures caused by asynchronous calls. Our evaluation of five such flaky tests shows that FaTB can reduce the running times of these tests by up to 78\% without empirically affecting the frequency of their flaky-test failures. Lastly, our study finds several cases where developers claim they "fixed" a flaky test but our empirical experiments show that their changes do not fix or reduce these tests' frequency of flaky-test failures. Future studies should be more cautious when basing their results on changes that developers claim to be "fixes".},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1471–1482},
numpages = {12},
keywords = {empirical study, flaky test, lifecycle},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{zhou2021assessing,
  title={Assessing generalizability of CodeBERT},
  author={Zhou, Xin and Han, DongGyun and Lo, David},
  booktitle={2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={425--436},
  year={2021},
  organization={IEEE}
}

@article{chakraborty2021deep,
  title={Deep learning based vulnerability detection: Are we there yet},
  author={Chakraborty, Saikat and Krishna, Rahul and Ding, Yangruibo and Ray, Baishakhi},
  journal={IEEE Transactions on Software Engineering},
  year={2021},
  publisher={IEEE}
}

@INPROCEEDINGS{FlakeFlagger,  author={Alshammari, Abdulrahman and Morris, Christopher and Hilton, Michael and Bell, Jonathan},  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},   title={FlakeFlagger: Predicting Flakiness Without Rerunning Tests},   year={2021},  volume={},  number={},  pages={1572-1584},  doi={10.1109/ICSE43902.2021.00140}}


@inproceedings{camara2021use,
  title={On the use of test smells for prediction of flaky tests},
  author={Camara, Bruno and Silva, Marco and Endo, Andre and Vergilio, Silvia},
  booktitle={Brazilian Symposium on Systematic and Automated Software Testing},
  pages={46--54},
  year={2021}
}


@inproceedings{cubert,
author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
title = {Learning and Evaluating Contextual Embedding of Source Code},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as BERT, which can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate. Specifically, first, we curate a massive, deduplicated corpus of 7.4M Python files from GitHub, which we use to pre-train CuBERT, an open-sourced code-understanding BERT model; and, second, we create an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. We fine-tune CuBERT on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, BiLSTM and Transformer models, as well as published state-of-the-art models, showing that CuBERT outperforms them all, even with shorter training, and with fewer labeled examples. Future work on source-code embedding can benefit from reusing our benchmark, and from comparing against CuBERT models as a strong baseline.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {474},
numpages = {12},
series = {ICML'20}
}

@article{why30,
  title={Determining sample size for research activities},
  author={Krejcie, Robert V and Morgan, Daryle W},
  journal={Educational and psychological measurement},
  volume={30},
  number={3},
  pages={607--610},
  year={1970},
  publisher={Sage publications Sage CA: Los Angeles, CA}
}


@inproceedings{feng-etal-2020-codebert,
    title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
    author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.139",
    doi = "10.18653/v1/2020.findings-emnlp.139",
    pages = "1536--1547",
    abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {``}bimodal{''} data of NL-PL pairs and {``}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.",
}


@article{transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{Guo2021GraphCodeBERTPC,
  title={GraphCodeBERT: Pre-training Code Representations with Data Flow},
  author={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie Liu and Long Zhou and Nan Duan and Jian Yin and Daxin Jiang and M. Zhou},
  journal={ArXiv},
  year={2021},
  volume={abs/2009.08366}
}


@article{TreeBert,
  author= {Xue Jiang and Zhuoran Zheng and Chen Lyu and Liang Li and Lei Lyu},
  title = {TreeBERT: {A} Tree-Based Pre-Trained Model for Programming Language},
  journal  = {CoRR},
  volume= {abs/2105.12485},
  year = {2021},
  url  = {https://arxiv.org/abs/2105.12485},
  eprinttype = {arXiv},
  eprint= {2105.12485},
  timestamp = {Tue, 01 Jun 2021 18:07:59 +0200},
  biburl= {https://dblp.org/rec/journals/corr/abs-2105-12485.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{deflaker,  author={Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko},  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},   title={DeFlaker: Automatically Detecting Flaky Tests},   year={2018},  volume={},  number={},  pages={433-444},  doi={10.1145/3180155.3180164}}




@inproceedings{eck,
author = {Eck, Moritz and Palomba, Fabio and Castelluccio, Marco and Bacchelli, Alberto},
title = {Understanding Flaky Tests: The Developer’s Perspective},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.sndl1.arn.dz/10.1145/3338906.3338945},
doi = {10.1145/3338906.3338945},
abstract = {Flaky tests are software tests that exhibit a seemingly random outcome (pass or fail)
despite exercising unchanged code. In this work, we examine the perceptions of software
developers about the nature, relevance, and challenges of flaky tests.  We asked 21
professional developers to classify 200 flaky tests they previously fixed, in terms
of the nature and the origin of the flakiness, as well as of the fixing effort. We
also examined developers' fixing strategies. Subsequently, we conducted an online
survey with 121 developers with a median industrial programming experience of five
years. Our research shows that: The flakiness is due to several different causes,
four of which have never been reported before, despite being the most costly to fix;
flakiness is perceived as significant by the vast majority of developers, regardless
of their team's size and project's domain, and it can have effects on resource allocation,
scheduling, and the perceived reliability of the test suite; and the challenges developers
report to face regard mostly the reproduction of the flaky behavior and the identification
of the cause for the flakiness. Public preprint [http://arxiv.org/abs/1907.01466],
data and materials [https://doi-org.sndl1.arn.dz/10.5281/zenodo.3265785].},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {830–840},
numpages = {11},
keywords = {Flaky Tests, Mixed-Method Research, Empirical Studies},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}


@article{Haben2021,
abstract = {Industrial reports indicate that flaky tests are one of the primary concerns of software testing mainly due to the false signals they provide. To deal with this issue, researchers have developed tools and techniques aiming at (automatically) identifying flaky tests with encouraging results. However, to reach industrial adoption and practice, these techniques need to be replicated and evaluated extensively on multiple datasets, occasions and settings. In view of this, we perform a replication study of a recently proposed method that predicts flaky tests based on their vocabulary. We thus replicate the original study on three different dimensions. First, we replicate the approach on the same subjects as in the original study but using a different evaluation methodology, i.e., we adopt a time-sensitive selection of training and test sets to better reflect the envisioned use case. Second, we consolidate the findings of the initial study by building a new dataset of 837 flaky tests from 9 projects in a different programming language, i.e., Python while the original study was in Java, which comforts the generalisability of the results. Third, we propose an extension to the original approach by experimenting with different features extracted from the Code Under Test. We find that a more robust validation consistently decreases performance on the reported results of the original study, but, fortunately, the model remains capable to decently predict flaky tests. We find reassuring results that the vocabulary-based models can also be used to predict test flakiness in Python. Finally, we find that the information lying in the Code Under Test has a limited impact on the performance of the vocabulary-based models.},
author = {Haben, Guillaume and Habchi, Sarra and Papadakis, Mike and Cordy, Maxime and {Le Traon}, Yves},
journal = {Proceedings of the International Conference on Mining Software Repositories (MSR)},
keywords = {Index Terms-Software testing,flakiness,regression testing},
title = {{A Replication Study on the Usability of Code Vocabulary in Predicting Flaky Tests}},
year = {2021}
}

@article{habchi2021qualitative,
  title={A Qualitative Study on the Sources, Impacts, and Mitigation Strategies of Flaky Tests},
  author={Habchi, Sarra and Haben, Guillaume and Papadakis, Mike and Cordy, Maxime and Traon, Yves Le},
  journal = {International Conference on Software Testing (ICST)},
  year={2022}
}

@inproceedings{parnin,
author = {Parnin, Chris and Orso, Alessandro},
title = {Are Automated Debugging Techniques Actually Helping Programmers?},
year = {2011},
isbn = {9781450305624},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2001420.2001445},
doi = {10.1145/2001420.2001445},
booktitle = {Proceedings of the 2011 International Symposium on Software Testing and Analysis},
pages = {199–209},
numpages = {11},
keywords = {user studies, statistical debugging},
location = {Toronto, Ontario, Canada},
series = {ISSTA '11}
}
@INPROCEEDINGS{monperrus-ICSME,  
author={Xuan, Jifeng and Monperrus, Martin},  
booktitle={2014 IEEE International Conference on Software Maintenance and Evolution},  title={Learning to Combine Multiple Ranking Metrics for Fault Localization},   year={2014},  volume={},  number={},  pages={191-200},  doi={10.1109/ICSME.2014.41}}

@ARTICLE{sohn-TSE,
  author={Sohn, Joengju and Yoo, Shin},
  journal={IEEE Transactions on Software Engineering}, 
  title={Empirical Evaluation of Fault Localisation Using Code and Change Metrics}, 
  year={2021},
  volume={47},
  number={8},
  pages={1605-1625},
  doi={10.1109/TSE.2019.2930977}}

@ARTICLE{wong-dstar,
  author={Wong, W. Eric and Debroy, Vidroha and Gao, Ruizhi and Li, Yihao},
  journal={IEEE Transactions on Reliability}, 
  title={The DStar Method for Effective Software Fault Localization}, 
  year={2014},
  volume={63},
  number={1},
  pages={290-308},
  doi={10.1109/TR.2013.2285319}}
  
@INPROCEEDINGS{De-Flake,
  author={Ziftci, Celal and Cavalcanti, Diego},
  booktitle={2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={De-Flake Your Tests : Automatically Locating Root Causes of Flaky Tests in Code At Google}, 
  year={2020},
  volume={},
  number={},
  pages={736-745},
  doi={10.1109/ICSME46990.2020.00083}}
  
@inproceedings{tarantula,
author = {Jones, James A. and Harrold, Mary Jean},
title = {Empirical Evaluation of the Tarantula Automatic Fault-Localization Technique},
year = {2005},
isbn = {1581139934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101908.1101949},
doi = {10.1145/1101908.1101949},
abstract = {The high cost of locating faults in programs has motivated the development of techniques that assist in fault localization by automating part of the process of searching for faults. Empirical studies that compare these techniques have reported the relative effectiveness of four existing techniques on a set of subjects. These studies compare the rankings that the techniques compute for statements in the subject programs and the effectiveness of these rankings in locating the faults. However, it is unknown how these four techniques compare with Tarantula, another existing fault-localization technique, although this technique also provides a way to rank statements in terms of their suspiciousness. Thus, we performed a study to compare the Tarantula technique with the four techniques previously compared. This paper presents our study---it overviews the Tarantula technique along with the four other techniques studied, describes our experiment, and reports and discusses the results. Our studies show that, on the same set of subjects, the Tarantula technique consistently outperforms the other four techniques in terms of effectiveness in fault localization, and is comparable in efficiency to the least expensive of the other four techniques.},
booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering},
pages = {273–282},
numpages = {10},
keywords = {empirical study, program analysis, fault localization, automated debugging},
location = {Long Beach, CA, USA},
series = {ASE '05}
}

@article{sbfl-evaluation,
author = {Abreu, Rui and Zoeteweij, Peter and Golsteijn, Rob and van Gemund, Arjan J. C.},
title = {A Practical Evaluation of Spectrum-Based Fault Localization},
year = {2009},
issue_date = {November, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {11},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.035},
doi = {10.1016/j.jss.2009.06.035},
abstract = {Spectrum-based fault localization (SFL) shortens the test-diagnose-repair cycle by reducing the debugging effort. As a light-weight automated diagnosis technique it can easily be integrated with existing testing schemes. Since SFL is based on discovering statistical coincidences between system failures and the activity of the different parts of a system, its diagnostic accuracy is inherently limited. Using a common benchmark consisting of the Siemens set and the space program, we investigate this diagnostic accuracy as a function of several parameters (such as quality and quantity of the program spectra collected during the execution of the system), some of which directly relate to test design. Our results indicate that the superior performance of a particular similarity coefficient, used to analyze the program spectra, is largely independent of test design. Furthermore, near-optimal diagnostic accuracy (exonerating over 80% of the blocks of code on average) is already obtained for low-quality error observations and limited numbers of test cases. In addition to establishing these results in the controlled environment of our benchmark set, we show that SFL can effectively be applied in the context of embedded software development in an industrial environment.},
journal = {J. Syst. Softw.},
month = {nov},
pages = {1780–1792},
numpages = {13},
keywords = {Software fault diagnosis, Real-time and embedded systems, Consumer electronics, Test data analysis, Program spectra}
}

@article{flakyloc,
  author    = {Jes{\'{u}}s Mor{\'{a}}n and
               Cristian Augusto and
               Antonia Bertolino and
               Claudio de la Riva and
               Javier Tuya},
  title     = {FlakyLoc: Flakiness Localization for Reliable Test Suites in Web Applications},
  journal   = {J. Web Eng.},
  volume    = {19},
  number    = {2},
  pages     = {267--296},
  year      = {2020},
  url       = {https://doi.org/10.13052/jwe1540-9589.1927},
  doi       = {10.13052/jwe1540-9589.1927},
  timestamp = {Mon, 13 Jul 2020 16:22:24 +0200},
  biburl    = {https://dblp.org/rec/journals/jwe/MoranABRT20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inbook{FLEX,
author = {Dutta, Saikat and Shi, August and Misailovic, Sasa},
title = {FLEX: Fixing Flaky Tests in Machine Learning Projects by Updating Assertion Bounds},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468615},
abstract = {Many machine learning (ML) algorithms are inherently random – multiple executions using the same inputs may produce slightly different results each time. Randomness impacts how developers write tests that check for end-to-end quality of their implementations of these ML algorithms. In particular, selecting the proper thresholds for comparing obtained quality metrics with the reference results is a non-intuitive task, which may lead to flaky test executions.  We present FLEX, the first tool for automatically fixing flaky tests due to algorithmic randomness in ML algorithms. FLEX fixes tests that use approximate assertions to compare actual and expected values that represent the quality of the outputs of ML algorithms. We present a technique for systematically identifying the acceptable bound between the actual and expected output quality that also minimizes flakiness. Our technique is based on the Peak Over Threshold method from statistical Extreme Value Theory, which estimates the tail distribution of the output values observed from several runs. Based on the tail distribution, FLEX updates the bound used in the test, or selects the number of test re-runs, based on a desired confidence level.  We evaluate FLEX on a corpus of 35 tests collected from the latest versions of 21 ML projects. Overall, FLEX identifies and proposes a fix for 28 tests. We sent 19 pull requests, each fixing one test, to the developers. So far, 9 have been accepted by the developers.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {603–614},
numpages = {12}
}
@article{chawla2002smote,
  title={SMOTE: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of artificial intelligence research},
  volume={16},
  pages={321--357},
  year={2002}
}
@misc{Featurei12:online,
author = {},
title = {Feature importances with a forest of trees — scikit-learn 1.1.1 documentation},
howpublished = {\url{https://scikit-learn.org/stable/auto\_examples/ensemble/plot\_forest\_importances.html}},
month = {},
year = {},
note = {(Accessed on 06/24/2022)}
}
@article{Rahman2018,
abstract = {Testing is an integral part of release engineering and continuous integration. In theory, a failed test on a build indicates a problem that should be fixed and the build should not be released. In practice, tests decay and developers often release builds, ignoring failing tests. In this paper, we studying the link between builds with failing tests and the number of crash reports on the Firefox webbrowser. Builds with all tests passing have a median of only two crash reports. In contrast, builds with one or more failing tests are associated with a median of 508 and 291 crash reports for Beta and Production builds, respectively. We further investigate the impact of "fiaky" tests, which can both pass and fail on the same build, and find that they have a median of 514 and 234 crash reports for Beta and Production builds. Finally, building on previous research that has shown that tests that have failed frequently in the past will fail frequently in the future, we find that Builds with HighFailureTests have a median of 585 and 780 crash reports for Beta and Production builds. Unlike other types of test failures, HighFailureTests have a larger impact on Production releases than on Beta builds, and they have a median of 2.7 times more crashes than builds with normal test failures. We conclude that ignoring test failures is related to a dramatic increase in the number of crashes reported by users.},
author = {Rahman, M. Tajmilur and Rigby, Peter C.},
doi = {10.1145/3236024.3275529},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/rahman2018.pdf:pdf},
isbn = {9781450355735},
journal = {ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Builds,Flaky Tests,Software Testing,User Crash Reports},
pages = {857--862},
title = {{The impact of failing, flaky, and high failure tests on the number of crash reports associated with firefox builds}},
year = {2018}
}
@article{Gruber2021,
abstract = {Tests that cause spurious failures without any code changes, i.e., flaky tests, hamper regression testing, increase maintenance costs, may shadow real bugs, and decrease trust in tests. While the prevalence and importance of flakiness is well established, prior research focused on Java projects, thus raising the question of how the findings generalize. In order to provide a better understanding of the role of flakiness in software development beyond Java, we empirically study the prevalence, causes, and degree of flakiness within software written in Python, one of the currently most popular programming languages. For this, we sampled 22 352 open source projects from the popular PyPI package index, and analyzed their 876 186 test cases for flakiness. Our investigation suggests that flakiness is equally prevalent in Python as it is in Java. The reasons, however, are different: Order dependency is a much more dominant problem in Python, causing 59\% of the 7 571 flaky tests in our dataset. Another 28\% were caused by test infrastructure problems, which represent a previously undocumented cause of flakiness. The remaining 13\% can mostly be attributed to the use of network and randomness APIs by the projects, which is indicative of the type of software commonly written in Python. Our data also suggests that finding flaky tests requires more runs than are often done in the literature: A 95\% confidence that a passing test case is not flaky on average would require 170 reruns.},
author = {Gruber, Martin and Lukasczyk, Stephan and Krois, Florian and Fraser, Gordon},
doi = {10.1109/ICST49551.2021.00026},
file = {:Users/guillaume.haben/Documents/Work/papers/flakiness/flakinessInPython.pdf:pdf},
isbn = {9781728168364},
journal = {Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation, ICST 2021},
keywords = {Empirical Study,Flaky Test,Python},
pages = {148--158},
title = {{An Empirical Study of Flaky Tests in Python}},
year = {2021}
}
@inproceedings{romano2021empirical,
  title={An Empirical Analysis of UI-based Flaky Tests},
  author={Romano, Alan and Song, Zihe and Grandhi, Sampath and Yang, Wei and Wang, Weihang},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages={1585--1597},
  year={2021},
  organization={IEEE}
}
@INPROCEEDINGS {Camara2021VocabExtendedReplication,
author = {B. Camara and M. Silva and A. T. Endo and S. Vergilio},
booktitle = {2021 2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC) (ICPC)},
title = {What is the Vocabulary of Flaky Tests? An Extended Replication},
year = {2021},
volume = {},
issn = {},
pages = {444-454},
keywords = {training;vocabulary;automation;companies;prediction algorithms;software systems;classification algorithms},
doi = {10.1109/ICPC52881.2021.00052},
url = {https://doi.ieeecomputersociety.org/10.1109/ICPC52881.2021.00052},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {5}
}
@article{DBLP:journals/corr/abs-1906-00673,
  author    = {Azeem Ahmad and
               Ola Leifler and
               Kristian Sandahl},
  title     = {Empirical Analysis of Factors and their Effect on Test Flakiness -
               Practitioners' Perceptions},
  journal   = {CoRR},
  volume    = {abs/1906.00673},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.00673},
  archivePrefix = {arXiv},
  eprint    = {1906.00673},
  timestamp = {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-00673.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{GreedyFlake,
  author={Vaidhyam Subramanian, Shivashree Vysali and McIntosh, Shane and Adams, Bram},
  journal={IEEE Transactions on Software Engineering}, 
  title={Quantifying, Characterizing, and Mitigating Flakily Covered Program Elements}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TSE.2020.3010045}
}
@article{Herzig2015,
abstract = {Applying code changes to software systems and testing these code changes can be a complex task that involves many different types of software testing strategies, e.g. system and integration tests. However, not all test failures reported during code integration are hinting towards code defects. Testing large systems such as the Microsoft Windows operating system requires complex test infrastructures, which may lead to test failures caused by faulty tests and test infrastructure issues. Such false test alarms are particular annoying as they raise engineer attention and require manual inspection without providing any benefit. The goal of this work is to use empirical data to minimize the number of false test alarms reported during system and integration testing. To achieve this goal, we use association rule learning to identify patterns among failing test steps that are typically for false test alarms and can be used to automatically classify them. A successful classification of false test alarms is particularly valuable for product teams as manual test failure inspection is an expensive and time-consuming process that not only costs engineering time and money but also slows down product development. We evaluating our approach on system and integration tests executed during Windows 8.1 and Microsoft Dynamics AX development. Performing more than 10,000 classifications for each product, our model shows a mean precision between 0.85 and 0.90 predicting between 34% and 48% of all false test alarms.},
author = {Herzig, Kim and Nagappan, Nachiappan},
doi = {10.1109/ICSE.2015.133},
file = {:Users/guillaume.haben/Documents/Work/papers/MendeleyArchive/Empirically-Detecting-False-Test-Alarms-Using-Association-Rules.pdf:pdf},
isbn = {9781479919345},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {association rules,classification model,false test alarms,software testing,test improvement},
pages = {39--48},
title = {{Empirically Detecting False Test Alarms Using Association Rules}},
volume = {2},
year = {2015}
}
@misc{onlineChromiumGithub,
author = {The Chromium Development team},
title = {chromium/chromium: The official GitHub mirror of the Chromium source},
howpublished = {\url{https://github.com/chromium/chromium}},
month = {7},
year = {2021},
note = {(Accessed on 07/06/2021)}
}
@inproceedings{liu1997techniques,
  title={Techniques for dealing with missing values in classification},
  author={Liu, Wei Zhong and White, Allan P and Thompson, Simon G and Bramer, Max A},
  booktitle={International Symposium on Intelligent Data Analysis},
  pages={527--536},
  year={1997},
  organization={Springer}
}
@inproceedings{precisionRecallCurve,
author = {Davis, Jesse and Goadrich, Mark},
title = {The Relationship between Precision-Recall and ROC Curves},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143874},
doi = {10.1145/1143844.1143874},
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results
for binary decision problems in machine learning. However, when dealing with highly
skewed datasets, Precision-Recall (PR) curves give a more informative picture of an
algorithm's performance. We show that a deep connection exists between ROC space and
PR space, such that a curve dominates in ROC space if and only if it dominates in
PR space. A corollary is the notion of an achievable PR curve, which has properties
much like the convex hull in ROC space; we show an efficient algorithm for computing
this curve. Finally, we also note differences in the two types of curves are significant
for algorithm design. For example, in PR space it is incorrect to linearly interpolate
between points. Furthermore, algorithms that optimize the area under the ROC curve
are not guaranteed to optimize the area under the PR curve.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {233–240},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}
@article{goldberg2017neural,
  title={Neural network methods for natural language processing},
  author={Goldberg, Yoav},
  journal={Synthesis lectures on human language technologies},
  volume={10},
  number={1},
  pages={1--309},
  year={2017},
  publisher={Morgan \& Claypool Publishers}
}
@article{chicco2020advantages,
  title={The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
  author={Chicco, Davide and Jurman, Giuseppe},
  journal={BMC genomics},
  volume={21},
  number={1},
  pages={1--13},
  year={2020},
  publisher={Springer}
}
@article{bergstra2012random,
  title={Random search for hyper-parameter optimization.},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}
@misc{TheChromiumProjects,
author = {Chromium contributors},
title = {The Chromium Projects},
howpublished = {\url{https://www.chromium.org/}},
month = {},
year = {2021},
note = {(Accessed on 08/17/2021)}
}
@inproceedings{Dong2021,
abstract = {Validation of Android apps via testing is difficult owing to the presence of flaky tests. Due to non-deterministic execution environments , a sequence of events (a test) may lead to success or failure in unpredictable ways. In this work, we present an approach and tool FlakeScanner for detecting flaky tests through exploration of event orders. Our key observation is that for a test in a mobile app, there is a testing framework thread which creates the test events, a main User-Interface (UI) thread processing these events, and there may be several other background threads running asynchronously. For any event {\'{i}} µ{\'{i}}± whose execution involves potential non-determinism, we localize the earliest (latest) event after (before) which {\'{i}} µ{\'{i}}± must happen. We then efficiently explore the schedules between the upper/lower bound events while grouping events within a single statement, to find whether the test outcome is flaky. We also create a suite of subject programs called FlakyAppRepo (containing 33 widely-used Android projects) to study flaky tests in Android apps. Our experiments on the subject-suite FlakyAppRepo show FlakeScanner detected 45 out of 52 known flaky tests as well as 245 previously unknown flaky tests among 1444 tests. CCS CONCEPTS • Software and its engineering → Software testing and de-bugging; KEYWORDS flaky tests, non-determinism, concurrency, event order ACM Reference Format:},
author = {Dong, Zhen and Tiwari, Abhishek and Yu, Xiao Liang and Roychoudhury, Abhik},
booktitle = {Proceedings of the 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE '21), August 23{\^{a}}•ﬁ28, 2021, Athens, Greece},
doi = {10.1145/3468264.3468584},
file = {:Users/guillaume.haben/Documents/Mendeley Desktop/2021 - Flaky test detection in Android via event order exploration.pdf:pdf},
isbn = {9781450385626},
keywords = {abhishek tiwari,acm reference format,and abhik roychoudhury,concurrency,event o,event order,flaky tests,non-determinism,xiao liang yu,zhen dong},
mendeley-groups = {Flakiness Detection},
number = {1},
pages = {367--378},
publisher = {Association for Computing Machinery},
title = {{Flaky test detection in Android via event order exploration}},
volume = {1},
year = {2021}
}

@article{fatima2021flakify,
  author={Fatima, Sakina and Ghaleb, Taher A. and Briand, Lionel},
  journal={IEEE Transactions on Software Engineering}, 
  title={Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests}, 
  year={2022},
  volume={},
  number={},
  pages={1-17},
  doi={10.1109/TSE.2022.3201209}}

@article{spoon,
  TITLE = "{Spoon: A Library for Implementing Analyses and Transformations of Java Source Code}",
  AUTHOR = {Pawlak, Renaud and Monperrus, Martin and Petitprez, Nicolas and Noguera, Carlos and Seinturier, Lionel},
  JOURNAL = "{Software: Practice and Experience}",
  PUBLISHER = "{Wiley-Blackwell}",
  PAGES = {1155-1179},
  VOLUME = {46},
  URL = {https://hal.archives-ouvertes.fr/hal-01078532/document},
  YEAR = {2015},
  doi = {10.1002/spe.2346},
}
@inproceedings{githubsearch,
  author    = {Ozren Dabic and Emad Aghajani and Gabriele Bavota},
  title     = {Sampling Projects in GitHub for {MSR} Studies},
  booktitle = {18th {IEEE/ACM} International Conference on Mining Software Repositories,
               {MSR} 2021},
  pages     = {560--564},
  publisher = {{IEEE}},
  year      = {2021}
}

@InProceedings{Lou:2021:fse,
  author    = {Lou, Yiling and Zhu, Qihao and Dong, Jinhao and Li, Xia and Sun, Zeyu and Hao, Dan and Zhang, Lu and Zhang, Lingming},
  booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  title     = {Boosting Coverage-Based Fault Localization via Graph-Based Representation Learning},
  doi       = {10.1145/3468264.3468580},
  isbn      = {9781450385626},
  location  = {Athens, Greece},
  pages     = {664–676},
  publisher = {Association for Computing Machinery},
  series    = {ESEC/FSE 2021},
  url       = {https://doi.org/10.1145/3468264.3468580},
  abstract  = {Coverage-based fault localization has been extensively studied in the literature due to its effectiveness and lightweightness for real-world systems. However, existing techniques often utilize coverage in an oversimplified way by abstracting detailed coverage into numbers of tests or boolean vectors, thus limiting their effectiveness in practice. In this work, we present a novel coverage-based fault localization technique, GRACE, which fully utilizes detailed coverage information with graph-based representation learning. Our intuition is that coverage can be regarded as connective relationships between tests and program entities, which can be inherently and integrally represented by a graph structure: with tests and program entities as nodes, while with coverage and code structures as edges. Therefore, we first propose a novel graph-based representation to reserve all detailed coverage information and fine-grained code structures into one graph. Then we leverage Gated Graph Neural Network to learn valuable features from the graph-based coverage representation and rank program entities in a listwise way. Our evaluation on the widely used benchmark Defects4J (V1.2.0) shows that GRACE significantly outperforms state-of-the-art coverage-based fault localization: GRACE localizes 195 bugs within Top-1 whereas the best compared technique can at most localize 166 bugs within Top-1. We further investigate the impact of each GRACE component and find that they all positively contribute to GRACE. In addition, our results also demonstrate that GRACE has learnt essential features from coverage, which are complementary to various information used in existing learning-based fault localization. Finally, we evaluate GRACE in the cross-project prediction scenario on extra 226 bugs from Defects4J (V2.0.0), and find that GRACE consistently outperforms state-of-the-art coverage-based techniques.},
  address   = {New York, NY, USA},
  keywords  = {Graph Neural Network, Fault Localization, Representation Learning},
  numpages  = {13},
  year      = {2021},
}

@inproceedings{Li0N21a,
  author    = {Yi Li and
               Shaohua Wang and
               Tien N. Nguyen},
  title     = {Fault Localization with Code Coverage Representation Learning},
  booktitle = {43rd {IEEE/ACM} International Conference on Software Engineering,
               {ICSE} 2021, Madrid, Spain, 22-30 May 2021},
  pages     = {661--673},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICSE43902.2021.00067},
  doi       = {10.1109/ICSE43902.2021.00067},
  timestamp = {Mon, 07 Jun 2021 16:39:41 +0200},
  biburl    = {https://dblp.org/rec/conf/icse/Li0N21a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{Li:2019:issta,
  author    = {Li, Xia and Li, Wei and Zhang, Yuqun and Zhang, Lingming},
  booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  title     = {DeepFL: Integrating Multiple Fault Diagnosis Dimensions for Deep Fault Localization},
  doi       = {10.1145/3293882.3330574},
  isbn      = {9781450362245},
  location  = {Beijing, China},
  pages     = {169–180},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2019},
  url       = {https://doi.org/10.1145/3293882.3330574},
  abstract  = {Learning-based fault localization has been intensively studied recently. Prior studies have shown that traditional Learning-to-Rank techniques can help precisely diagnose fault locations using various dimensions of fault-diagnosis features, such as suspiciousness values computed by various off-the-shelf fault localization techniques. However, with the increasing dimensions of features considered by advanced fault localization techniques, it can be quite challenging for the traditional Learning-to-Rank algorithms to automatically identify effective existing/latent features. In this work, we propose DeepFL, a deep learning approach to automatically learn the most effective existing/latent features for precise fault localization. Although the approach is general, in this work, we collect various suspiciousness-value-based, fault-proneness-based and textual-similarity-based features from the fault localization, defect prediction and information retrieval areas, respectively. DeepFL has been studied on 395 real bugs from the widely used Defects4J benchmark. The experimental results show DeepFL can significantly outperform state-of-the-art TraPT/FLUCCS (e.g., localizing 50+ more faults within Top-1). We also investigate the impacts of deep model configurations (e.g., loss functions and epoch settings) and features. Furthermore, DeepFL is also surprisingly effective for cross-project prediction.},
  address   = {New York, NY, USA},
  keywords  = {Mutation testing, Deep learning, Fault localization},
  numpages  = {12},
  year      = {2019},
}

@Article{Wen:2019:tse,
  author   = {M. {Wen} and J. {Chen} and Y. {Tian} and R. {Wu} and D. {Hao} and S. {Han} and S. C. {Cheung}},
  title    = {Historical Spectrum based Fault Localization},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2019},
  pages    = {1-1},
  doi      = {10.1109/TSE.2019.2948158},
  keywords = {Computer bugs;History;Debugging;Industries;Software;Benchmark testing;Maintenance engineering;Fault Localization;Version Histories;Bug-Inducing Commits},
}

@InProceedings{perez:2017:icse,
  author    = {A. {Perez} and R. {Abreu} and A. {van Deursen}},
  booktitle = {2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)},
  title     = {A Test-Suite Diagnosability Metric for Spectrum-Based Fault Localization Approaches},
  doi       = {10.1109/ICSE.2017.66},
  pages     = {654-664},
  year      = {2017},
}

@Article{Wong:2016:tse,
  author  = {W. E. {Wong} and R. {Gao} and Y. {Li} and R. {Abreu} and F. {Wotawa}},
  title   = {A Survey on Software Fault Localization},
  journal = {IEEE Transactions on Software Engineering},
  year    = {2016},
  volume  = {42},
  number  = {8},
  pages   = {707-740},
}

@inproceedings{Abreu:2006yf,
	Author = {Abreu, Rui and Zoeteweij, Peter and van Gemund, Arjan JC},
	Booktitle = {The proceedings of the 12th Pacific Rim International Symposium on Dependable Computing},
	Date-Added = {2014-03-18 11:16:05 +0000},
	Date-Modified = {2014-03-18 11:16:36 +0000},
	Keywords = {localisation},
	Organization = {IEEE},
	Owner = {jjsohn},
	Pages = {39--46},
	Series = {PRDC 2006},
	Timestamp = {2018.03.27},
	Title = {An evaluation of similarity coefficients for software fault localization},
	Year = {2006}}

@inproceedings{abreu2009spectrum,
  title={Spectrum-based multiple fault localization},
  author={Abreu, Rui and Zoeteweij, Peter and Van Gemund, Arjan JC},
  booktitle={2009 IEEE/ACM International Conference on Automated Software Engineering},
  pages={88--99},
  year={2009},
  organization={IEEE}
}

@article{habchi2021mutinject,
  author    = {Sarra Habchi and
               Maxime Cordy and
               Mike Papadakis and
               Yves Le Traon},
  title     = {On the Use of Mutation in Injecting Test Order-Dependency},
  journal   = {CoRR},
  volume    = {abs/2104.07441},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.07441},
  eprinttype = {arXiv},
  eprint    = {2104.07441},
  timestamp = {Thu, 14 Oct 2021 09:14:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-07441.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Jones:2002kx,
	Address = {New York, NY, USA},
	Author = {Jones, James A. and Harrold, Mary Jean and Stasko, John},
	Booktitle = {Proceedings of the 24th International Conference on Software Engineering},
	Date-Added = {2011-03-10 00:17:46 +0000},
	Date-Modified = {2014-12-11 14:00:06 +0000},
	Keywords = {localisation; visualisation},
	Location = {Orlando, Florida},
	Numpages = {11},
	Owner = {jjsohn},
	Pages = {467--477},
	Publisher = {ACM},
	Timestamp = {2018.03.27},
	Title = {Visualization of test information to assist fault localization},
	Year = {2002},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/581339.581397}}



@inproceedings{Jones:2001vn,
	Author = {James A. Jones and Mary Jean Harrold and John T. Stasko},
	Booktitle = {Proceedings of ICSE Workshop on Software Visualization},
	Date-Added = {2011-03-10 00:16:01 +0000},
	Date-Modified = {2014-12-11 14:00:06 +0000},
	Keywords = {localisation; visualisation},
	Owner = {jjsohn},
	Pages = {71--75},
	Timestamp = {2018.03.27},
	Title = {Visualization for Fault Localization},
	Year = {2001},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAdLi4vLi4vUGFwZXJzL0pvbmVzMjAwMWFhXy5wZGZPEQFaAAAAAAFaAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8QSm9uZXMyMDAxYWFfLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAACAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAC8vOlVzZXJzOm50cm9sbHM6RHJvcGJveDpQYXBlcnM6Sm9uZXMyMDAxYWFfLnBkZgAADgAiABAASgBvAG4AZQBzADIAMAAwADEAYQBhAF8ALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAC1Vc2Vycy9udHJvbGxzL0Ryb3Bib3gvUGFwZXJzL0pvbmVzMjAwMWFhXy5wZGYAABMAAS8AABUAAgAO//8AAAAIAA0AGgAkAEQAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==}}
	
@inproceedings{Sohn2019aa,
  author = {Sohn, Jeongju and Yoo, Shin},
  booktitle = {Proceedings of the Annual Conference on Genetic and Evolutionary Computation},
  pages = {1408--1416},
  series = {GECCO 2019},
  title = {Why Train-and-Select when you can use them all? {E}nsemble Model for Fault Localisation},
  year = {2019}
}

@inproceedings{Sohn2021ea,
  author = {Sohn, Jeongju and An, Gabin and Hong, Jingun and Hwang, Dongwon and Yoo, Shin},
  booktitle = {Proceedings of the 14th IEEE International Conference on Software Testing, Verification and Validation},
  date-added = {2021-02-01 14:00:57 +0900},
  date-modified = {2021-02-01 14:02:46 +0900},
  title = {Assisting Bug Report Assignment Using Automated Fault Localisation: An Industrial Case Study},
  year = {2021}
}


@techreport{Yoo:2014fv,
	Author = {Shin Yoo and Xiaoyuan Xie and Fei-Ching Kuo and Tsong Yueh Chen and Mark Harman},
	Date-Added = {2014-11-05 13:03:34 +0000},
	Date-Modified = {2015-02-26 13:16:03 +0000},
	Institution = {{U}niversity {C}ollege {L}ondon},
	Keywords = {localisation},
	Number = {RN/14/14},
	Owner = {jjsohn},
	Timestamp = {2018.03.27},
	Title = {No Pot of Gold at the End of Program Spectrum Rainbow: Greatest Risk Evaluation Formula Does Not Exist},
	Year = {2014},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAXLi4vUGFwZXJzL1lvbzIwMTRmdi5wZGZPEQGIAAAAAAGIAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADRsplZSCsAAABFOqsNWW9vMjAxNGZ2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEU8JtB+QRUAAAAAAAAAAAABAAIAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAANGyGskAAAARAAgAANB9woUAAAABABAARTqrAEUzfABEVmUAApg5AAIAO01hY2ludG9zaCBIRDpVc2VyczoAbnRyb2xsczoARHJvcGJveDoAUGFwZXJzOgBZb28yMDE0ZnYucGRmAAAOABwADQBZAG8AbwAyADAAMQA0AGYAdgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAKlVzZXJzL250cm9sbHMvRHJvcGJveC9QYXBlcnMvWW9vMjAxNGZ2LnBkZgATAAEvAAAVAAIADv//AAAACAANABoAJAA+AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAco=}}
	
	
@inproceedings{Xuan:2014kq,
	Author = {Jifeng Xuan and Monperrus, M.},
	Booktitle = {Proceedings of the IEEE International Conference on Software Maintenance and Evolution},
	Date-Added = {2015-09-17 12:02:28 +0000},
	Date-Modified = {2015-09-17 12:02:57 +0000},
	Issn = {1063-6773},
	Keywords = {localisation},
	Month = {Sept},
	Owner = {jjsohn},
	Pages = {191-200},
	Series = {ICSME 2014},
	Timestamp = {2018.03.27},
	Title = {Learning to Combine Multiple Ranking Metrics for Fault Localization},
	Year = {2014},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAjLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9pY3NtZV8xNC5wZGZPEQGIAAAAAAGIAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADS+Io/SCsAAAAhFi4MaWNzbWVfMTQucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKgt2tQIu4cAAAAAAAAAAAACAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAANL4C68AAAARAAgAANQIPPcAAAABABAAIRYuACBkNAAfpy8ABiwUAAIAPE1hY2ludG9zaCBIRDpVc2VyczoAbnRyb2xsczoARG9jdW1lbnRzOgBQYXBlcnM6AGljc21lXzE0LnBkZgAOABoADABpAGMAcwBtAGUAXwAxADQALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACtVc2Vycy9udHJvbGxzL0RvY3VtZW50cy9QYXBlcnMvaWNzbWVfMTQucGRmAAATAAEvAAAVAAIADv//AAAACAANABoAJABKAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAdY=}}
	
@inproceedings{B.-Le:2016yu,
	Acmid = {2931049},
	Address = {New York, NY, USA},
	Author = {B. Le, Tien-Duy and Lo, David and Le Goues, Claire and Grunske, Lars},
	Booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
	Date-Added = {2016-09-08 06:32:38 +0000},
	Date-Modified = {2016-09-21 13:23:55 +0000},
	Isbn = {978-1-4503-4390-9},
	Keywords = {localisation},
	Numpages = {12},
	Owner = {jjsohn},
	Pages = {177--188},
	Publisher = {ACM},
	Series = {ISSTA 2016},
	Timestamp = {2018.03.27},
	Title = {A Learning-to-rank Based Fault Localization Approach Using Likely Invariants},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAcLi4vLi4vUGFwZXJzL0IuLUxlMjAxNnl1LnBkZk8RAVQAAAAAAVQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w9CLi1MZTIwMTZ5dS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAIAAgAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6bnRyb2xsczpEcm9wYm94OlBhcGVyczpCLi1MZTIwMTZ5dS5wZGYADgAgAA8AQgAuAC0ATABlADIAMAAxADYAeQB1AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgAsVXNlcnMvbnRyb2xscy9Ecm9wYm94L1BhcGVycy9CLi1MZTIwMTZ5dS5wZGYAEwABLwAAFQACAA7//wAAAAgADQAaACQAQwAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb}}

@article{Fortin:2012aa,
	Author = {F\'elix-Antoine Fortin and Fran\c{c}ois-Michel {De Rainville} and Marc-Andr\'e Gardner and Marc Parizeau and Christian Gagn\'e},
	Date-Added = {2016-09-22 13:59:16 +0000},
	Date-Modified = {2016-09-22 13:59:34 +0000},
	Journal = {Journal of Machine Learning Research},
	Month = {7},
	Owner = {jjsohn},
	Pages = {2171--2175},
	Timestamp = {2018.03.27},
	Title = {{DEAP}: Evolutionary Algorithms Made Easy},
	Volume = {13},
	Year = {2012}}
	
	
@InProceedings{Rahman:2013:icse,
  author    = {Rahman, Foyzur and Devanbu, Premkumar},
  booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
  title     = {How, and Why, Process Metrics Are Better},
  isbn      = {9781467330763},
  location  = {San Francisco, CA, USA},
  pages     = {432–441},
  publisher = {IEEE Press},
  series    = {ICSE '13},
  abstract  = {Defect prediction techniques could potentially help us to focus quality-assurance efforts on the most defect-prone files. Modern statistical tools make it very easy to quickly build and deploy prediction models. Software metrics are at the heart of prediction models; understanding how and especially why different types of metrics are effective is very important for successful model deployment. In this paper we analyze the applicability and efficacy of process and code metrics from several different perspectives. We build many prediction models across 85 releases of 12 large open source projects to address the performance, stability, portability and stasis of different sets of metrics. Our results suggest that code metrics, despite widespread use in the defect prediction literature, are generally less useful than process metrics for prediction. Second, we find that code metrics have high stasis; they dont change very much from release to release. This leads to stagnation in the prediction models, leading to the same files being repeatedly predicted as defective; unfortunately, these recurringly defective files turn out to be comparatively less defect-dense.},
  numpages  = {10},
  year      = {2013},
}

@Article{McI:2018:tse,
  author   = {S. {McIntosh} and Y. {Kamei}},
  title    = {Are Fix-Inducing Changes a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Prediction},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2018},
  volume   = {44},
  number   = {5},
  month    = {May},
  pages    = {412-428},
  doi      = {10.1109/TSE.2017.2693980},
  keywords = {data mining;just-in-time;learning (artificial intelligence);public domain software;software management;software quality;source code (software);Just-In-Time models;fix-inducing code changes;code change properties;target moving;just-in-time defect prediction;fix-inducing changes;JIT models;OpenStack systems;Qt systems;mining software repositories;Predictive models;Data models;Software;Complexity theory;Market research;Context modeling;Calibration;Just-In-Time prediction;defect prediction;mining software repositories},
}

@ARTICLE{Kamei:2012:tse,  author={Kamei, Yasutaka and Shihab, Emad and Adams, Bram and Hassan, Ahmed E. and Mockus, Audris and Sinha, Anand and Ubayashi, Naoyasu},  journal={IEEE Transactions on Software Engineering},   title={A large-scale empirical study of just-in-time quality assurance},   year={2013},  volume={39},  number={6},  pages={757-773},  doi={10.1109/TSE.2012.70}}
@inproceedings{perez2018leveraging,
  title={Leveraging Qualitative Reasoning to Improve SFL.},
  author={Perez, Alexandre and Abreu, Rui and HASLab, IT},
  booktitle={IJCAI},
  pages={1935--1941},
  year={2018}
}
@inproceedings{briand2007,
  title={Using machine learning to support debugging with tarantula},
  author={Briand, Lionel C and Labiche, Yvan and Liu, Xuetao},
  booktitle={The 18th IEEE International Symposium on Software Reliability (ISSRE'07)},
  pages={137--146},
  year={2007},
  organization={IEEE}
}
@article{wong2016survey,
  title={A survey on software fault localization},
  author={Wong, W Eric and Gao, Ruizhi and Li, Yihao and Abreu, Rui and Wotawa, Franz},
  journal={IEEE Transactions on Software Engineering},
  volume={42},
  number={8},
  pages={707--740},
  year={2016},
  publisher={IEEE}
}
@inproceedings{renieres2003fault,
  title={Fault localization with nearest neighbor queries},
  author={Renieres, Manos and Reiss, Steven P},
  booktitle={18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.},
  pages={30--39},
  year={2003},
  organization={IEEE}
}
@article{zou2019empirical,
  title={An empirical study of fault localization families and their combinations},
  author={Zou, Daming and Liang, Jingjing and Xiong, Yingfei and Ernst, Michael D and Zhang, Lu},
  journal={IEEE Transactions on Software Engineering},
  year={2019},
  publisher={IEEE}
}
@inproceedings{li2019deepfl,
  title={Deepfl: Integrating multiple fault diagnosis dimensions for deep fault localization},
  author={Li, Xia and Li, Wei and Zhang, Yuqun and Zhang, Lingming},
  booktitle={Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages={169--180},
  year={2019}
}
@inproceedings{Pontillo,
  title={Toward static test flakiness prediction: A feasibility study},
  author={Pontillo, Valeria and Palomba, Fabio and Ferrucci, Filomena},
  booktitle={Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
  pages={19--24},
  year={2021}
}
@article{Parry2021,
author = {Parry, Owain},
file = {:Users/guillaume.haben/Documents/Mendeley Desktop/2021 - A Survey of Flaky Tests.pdf:pdf},
journal = {ACM transactions on software engineering and methodology},
mendeley-groups = {Flakiness Studies},
number = {1},
title = {{A Survey of Flaky Tests}},
volume = {31},
year = {2021}
}
@article{xu2011ties,
  title={Ties within fault localization rankings: Exposing and addressing the problem},
  author={Xu, Xiaofeng and Debroy, Vidroha and Eric Wong, W and Guo, Donghui},
  journal={International Journal of Software Engineering and Knowledge Engineering},
  volume={21},
  number={06},
  pages={803--827},
  year={2011},
  publisher={World Scientific}
}
@inproceedings{flakime,
  title={FlakiMe: Laboratory-Controlled Test Flakiness Impact Assessment},
  author={Cordy, Maxime and Rwemalika, Renaud and Franci, Adriano and Papadakis, Mike and Harman, Mark},
  booktitle={2021 IEEE/ACM 44rd International Conference on Software Engineering (ICSE)},
  year={2022},
  organization={IEEE}
}

@article{Papadakis:2015sf,
	Author = {Mike Papadakis and Yves Le Traon},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Date-Added = {2016-02-24 07:06:08 +0000},
	Date-Modified = {2019-01-31 12:45:58 +0900},
	Journal = {{Journal of Software Testing, Verification and Reliability}},
	Keywords = {localisation},
	Number = {5-7},
	Pages = {605--628},
	Timestamp = {Fri, 21 Aug 2015 12:11:53 +0200},
	Title = {Metallaxis-FL: mutation-based fault localization},
	Volume = {25},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/stvr.1509}}


@inproceedings{Hong:2015db,
	Author = {Shin Hong and Byeongcheol Lee and Taehoon Kwak and Yiru Jeon and Bongsuk Ko and Yunho Kim and Moonzoo Kim},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Booktitle = {30th {IEEE/ACM} International Conference on Automated Software Engineering, {ASE} 2015, Lincoln, NE, USA, November 9-13, 2015},
	Date-Added = {2016-02-24 06:53:42 +0000},
	Date-Modified = {2019-01-31 12:44:50 +0900},
	Keywords = {localisation},
	Owner = {jjsohn},
	Pages = {464--475},
	Timestamp = {Wed, 13 Jan 2016 17:14:02 +0100},
	Title = {Mutation-Based Fault Localization for Real-World Multilingual Programs {(T)}},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ASE.2015.14}}
	
	
@techreport{Tassey:2002rt,
	Author = {Tassey, Gregory},
	Date-Added = {2009-07-01 13:11:44 +0100},
	Date-Modified = {2009-07-01 13:12:48 +0100},
	Institution = {National Institute of Standards and Technology},
	Month = {May},
	Owner = {jjsohn},
	Timestamp = {2018.03.27},
	Title = {The Economic Impacts of Inadequate Infrastructure for Software Testing},
	Year = {2002},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAgLi4vLi4vUGFwZXJzL1Rhc3NleToyMDAycnRfLi5wZGZPEQFkAAAAAAFkAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8TVGFzc2V5LzIwMDJydF8uLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAACAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACADIvOlVzZXJzOm50cm9sbHM6RHJvcGJveDpQYXBlcnM6VGFzc2V5LzIwMDJydF8uLnBkZgAOACgAEwBUAGEAcwBzAGUAeQAvADIAMAAwADIAcgB0AF8ALgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAMFVzZXJzL250cm9sbHMvRHJvcGJveC9QYXBlcnMvVGFzc2V5OjIwMDJydF8uLnBkZgATAAEvAAAVAAIADv//AAAACAANABoAJABHAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAa8=},
	Bdsk-Url-1 = {http://www.nist.gov/director/prog-ofc/report02-3.pdf}}
	
@Article{Catal2011fc,
  author  = {Cagatay Catal},
  title   = {Software fault prediction: A literature review and current trends},
  journal = {Expert Systems with Applications},
  year    = {2011},
  volume  = {38},
  number  = {4},
  pages   = {4626 - 4636},
}

@inproceedings{Baah:2010fj,
	Acmid = {1831717},
	Author = {Baah, George K. and Podgurski, Andy and Harrold, Mary Jean},
	Booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis (ISSTA 2010)},
	Date-Added = {2010-12-28 16:05:10 +0000},
	Date-Modified = {2012-05-01 08:57:03 +0000},
	Isbn = {978-1-60558-823-0},
	Keywords = {localisation},
	Location = {Trento, Italy},
	Month = {7},
	Numpages = {12},
	Owner = {jjsohn},
	Pages = {73--84},
	Publisher = {{ACM} Press},
	Timestamp = {2018.03.27},
	Title = {Causal inference for statistical fault localization},
	Year = {2010},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAZLi4vLi4vUGFwZXJzL0JhYWgyMDEwLnBkZk8RAUoAAAAAAUoAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////wxCYWFoMjAxMC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAIAAgAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAKy86VXNlcnM6bnRyb2xsczpEcm9wYm94OlBhcGVyczpCYWFoMjAxMC5wZGYAAA4AGgAMAEIAYQBhAGgAMgAwADEAMAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAKVVzZXJzL250cm9sbHMvRHJvcGJveC9QYXBlcnMvQmFhaDIwMTAucGRmAAATAAEvAAAVAAIADv//AAAACAANABoAJABAAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAY4=},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1831708.1831717}}
	
	
	
	
@article{Naish:2011fk,
	Address = {New York, NY, USA},
	Articleno = {11},
	Author = {Naish, Lee and Lee, Hua Jie and Ramamohanarao, Kotagiri},
	Date-Added = {2012-04-19 20:10:06 +0000},
	Date-Modified = {2012-04-19 20:10:39 +0000},
	Journal = {{ACM} {T}ransactions on {S}oftware {E}ngineering {M}ethodology},
	Keywords = {localisation},
	Month = {8},
	Number = {3},
	Numpages = {32},
	Owner = {jjsohn},
	Pages = {11:1--11:32},
	Publisher = {{ACM} Press},
	Timestamp = {2018.03.27},
	Title = {A model for spectra-based software diagnosis},
	Volume = {20},
	Year = {2011},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAdLi4vLi4vUGFwZXJzL05haXNoMjAxMWZrXy5wZGZPEQFaAAAAAAFaAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8QTmFpc2gyMDExZmtfLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAACAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAC8vOlVzZXJzOm50cm9sbHM6RHJvcGJveDpQYXBlcnM6TmFpc2gyMDExZmtfLnBkZgAADgAiABAATgBhAGkAcwBoADIAMAAxADEAZgBrAF8ALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAC1Vc2Vycy9udHJvbGxzL0Ryb3Bib3gvUGFwZXJzL05haXNoMjAxMWZrXy5wZGYAABMAAS8AABUAAgAO//8AAAAIAA0AGgAkAEQAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==}}
	
	
@inproceedings{Wong:2007rt,
	Address = {Washington, DC, USA},
	Author = {Wong, W. Eric and Qi, Yu and Zhao, Lei and Cai, Kai-Yuan},
	Booktitle = {Proceedings of the 31st Annual International Computer Software and Applications Conference - Volume 01},
	Date-Added = {2012-04-21 13:07:16 +0000},
	Date-Modified = {2012-05-01 08:56:44 +0000},
	Isbn = {0-7695-2870-8},
	Keywords = {localisation},
	Numpages = {8},
	Owner = {jjsohn},
	Pages = {449--456},
	Publisher = {IEEE Computer Society},
	Series = {COMPSAC '07},
	Timestamp = {2018.03.27},
	Title = {Effective Fault Localization using Code Coverage},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/COMPSAC.2007.109}}
	
@article{Xie:2013uq,
	Acmid = {2522924},
	Address = {New York, NY, USA},
	Articleno = {31},
	Author = {Xie, Xiaoyuan and Chen, Tsong Yueh and Kuo, Fei-Ching and Xu, Baowen},
	Date-Added = {2014-03-18 12:28:39 +0000},
	Date-Modified = {2016-09-21 13:26:32 +0000},
	Issn = {1049-331X},
	Issue_Date = {October 2013},
	Journal = {{ACM} {T}ransactions on {S}oftware {E}ngineering {M}ethodology},
	Keywords = {localisation},
	Month = {October},
	Number = {4},
	Numpages = {40},
	Owner = {jjsohn},
	Pages = {31:1--31:40},
	Publisher = {ACM},
	Timestamp = {2018.03.27},
	Title = {A Theoretical Analysis of the Risk Evaluation Formulas for Spectrum-based Fault Localization},
	Volume = {22},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAaLi4vLi4vUGFwZXJzL1hpZTIwMTNhYS5wZGZPEQFMAAAAAAFMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8NWGllMjAxM2FhLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAACAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOm50cm9sbHM6RHJvcGJveDpQYXBlcnM6WGllMjAxM2FhLnBkZgAOABwADQBYAGkAZQAyADAAMQAzAGEAYQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAKlVzZXJzL250cm9sbHMvRHJvcGJveC9QYXBlcnMvWGllMjAxM2FhLnBkZgATAAEvAAAVAAIADv//AAAACAANABoAJABBAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAZE=}}
	
@incollection{Xie:2013kx,
	Author = {Xiaoyuan Xie and Fei-Ching Kuo and Tsong Yueh Chen and Shin Yoo and Mark Harman},
	Booktitle = {Search Based Software Engineering},
	Date-Added = {2013-08-25 20:24:35 +0000},
	Date-Modified = {2016-02-24 06:57:46 +0000},
	Editor = {Ruhe, G{\"u}nther and Zhang, Yuanyuan},
	Isbn = {978-3-642-39741-7},
	Keywords = {localisation},
	Owner = {jjsohn},
	Pages = {224-238},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Timestamp = {2018.03.27},
	Title = {Provably Optimal and Human-Competitive Results in SBSE for Spectrum Based Fault Localisation},
	Volume = {8084},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAXLi4vUGFwZXJzL1hpZTIwMTNreC5wZGZPEQGIAAAAAAGIAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADRsplZSCsAAABFOqsNWGllMjAxM2t4LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEU8C83TzicAAAAAAAAAAAABAAIAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAANGyGskAAAARAAgAAM3TT5cAAAABABAARTqrAEUzfABEVmUAApg5AAIAO01hY2ludG9zaCBIRDpVc2VyczoAbnRyb2xsczoARHJvcGJveDoAUGFwZXJzOgBYaWUyMDEza3gucGRmAAAOABwADQBYAGkAZQAyADAAMQAzAGsAeAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAKlVzZXJzL250cm9sbHMvRHJvcGJveC9QYXBlcnMvWGllMjAxM2t4LnBkZgATAAEvAAAVAAIADv//AAAACAANABoAJAA+AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAco=}}

@techreport{Yoo:2012fk,
	Author = {Shin Yoo},
	Date-Added = {2012-05-18 14:01:36 +0000},
	Date-Modified = {2012-05-18 14:02:48 +0000},
	Institution = {{U}niversity {C}ollege {L}ondon},
	Owner = {jjsohn},
	Timestamp = {2018.03.27},
	Title = {{NIA$^3$CIN: Non-Invasive Autonomous and Amortised Adaptivity Code Injection}},
	Year = {2012}}
	
@article{Yoo:2017ss,
	Author = {Shin Yoo and Xiaoyuan Xie and Fei-Ching Kuo and Tsong Yueh Chen and Mark Harman},
	Date-Added = {2018-10-12 14:49:58 +0900},
	Date-Modified = {2018-10-12 14:49:58 +0900},
	Journal = {ACM Transactions on Software Engineering and Methodology},
	Month = {7},
	Number = {1},
	Pages = {4:1--4:30},
	Title = {Human Competitiveness of Genetic Programming in SBFL: Theoretical and Empirical Analysis},
	Volume = {26},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAbLi4vLi4vUGFwZXJzL1lvbzIwMTdzc2EucGRmTxEBUgAAAAABUgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DllvbzIwMTdzc2EucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAgACAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpudHJvbGxzOkRyb3Bib3g6UGFwZXJzOllvbzIwMTdzc2EucGRmAAAOAB4ADgBZAG8AbwAyADAAMQA3AHMAcwBhAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgArVXNlcnMvbnRyb2xscy9Ecm9wYm94L1BhcGVycy9Zb28yMDE3c3NhLnBkZgAAEwABLwAAFQACAA7//wAAAAgADQAaACQAQgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGY}}


@InProceedings{Wong:2014:icsme,
  author    = {C. {Wong} and Y. {Xiong} and H. {Zhang} and D. {Hao} and L. {Zhang} and H. {Mei}},
  title     = {Boosting Bug-Report-Oriented Fault Localization with Segmentation and Stack-Trace Analysis},
  booktitle = {2014 IEEE International Conference on Software Maintenance and Evolution},
  year      = {2014},
  pages     = {181-190},
  doi       = {10.1109/ICSME.2014.40},
}

@InProceedings{Zhou:2012:icse,
  author    = {J. {Zhou} and H. {Zhang} and D. {Lo}},
  title     = {Where should the bugs be fixed? More accurate information retrieval-based bug localization based on bug reports},
  booktitle = {2012 34th International Conference on Software Engineering (ICSE)},
  year      = {2012},
  pages     = {14-24},
  doi       = {10.1109/ICSE.2012.6227210},
}

@inproceedings{Renieres:2003uq,
	Author = {Renieres, M. and Reiss, S.P.},
	Booktitle = {Proceedings of the 18th International Conference on Automated Software Engineering},
	Date-Added = {2011-03-04 18:17:45 +0000},
	Date-Modified = {2011-03-10 10:48:45 +0000},
	Issn = {1527-1366},
	Keywords = {localisation},
	Month = {October},
	Owner = {jjsohn},
	Pages = {30 - 39},
	Timestamp = {2018.03.27},
	Title = {Fault localization with nearest neighbor queries},
	Year = {2003},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAdLi4vLi4vUGFwZXJzL1JlbmllcmVzMjAwMy5wZGZPEQFaAAAAAAFaAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8QUmVuaWVyZXMyMDAzLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAACAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAC8vOlVzZXJzOm50cm9sbHM6RHJvcGJveDpQYXBlcnM6UmVuaWVyZXMyMDAzLnBkZgAADgAiABAAUgBlAG4AaQBlAHIAZQBzADIAMAAwADMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAC1Vc2Vycy9udHJvbGxzL0Ryb3Bib3gvUGFwZXJzL1JlbmllcmVzMjAwMy5wZGYAABMAAS8AABUAAgAO//8AAAAIAA0AGgAkAEQAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ASE.2003.1240292}}
	
	
@INPROCEEDINGS{Wen:2012icse,  author={Wen, Wanzhi},  booktitle={2012 34th International Conference on Software Engineering (ICSE)},   title={Software fault localization based on program slicing spectrum},   year={2012},  volume={},  number={},  pages={1511-1514},  doi={10.1109/ICSE.2012.6227049}}


@inproceedings{Li:2018:icse,
author = {Li, Xiangyu and Zhu, Shaowei and d'Amorim, Marcelo and Orso, Alessandro},
title = {Enlightened Debugging},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180242},
doi = {10.1145/3180155.3180242},
abstract = {Numerous automated techniques have been proposed to reduce the cost of software debugging, a notoriously time-consuming and human-intensive activity. Among these techniques, Statistical Fault Localization (SFL) is particularly popular. One issue with SFL is that it is based on strong, often unrealistic assumptions on how developers behave when debugging. To address this problem, we propose Enlighten, an interactive, feedback-driven fault localization technique. Given a failing test, Enlighten (1) leverages SFL and dynamic dependence analysis to identify suspicious method invocations and corresponding data values, (2) presents the developer with a query about the most suspicious invocation expressed in terms of inputs and outputs, (3) encodes the developer feedback on the correctness of individual data values as extra program specifications, and (4) repeats these steps until the fault is found. We evaluated Enlighten in two ways. First, we applied Enlighten to 1,807 real and seeded faults in 3 open source programs using an automated oracle as a simulated user; for over 96% of these faults, Enlighten required less than 10 interactions with the simulated user to localize the fault, and a sensitivity analysis showed that the results were robust to erroneous responses. Second, we performed an actual user study on 4 faults with 24 participants and found that participants who used Enlighten performed significantly better than those not using our tool, in terms of both number of faults localized and time needed to localize the faults.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {82–92},
numpages = {11},
keywords = {dynamic analysis, debugging, fault localization},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}
@inproceedings{parry2022surveying,
  title={Surveying the developer experience of flaky tests},
  author={Parry, Owain and Kapfhammer, Gregory M and Hilton, Michael and McMinn, Phil},
  booktitle={Proceedings of the International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
  year={2022}
}
@inproceedings{gruber2022survey,
  title={A Survey on How Test Flakiness Affects Developers and What Support They Need To Address It},
  author={Gruber, Martin and Fraser, Gordon},
  booktitle={Proceedings of the 15th IEEE International Conference on Software Testing, Verification and Validation},
  year={2022},
  series = {ICST '22}
}
@inproceedings{gyori2016nondex,
  title={NonDex: A tool for detecting and debugging wrong assumptions on Java API specifications},
  author={Gyori, Alex and Lambeth, Ben and Shi, August and Legunsen, Owolabi and Marinov, Darko},
  booktitle={Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  pages={993--997},
  year={2016}
}
@inproceedings{flake16,
  title={Evaluating Features for Machine Learning Detection of Order-and Non-Order-Dependent Flaky Tests},
  author={Parry, Owain and Kapfhammer, Gregory M and Hilton, Michael and McMinn, Phil},
  booktitle={2022 IEEE Conference on Software Testing, Verification and Validation (ICST)},
  pages={93--104},
  year={2022},
  organization={IEEE}
}

@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}
@article{sun2021research,
  title={Research progress on few-shot learning for remote sensing image interpretation},
  author={Sun, Xian and Wang, Bing and Wang, Zhirui and Li, Hao and Li, Hengchao and Fu, Kun},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume={14},
  pages={2387--2402},
  year={2021},
  publisher={IEEE}
}
@article{he2020vul,
  title={Vul-mirror: a few-shot learning method for discovering vulnerable code clone},
  author={He, Yuan and Wang, Wenjie and Sun, Hongyu and Zhang, Yuqing},
  journal={EAI Endorsed Transactions on Security and Safety},
  volume={7},
  number={23},
  pages={e4},
  year={2020}
}
@article{khajezade2022evaluating,
  title={Evaluating few shot and Contrastive learning Methods for Code Clone Detection},
  author={Khajezade, Mohamad and Fard, Fatemeh Hendijani and Shehata, Mohamed S},
  journal={arXiv preprint arXiv:2204.07501},
  year={2022}
}

@inproceedings{tsdetect,
author = {Peruma, Anthony and Almalki, Khalid and Newman, Christian D. and Mkaouer, Mohamed Wiem and Ouni, Ali and Palomba, Fabio},
title = {TsDetect: An Open Source Test Smells Detection Tool},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417921},
doi = {10.1145/3368089.3417921},
abstract = {The test code, just like production source code, is subject to bad design and programming practices, also known as smells. The presence of test smells in a software project may affect the quality, maintainability, and extendability of test suites making them less effective in finding potential faults and quality issues in the project's production code. In this paper, we introduce tsDetect, an automated test smell detection tool for Java software systems that uses a set of detection rules to locate existing test smells in test code. We evaluate the effectiveness of tsDetect on a benchmark of 65 unit test files containing instances of 19 test smell types. Results show that tsDetect achieves a high detection accuracy with an average precision score of 96\% and an average recall score of 97\%. tsDetect is publicly available, with a demo video, at: https://testsmells.github.io/},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1650–1654},
numpages = {5},
keywords = {Test Smells, Software Quality, Detection Tool},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}
@article{shahin2017continuous,
  title={Continuous integration, delivery and deployment: a systematic review on approaches, tools, challenges and practices},
  author={Shahin, Mojtaba and Babar, Muhammad Ali and Zhu, Liming},
  journal={IEEE Access},
  volume={5},
  pages={3909--3943},
  year={2017},
  publisher={IEEE}
}

@inproceedings{memon2017taming,
  title={Taming Google-scale continuous testing},
  author={Memon, Atif and Gao, Zebao and Nguyen, Bao and Dhanda, Sanjeev and Nickell, Eric and Siemborski, Rob and Micco, John},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP)},
  pages={233--242},
  year={2017},
  organization={IEEE}
}

@inproceedings{li2022repairing,
  title={Repairing order-dependent flaky tests via test generation},
  author={Li, Chengpeng and Zhu, Chenguang and Wang, Wenxi and Shi, August},
  year={2022},
  organization={ICSE},
  booktitle = {Proceedings of the 44th International Conference on Software Engineering - ICSE '22}
}

@misc{shaponline,
author = {Scott Lundberg},
title = {SHAP documentation},
howpublished = {\url{https://shap.readthedocs.io/}},
month = {},
year = {2018},
note = {(Accessed on 06/23/2022)}
}

@article{zeller2002simplifying,
  title={Simplifying and isolating failure-inducing input},
  author={Zeller, Andreas and Hildebrandt, Ralf},
  journal={IEEE Transactions on Software Engineering},
  volume={28},
  number={2},
  pages={183--200},
  year={2002},
  publisher={IEEE}
}

@misc{IGonline,
author = {},
title = {Permutation Importance vs Random Forest Feature Importance},
howpublished = {\url{https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#tree-s-feature-importance-from-mean-decrease-in-impurity-mdi}},
month = {},
year = {2022},
note = {(Accessed on 06/23/2022)}
}

@inproceedings{lampel2021life,
  title={When life gives you oranges: detecting and diagnosing intermittent job failures at Mozilla},
  author={Lampel, Johannes and Just, Sascha and Apel, Sven and Zeller, Andreas},
  booktitle={Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1381--1392},
  year={2021}
}

@article{carvalho2019machine,
  title={Machine learning interpretability: A survey on methods and metrics},
  author={Carvalho, Diogo V and Pereira, Eduardo M and Cardoso, Jaime S},
  journal={Electronics},
  volume={8},
  number={8},
  pages={832},
  year={2019},
  publisher={MDPI}
}

@article{wan2022they,
  title={What Do They Capture?--A Structural Analysis of Pre-Trained Language Models for Source Code},
  author={Wan, Yao and Zhao, Wei and Zhang, Hongyu and Sui, Yulei and Xu, Guandong and Jin, Hai},
  journal={arXiv preprint arXiv:2202.06840},
  year={2022}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}

@article{attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{across_pr,  
author={Costa, Keila and Ferreira, Ronivaldo and Pinto, Gustavo and d'Amorim, Marcelo and Miranda, Breno},  
journal={IEEE Transactions on Software Engineering},   
title={Test Flakiness Across Programming Languages},   year={2022},  volume={},  number={},  pages={1-14},  doi={10.1109/TSE.2022.3208864}}



@article{habchi2022made,
  title={What Made This Test Flake? Pinpointing Classes Responsible for Test Flakiness},
  author={Habchi, Sarra and Haben, Guillaume and Sohn, Jeongju and Franci, Adriano and Papadakis, Mike and Cordy, Maxime and Le Traon, Yves},
  journal={arXiv e-prints},
  pages={arXiv--2207},
  year={2022}
}

@inproceedings{li2022evolution,
  title={Evolution-aware detection of order-dependent flaky tests},
  author={Li, Chengpeng and Shi, August},
  booktitle={Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages={114--125},
  year={2022}
}

@inproceedings{parry2022evaluating,
  title={Evaluating Features for Machine Learning Detection of Order-and Non-Order-Dependent Flaky Tests},
  author={Parry, Owain and Kapfhammer, Gregory M and Hilton, Michael and McMinn, Phil},
  booktitle={2022 IEEE Conference on Software Testing, Verification and Validation (ICST)},
  pages={93--104},
  year={2022},
  organization={IEEE}
}

@misc{TestReporter,
author = {Michal Dorner},
title = {Test Reporter · Actions · GitHub Marketplace},
howpublished = {\url{https://github.com/marketplace/actions/test-reporter}},
month = {7},
year = {2021},
note = {(Accessed on 08/23/2022)}
}

@misc{flakicat,
  doi = {10.48550/ARXIV.2208.14799},
  url = {https://arxiv.org/abs/2208.14799},
  author = {Akli, Amal and Haben, Guillaume and Habchi, Sarra and Papadakis, Mike and Traon, Yves Le},
  keywords = {Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Predicting Flaky Tests Categories using Few-Shot Learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{olewickiBrown,
author = {Olewicki, Doriane and Nayrolles, Mathieu and Adams, Bram},
title = {Towards Language-Independent Brown Build Detection},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510122},
doi = {10.1145/3510003.3510122},
abstract = {In principle, continuous integration (CI) practices allow modern software organizations to build and test their products after each code change to detect quality issues as soon as possible. In reality, issues with the build scripts (e.g., missing dependencies) and/or the presence of "flaky tests" lead to build failures that essentially are false positives, not indicative of actual quality problems of the source code. For our industrial partner, which is active in the video game industry, such "brown builds" not only require multidisciplinary teams to spend more effort interpreting or even re-running the build, leading to substantial redundant build activity, but also slows down the integration pipeline. Hence, this paper aims to prototype and evaluate approaches for early detection of brown build results based on textual similarity to build logs of prior brown builds. The approach is tested on 7 projects (6 closed-source from our industrial collaborators and 1 open-source, Graphviz). We find that our model manages to detect brown builds with a mean F1-score of 53\% on the studied projects, which is three times more than the best baseline considered, and at least as good as human experts (but with less effort). Furthermore, we found that cross-project prediction can be used for a project's onboarding phase, that a training set of 30-weeks works best, and that our retraining heuristics keep the F1-score higher than the baseline, while retraining only every 4--5 weeks.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2177–2188},
numpages = {12},
keywords = {continuous integration, concept drift, classification, build automation, brown build},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}
@article{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011},
  pages = {},
  numpages = {9},
}
@misc{countvectorizer,
author = {Scikit-learn developers},
title = {sklearn.feature\_extraction.text.CountVectorizer — scikit-learn 1.1.2 documentation},
howpublished = {\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html}},
month = {8},
year = {2022},
note = {(Accessed on 08/19/2022)}
}
@misc{onehotencoding,
author = {Scikit-learn developers},
title = {sklearn.preprocessing.OneHotEncoder — scikit-learn 1.1.2 documentation},
howpublished = {\url{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html}},
month = {8},
year = {2022},
note = {(Accessed on 08/10/2022)}
}

@INPROCEEDINGS{Habchi2022Qualitative,
  author={Habchi, Sarra and Haben, Guillaume and Papadakis, Mike and Cordy, Maxime and Traon, Yves Le},
  booktitle={2022 IEEE Conference on Software Testing, Verification and Validation (ICST)}, 
  title={A Qualitative Study on the Sources, Impacts, and Mitigation Strategies of Flaky Tests}, 
  year={2022},
  volume={},
  number={},
  pages={244-255},
  doi={10.1109/ICST53961.2022.00034}}

  
@inproceedings{qin2022peeler,
  author={Qin, Yihao and Wang, Shangwen and Liu, Kui and Lin, Bo and Wu, Hongjun and Li, Li and Mao, Xiaoguang and Bissyandé, Tegawendé F.},
  booktitle={2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={Peeler: Learning to Effectively Predict Flakiness without Running Tests}, 
  year={2022},
  volume={},
  number={},
  publisher={ICSME},
  address={Limassol, Cyprus},
  pages={257-268},
  doi={10.1109/ICSME55016.2022.00031}}

@misc{akliflakycat,
  doi = {10.48550/ARXIV.2208.14799},
  url = {https://arxiv.org/abs/2208.14799},
  author = {Akli, Amal and Haben, Guillaume and Habchi, Sarra and Papadakis, Mike and Traon, Yves Le},
  keywords = {Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Predicting Flaky Tests Categories using Few-Shot Learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{habchiPinpointing,
  author={Habchi, Sarra and Haben, Guillaume and Sohn, Jeongju and Franci, Adriano and Papadakis, Mike and Cordy, Maxime and Traon, Yves Le},
  booktitle={2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={What Made This Test Flake? Pinpointing Classes Responsible for Test Flakiness}, 
  year={2022},
  volume={},
  number={},
  pages={352-363},
  doi={10.1109/ICSME55016.2022.00039}}

@article{SleepRacer,
abstract = {Assuring quality of web applications is fundamental, given their relevance in the today's world. A possible way to reach this goal is through end-to-end (E2E) testing, an approach in which a web application is automatically tested by performing the actions that a user would do. With modern web applications (for example, single-page applications), it is of great importance to properly handle asynchronous calls in the test suite. In E2E Selenium WebDriver test suites, asynchronous calls are usually managed in two ways: using thread sleeps or explicit waits. The first is easier to use, but is inefficient and can lead to instability (also called flakiness, a problem often present in test suites that makes us lose confidence in the testing phase), while the second is usually more efficient but harder to use because, if the correct kind of wait is not carefully selected, it can introduce flakiness too. To help Testers, who often opt for the first strategy, we present in this work a tool-based approach to automatically replace thread sleeps with explicit waits in an E2E Selenium WebDriver test suite without introducing new flakiness. We empirically validated our tool named SleepReplacer on four different test suites, and we found that it can correctly replace in an automatic way from 81 to 100\% of thread sleeps, leading to a significant reduction of the total execution time of the test suite (i.e., from 13 to 71\%).},
author = {Olianas, Dario and Leotta, Maurizio and Ricca, Filippo},
doi = {10.1007/s11219-022-09596-z},
issn = {1573-1367},
journal = {Software Quality Journal},
title = {{SleepReplacer: a novel tool-based approach for replacing thread sleeps in selenium WebDriver test code}},
url = {https://doi.org/10.1007/s11219-022-09596-z},
year = {2022},
number = {},
volume = {1},
pages = {55-65}
}

@article{smote,
  title={SMOTE: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of artificial intelligence research},
  volume={16},
  pages={321--357},
  year={2002}
}
@misc{selectkbest,
author = {Scikit-learn developers},
title = {sklearn.feature\_selection.SelectKBest — scikit-learn 1.1.2 documentation},
howpublished = {\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html}},
month = {8},
year = {2022},
note = {(Accessed on 08/11/2022)}
}
@inproceedings{sohn2021assisting,
  title={Assisting Bug Report Assignment Using Automated Fault Localisation: An Industrial Case Study},
  author={Sohn, Jeongju and An, Gabin and Hong, Jingun and Hwang, Dongwon and Yoo, Shin},
  booktitle={2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)},
  pages={284--294},
  year={2021},
  publisher={IEEE},
  address={Online event}
}
@inproceedings{mehta2021data,
  title={Data-driven test selection at scale},
  author={Mehta, Sonu and Farmahinifarahani, Farima and Bhagwan, Ranjita and Guptha, Suraj and Jafari, Sina and Kumar, Rahul and Saini, Vaibhav and Santhiar, Anirudh},
  booktitle={Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1225--1235},
  year={2021},
  publisher={ACM},
  address={Athens, Greece}
}
@inproceedings{bertolino2020learning,
  title={Learning-to-rank vs ranking-to-learn: Strategies for regression testing in continuous integration},
  author={Bertolino, Antonia and Guerriero, Antonio and Miranda, Breno and Pietrantuono, Roberto and Russo, Stefano},
  booktitle={Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  pages={1--12},
  year={2020},
  publisher={IEEE},
  address={Online event}
}
@inproceedings{zhang2018hybrid,
  title={Hybrid regression test selection},
  author={Zhang, Lingming},
  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},
  pages={199--209},
  year={2018},
  publisher={IEEE},
  address={Gothenburg, Sweden}
}
@inproceedings{ziftci2020flake,
  title={De-flake your tests: Automatically locating root causes of flaky tests in code at google},
  author={Ziftci, Celal and Cavalcanti, Diego},
  booktitle={2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={736--745},
  year={2020},
  publisher={IEEE},
  address={Online event},
  organization={IEEE}
}
@inproceedings{Memon2017,
author = {Memon, Atif and {Zebao Gao} and {Bao Nguyen} and Dhanda, Sanjeev and Nickell, Eric and Siemborski, Rob and Micco, John},
booktitle = {2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP)},
doi = {10.1109/ICSE-SEIP.2017.16},
file = {:Users/guillaume.haben/Documents/Mendeley Desktop/2017 - Taming Google-scale continuous testing.pdf:pdf},
isbn = {978-1-5386-2717-4},
keywords = {-software testing,continuous integration,selection},
mendeley-groups = {Continuous Integration},
month = {may},
pages = {233--242},
publisher = {IEEE},
title = {{Taming Google-scale continuous testing}},
url = {http://ieeexplore.ieee.org/document/7965447/},
year = {2017},
address={Buenos Aires, Argentina}
}

@inproceedings{lampelOrange,
author = {Lampel, Johannes and Just, Sascha and Apel, Sven and Zeller, Andreas},
title = {When Life Gives You Oranges: Detecting and Diagnosing Intermittent Job Failures at Mozilla},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.bnl.lu/10.1145/3468264.3473931},
doi = {10.1145/3468264.3473931},
abstract = {Continuous delivery of cloud systems requires constant running of jobs (build processes, tests, etc.). One issue that plagues this continuous integration (CI) process are intermittent failures - non-deterministic, false alarms that do not result from a bug in the software or job specification, but rather from issues in the underlying infrastructure. At Mozilla, such intermittent failures are called oranges as a reference to the color of the build status indicator. As such intermittent failures disrupt CI and lead to failures, they erode the developers' trust in the jobs. We present a novel approach that automatically classifies failing jobs to determine whether job execution failures arise from an actual software bug or were caused by flakiness in the job (e.g., test) or the underlying infrastructure. For this purpose, we train classification models using job telemetry data to diagnose failure patterns involving features such as runtime, cpu load, operating system version, or specific platform with high precision. In an evaluation on a set of Mozilla CI jobs, our approach achieves precision scores of 73\%, on average, across all data sets with some test suites achieving precision scores good enough for fully automated classification (i.e., precision scores of up to 100\%), and recall scores of 82\% on average (up to 94\%).},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1381–1392},
numpages = {12},
keywords = {intermittent failures, Software testing, flaky tests, machine learning, continuous integration},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}
@article{chen2004using,
  title={Using random forest to learn imbalanced data},
  author={Chen, Chao and Liaw, Andy and Breiman, Leo and others},
  journal={University of California, Berkeley},
  volume={110},
  number={1-12},
  pages={24},
  year={2004}
}

@article{ParryKHM22,
  author    = {Owain Parry and
               Gregory M. Kapfhammer and
               Michael Hilton and
               Phil McMinn},
  title     = {A Survey of Flaky Tests},
  journal   = {{ACM} Trans. Softw. Eng. Methodol.},
  volume    = {31},
  number    = {1},
  pages     = {17:1--17:74},
  year      = {2022},
  url       = {https://doi.org/10.1145/3476105},
  doi       = {10.1145/3476105},
}