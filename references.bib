@article{Ukkonen1985,
abstract = {The edit distance between strings a1 ... am and b1 ... bn is the minimum cost s of a sequence of editing steps (insertions, deletions, changes) that convert one string into the other. A well-known tabulating method computes s as well as the corresponding editing sequence in time and in space O(mn) (in space O(min(m, n)) if the editing sequence is not required). Starting from this method, we develop an improved algorithm that works in time and in space O(s ?? min(m, n)). Another improvement with time O(s ?? min(m, n)) and space O(s ?? min(s, m, n)) is given for the special case where all editing steps have the same cost independently of the characters involved. If the editing sequence that gives cost s is not required, our algorithms can be implemented in space O(min(s, m, n)). Since s = O(max(m, n)), the new methods are always asymptotically as good as the original tabulating method. As a by-product, algorithms are obtained that, given a threshold value t, test in time O(t ?? min(m, n)) and in space O(min(t, m, n)) whether s ??? t. Finally, different generalized edit distances are analyzed and conditions are given under which our algorithms can be used in conjunction with extended edit operation sets, including, for example, transposition of adjacent characters. ?? 1985 Academic Press, Inc.},
author = {Ukkonen, Esko},
doi = {10.1016/S0019-9958(85)80046-2},
issn = {00199958},
journal = {Information and Control},
month = {1},
number = {1-3},
pages = {100--118},
title = {{Algorithms for approximate string matching}},
volume = {64},
year = {1985}
}

@inproceedings{Myers1992,
address = {New York, New York, USA},
author = {Myers, Brad A. and Rosson, Mary Beth},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '92},
doi = {10.1145/142750.142789},
isbn = {0897915135},
pages = {195--202},
publisher = {ACM Press},
title = {{Survey on user interface programming}},
url = {http://portal.acm.org/citation.cfm?doid=142750.142789},
year = {1992}
}


@article{Myers1994,
author = {Myers, Brad},
doi = {10.1145/174800.174808},
issn = {1072-5520},
journal = {Interactions},
month = {1},
number = {1},
pages = {73--83},
title = {{Challenges of HCI design and implementation}},
volume = {1},
year = {1994}
}


@inproceedings{Baker1995,
author={Baker, B.S.},
booktitle={Reverse Engineering, 1995., Proceedings of 2nd Working Conference on},
title={On finding duplication and near-duplication in large software systems},
year=1995,
pages={86-95},
keywords={program debugging;software tools;system documentation;systems analysis;systems re-engineering;constants;debugging;dup;experimental results;large software systems;redocumentation;software duplication;system reengineering;systematic substitution;variable names;Application software;Computer bugs;Programming profession;Reverse engineering;Scattering parameters;Sections;Software systems;Terminology;White spaces},
doi={10.1109/WCRE.1995.514697},
month={7},
}

@article{Myers1995,
abstract = {Almost as long as there have been user interfaces, there have been special software systems and tools to help design and implement the user interface software. Many of these tools have demonstrated significant productivity gains for programmers, and have become important commercial products. Others have proven less successful at supporting the kinds of user interfaces people want to build. This article discusses the different kinds of user interface software tools, and investigates why some approaches have worked and others have not. Many examples of commercial and research systems are included. Finally, current research directions and open issues in the field are discussed. {\textcopyright} 1995, ACM. All rights reserved.},
author = {Myers, Brad A.},
doi = {10.1145/200968.200971},
issn = {15577325},
journal = {ACM Transactions on Computer-Human Interaction (TOCHI)},
keywords = {interface builders,toolkits,user interface development environments,user interface software},
number = {1},
pages = {64--103},
title = {{User Interface Software Tools}},
volume = {2},
year = {1995}
}

@inproceedings{Chawathe1996,
 author = {Chawathe, Sudarshan S. and Rajaraman, Anand and Garcia-Molina, Hector and Widom, Jennifer},
 title = {Change Detection in Hierarchically Structured Information},
 booktitle = {Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data},
 series = {SIGMOD '96},
 year = 1996,
 isbn = {0-89791-794-4},
 location = {Montreal, Quebec, Canada},
 pages = {493--504},
 numpages = 12,
 doi = {10.1145/233269.233366},
 acmid = 233366,
 publisher = {ACM},
 address = {New York, NY, USA},
}

@book{Fowler1999,
title = {Refactoring: Improving the Design of Existing Code},
author = {Fowler, Martin and Beck, Kent and Brant, John and Opdyke, William and Roberts, Don},
year = {1999},
isbn = {0201485672},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA}
}

@article{Ronsse1999,
abstract = {This article presents a practical solution for the cyclic debugging of nondeterministic parallel programs. The solution consists of a combination of record/replay with automatic on-the-fly data race detection. This combination enables us to limit the record phase to the more efficient recording of the synchronization operations, while deferring the time-consuming data race detection to the replay phase. As the record phase is highly efficient, there is no need to switch it off, hereby eliminating the possibility of Heisenbugs because tracing can be left on all the time. This article describes an implementation of the tools needed to support RecPlay.},
author = {Ronsse, Michiel and {De Bosschere}, Koen},
doi = {10.1145/312203.312214},
issn = {07342071},
journal = {ACM Transactions on Computer Systems},
keywords = {D.1.3 [Programming Techniques]: Concurrent Programming - Parallel programming,D.2.5 [Software Engineering]: Testing and Debugging -Debugging aids,D.4.1 [Operating Systems]: Process Management - Concurrency,Deadlocks,Monitors,Tracing},
number = {2},
pages = {133--152},
title = {{RecPlay: A fully integrated practical record/replay system}},
volume = {17},
year = {1999}
}

@inproceedings{VanDeursen2001,
abstract = {Two key aspects of extreme programming (XP) are unit testing and merciless refactoring. Given the fact that the ideal test code / production code ratio approaches 1:1, it is not surprising that unit tests are being refactored. We found that refactoring test code is different from refactoring pro- duction code in two ways: (1) there is a distinct set of bad smells involved, and (2) improving test code involves ad- ditional test-specific refactorings. To share our experiences with other XP practitioners, we describe a set of bad smells that indicate trouble in test code, and a collection of test refactorings to remove these smells.},
author = {van Deursen, Arie and Moonen, Leon and van den Bergh, Alex and Kok, Gerard},
booktitle = {Proceedings of the 2nd International Conference on Extreme Programming and Flexible Processes in Software Engineering (XP)},
doi = {10.1134/S106422690609004X},
issn = {1064-2269},
keywords = {extreme programming,refactoring,unit testing},
month = {9},
pages = {92--95},
title = {{Simulation of corrugated horns and exciters for circular corrugated waveguides}},
year = {2001}
}

@article{Gottlob2002,
abstract = {Several important decision problems on conjunctive queries (CQs) are NP-complete in general but become tractable, and actually highly parallelizable, if restricted to acyclic or nearly acyclic queries. Examples are the evaluation of Boolean CQs and query containment. These problems were shown tractable for conjunctive queries of bounded treewidth (Ch. Chekuri and A. Rajaraman, Theoret. Comput. Sci. 239 (2000), 211-229), and of bounded degree of cyclicity (M. Gyssens et al., Artif. Intell. 66 (1994), 57-89; M. Gyssens and J. Paredaens, in "Advances in Database Theory," Vol. 2, pp. 85-122, Plenum Press, New York, 1984). The so far most general concept of nearly acyclic queries was the notion of queries of bounded query-width introduced by Chekuri and Rajaraman (2000). While CQs of bounded query-width are tractable, it remained unclear whether such queries are efficiently recognizable. Chekuri and Rajaraman (2000) stated as an open problem whether for each constant k it can be determined in polynomial time if a query has query-width at most k. We give a negative answer by proving the NP-completeness of this problem (specifically, for k = 4). In order to circumvent this difficulty, we introduce the new concept of hypertree decomposition of a query and the corresponding notion of hypertree-width. We prove: (a) for each k, the class of queries with query-width bounded by k is properly contained in the class of queries whose hypertree-width is bounded by k; (b) unlike query-width, constant hypertree-width is efficiently recognizable; and (c) Boolean queries of bounded hypertree-width can be efficiently evaluated. {\textcopyright} 2002 Elsevier Science (USA).},
author = {Gottlob, Georg and Leone, Nicola and Scarcello, Francesco},
journal = {Journal of Computer and System Sciences},
month = {5},
number = {3},
pages = {579--627},
title = {{Hypertree Decompositions and Tractable Queries}},
volume = {64},
year = {2002}
}

@inproceedings{Barton2003,
abstract = {We present a streaming algorithm for evaluating XPath expressions that use backward axes (parent and ancestor) and forward axes in a single document-order traversal of an XML document. Other streaming XPath processors handle only forward axes. We show through experiments that our algorithm significantly outperforms (by more than a factor of two) a traditional non-streaming XPath engine. Furthermore, our algorithm scales better because it retains only the relevant portions of the input document in memory. Our engine successfully processes documents over 1GB in size, whereas the traditional XPath engine degrades considerably in performance for documents over 100 MB in size and fails to complete for documents of size over 200 MB.},
author = {Barton, Charles and Charles, Philippe and {Deepak Goyal} and {Mukund Raghavachari} and Fontoura, Marcus and Josifovski, Vanja},
booktitle = {Proceedings 19th International Conference on Data Engineering (Cat. No.03CH37405)},
pages = {455--466},
publisher = {IEEE},
title = {{Streaming XPath processing with forward and backward axes}},
year = {2003}
}

@inproceedings{Memon2003a,
abstract = {Although graphical user interfaces (GUIs) constitute a large part of the software being developed today and are typically created using rapid prototyping, there are no effective regression testing techniques for GUIs. The needs of GUI regression testing differ from those of traditional software. When the structure of a GUI is modified, test cases from the original GUI are either reusable or unusable on the modified GUI. Since GUI test case generation is expensive, our goal is to make the unusable test cases usable. The idea of reusing these unusable (a.k.a. obsolete) test cases has not been explored before. In this paper, we show that for GUIs, the unusability of a large number of test cases is a serious problem. We present a novel GUI regression testing technique that first automatically determines the usable and unusable test cases from a test suite after a GUI modification. It then determines which of the unusable test cases can be repaired so they can execute on the modified GUI. The last step is to repair the test cases. Our technique is integrated into a GUI testing framework that, given a test case, automatically executes it on the GUI. We implemented our regression testing technique and demonstrate for two case studies that our approach is effective in that many of the test cases can be repaired, and is practical in terms of its time performance.},
address = {New York, New York, USA},
author = {Memon, Atif M. and Soffa, Mary Lou},
booktitle = {Proceedings of the 9th European software engineering conference held jointly with 10th ACM SIGSOFT international symposium on Foundations of software engineering - ESEC/FSE '03},
doi = {10.1145/940071.940088},
isbn = {1581137435},
issn = {01635948},
keywords = {Regression testing, repairing test cases, GUI testing, GUI control-flow graph, GUI call-graph, call-tree, classification of events},
number = {5},
pages = {118},
publisher = {ACM Press},
title = {{Regression testing of GUIs}},
volume = {28},
year = {2003}
}


@inproceedings{Marinescu2004,
abstract = {In order to support the maintenance of an object-oriented software system, the quality of its design must be evaluated using adequate quantification means. In spite of the current extensive use of metrics, if used in isolation metrics are oftentimes too fine grained to quantify comprehensively an investigated design aspect (e.g., distribution of system's intelligence among classes). To help developers and maintainers detect and localize design problems in a system, we propose a novel mechanism - called detection strategy - for formulating metrics-based rules that capture deviations from good design principles and heuristics. Using detection strategies an engineer can directly localize classes or methods affected by a particular design flaw (e.g., God Class), rather than having to infer the real design problem from a large set of abnormal metric values. We have defined such detection strategies for capturing around ten important flaws of object-oriented design found in the literature and validated the approach experimentally on multiple large-scale case-studies. {\textcopyright} 2004 IEEE.},
author = {Marinescu, Radu},
booktitle = {20th IEEE International Conference on Software Maintenance, 2004. Proceedings.},
doi = {10.1109/ICSM.2004.1357820},
isbn = {0-7695-2213-0},
keywords = {Design flaws,Design heuristics,Metrics,Object-oriented design,Quality assurance},
pages = {350--359},
publisher = {IEEE},
title = {{Detection strategies: metrics-based rules for detecting design flaws}},
year = {2004}
}

@inproceedings{Skoglund2004,
author = {Skoglund, Mats and Runeson, Per},
booktitle = {IEEE International Conference on Software Maintenance, ICSM},
doi = {10.1109/ICSM.2004.1357831},
isbn = {0-7695-2213-0},
issn = {1063-6773},
pages = {438--442},
title = {{A case study on regression test suite maintenance in system evolution}},
year = {2004}
}

@article{Anton2005,
abstract = {We introduce a wrapper induction algorithm for extracting information from tree-structured docu- ments like HTML or XML. It derives XPath- compatible extraction rules from a set of anno- tated example documents. The approach builds a minimally generalized tree traversal pattern, and augments it with conditions. Another variant se- lects a subset of conditions so that (a) the pattern is consistent with the training data, (b) the pat- tern's document coverage is minimized, and (c) conditions that match structures preceding the target nodes are preferred. We discuss the ro- bustness of rules induced by this selection strat- egy and we illustrate how these rules exhibit knowledge of the target concept.},
author = {Anton, Tobias},
journal = {LWA 2005 - Workshopwoche der GI-Fachgruppen/Arbeitskreise},
pages = {126--133},
title = {{XPath-Wrapper Induction by generalizing tree traversal patterns}},
year = {2005}
}

@inproceedings{Berner2005,
abstract = {This report addresses some of our observations made in a dozen of projects in the area of software testing, and more specifically, in automated testing. It documents, analyzes and consolidates what we consider to be of interest to the community. The major findings can be summarized in a number of lessons learned, covering test strategy, testability, daily integration, and best practices. The report starts with a brief description of five sample projects. Then, we discuss our observations and experiences and illustrate them with the sample projects. The report concludes with a synopsis of these experiences and with suggestions for future test automation endeavors. Copyright 2005 ACM.},
author = {Berner, Stefan and Weber, Roland and Keller, R.K.},
booktitle = {Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.},
doi = {10.1109/ICSE.2005.1553603},
isbn = {1-59593-963-2},
issn = {02705257},
keywords = {Automated testing,Software test,Test management},
pages = {571--579},
publisher = {IEEE},
title = {{Observations and lessons learned from automated testing}},
volume = {2005},
year = {2005}
}


@article{Gottlob2005,
abstract = {We study the complexity of two central XML processing problems. The first is XPath 1.0 query processing, which has been shown to be in PTIME in previous work. We prove that both the data complexity and the query complexity of XPath 1.0 fall into lower (highly parallelizable) complexity classes, while the combined complexity is PTIME-hard. Subsequently, we study the sources of this hardness and identify a large and practically important fragment of XPath 1.0 for which the combined complexity is LOGCFL-complete and, therefore, in the highly parallelizable complexity class NC2. The second problem is the complexity of validating XML documents against various typing schemes like Document Type Definitions (DTDs), XML Schema Definitions (XSDs), and tree automata, both with respect to data and to combined complexity. For data complexity, we prove that validation is in LOGSPACE and depends crucially on how XML data is represented. For the combined complexity, we show that the complexity ranges from LOGSPACE to LOGCFL, depending on the typing scheme. {\textcopyright} 2005 ACM.},
author = {Gottlob, Georg and Koch, Christoph and Pichler, Reinhard and Segoufin, Luc},
journal = {Journal of the ACM},
keywords = {Complexity,DTD,LOGCFL,XML,XPath},
pages = {284--335},
title = {{The complexity of XPath query evaluation and XML typing}},
volume = {52},
year = {2005}
}

@inproceedings{Elssamadisy2006,
abstract = {Functional tests are automated, business process tests co-owned by customers and developers. They are particularly useful for rescuing projects from high bug counts, delayed releases, and dissatisfied customers. Functional tests help projects by elucidating requirements, making project progress visible, and preventing bugs. We present functional testing in pattern format because it is especially expressive in conveying expert advice and enables the reader to make an informed decision regarding the applicability of the solution. The pattern presented aggregates multiple experiences with functional testing over several agile development projects. However, we have seen functional testing become more costly than its benefits, so we describe the symptoms - smells - of potentially costly problems. These are not problems with functional testing per se, but with the misinterpretation and mis-implementation of this practice. We draw on our experience to suggest ways of addressing these smells. Done right, functional testing successfully increases software's quality and business value. Copyright 2006 ACM.},
address = {New York, New York, USA},
author = {Elssamadisy, Amr and Whitmore, Jean},
booktitle = {Proceedings of the 2006 conference on Pattern languages of programs - PLoP '06},
doi = {10.1145/1415472.1415504},
isbn = {9781605583723},
keywords = {Acceptance testing,Agile development practices,Functional testing,Patterns},
pages = {10},
publisher = {ACM Press},
title = {{Functional testing}},
year = {2006}
}

@article{Fluri2007,
author={B. Fluri and M. Wuersch and M. PInzger and H. Gall},
journal={IEEE Transactions on Software Engineering},
title={Change Distilling:Tree Differencing for Fine-Grained Source Code Change Extraction},
year=2007,
volume=33,
number=11,
pages={725-743},
keywords={software maintenance;software prototyping;tree data structures;minimum edit script;abstract syntax trees;software evolution analysis;fine-grained source code change extraction;change distilling tree differencing algorithm;Data mining;Taxonomy;Software maintenance;Programming profession;Software algorithms;Algorithm design and analysis;Software tools;Maintenance engineering;Software systems;History;Source code change extraction;tree differencing algorithms;software repositories;software evolution analysis},
doi={10.1109/TSE.2007.70731},
ISSN={0098-5589},
month={11},
}

@techreport{Kitchenham2007,
  abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
  author = {Kitchenham, Barbara Ann and Charters, Stuart},
  institution = {Keele University and Durham University Joint Report},
  keywords = {engineering evidence evidence-based literature real review software systematic},
  number = {EBSE 2007-001},
  school = {Keele University},
  title = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
  year = {2007}
}

@article{Memon2007,
abstract = {Graphical user interfaces (GUIs) are by far the most popular means used to interact with today's software. The functional correctness of a GUI is required to ensure the safety, robustness and usability of an entire software system. GUI testing techniques used in practice are resource intensive; model-based automated techniques are rarely employed. A key reason for the reluctance in the adoption of model-based solutions proposed by researchers is their limited applicability; moreover, the models are expensive to create. Over the past few years, the present author has been developing different models for various aspects ofGUI testing. This paper consolidates all of the models into one scalable event-flow model and outlines algorithms to semi-automatically reverse-engineer the model from an implementation. Earlier work on model-based test-case generation, test-oracle creation, coverage evaluation, and regression testing is recast in terms of this model by defining event-space exploration strategies (ESESs) and creating an end-to-end GUI testing process. Three such ESESs are described: for checking the event-flow model, test-case generation, and test- oracle creation. Two demonstrational scenarios show the application of the model and the three ESESs for experimentation and application in GUI testing.},
author = {Memon, Atif M.},
doi = {10.1002/stvr.364},
issn = {09600833},
journal = {Software Testing, Verification and Reliability},
keywords = {event-driven software,event-flow graph,event-flow model,graphical user interfaces,integration,model checking,test oracles,test-case generation,tree},
month = {9},
number = {3},
pages = {137--157},
pmid = {9189301},
title = {{An event-flow model of GUI-based applications for testing}},
volume = {17},
year = {2007}
}

@book{Meszaros2007,
abstract = {Automated testing is a cornerstone of agile development. An effective testing strategy will deliver new functionality more aggressively, accelerate user feedback, and improve quality. However, for many developers, creating effective automated tests is a unique and unfamiliar challenge. xUnit Test Patterns is the definitive guide to writing automated tests using xUnit, the most popular unit testing framework in use today. Agile coach and test automation expert Gerard Meszaros describes 68 proven patterns for making tests easier to write, understand, and maintain. He then shows you how to make them more robust and repeatable--and far more cost-effective. Loaded with information, this book feels like three books in one. The first part is a detailed tutorial on test automation that covers everything from test strategy to in-depth test coding. The second part, a catalog of 18 frequently encountered "test smells," provides trouble-shooting guidelines to help you determine the root cause of problems and the most applicable patterns. The third part contains detailed descriptions of each pattern, including refactoring instructions illustrated by extensive code samples in multiple programming languages. Topics covered include Writing better tests--and writing them faster The four phases of automated tests: fixture setup, exercising the system under test, result verification, and fixture teardown Improving test coverage by isolating software from its environment using Test Stubs and Mock Objects Designing software for greater testability Using test "smells" (including code smells, behavior smells, and project smells) to spot problems and know when and how to eliminate them Refactoring tests for greater simplicity, robustness, and execution speed This book will benefit developers, managers, and testers working with any agile or conventional development process, whether doing test-driven development or writing the tests last. While the patterns and smells are especially applicable to all members of the xUnit family, they also apply to next-generation behavior-driven development frameworks such as RSpec and JBehave and to other kinds of test automation tools, including recorded test tools and data-driven test tools such as Fit and FitNesse. Visual Summary of the Pattern Language Foreword Preface Acknowledgments Introduction Refactoring a Test PART I: The Narratives Chapter 1 A Brief Tour Chapter 2 Test Smells Chapter 3 Goals of Test Automation Chapter 4 Phi...},
year = {2007},
title = {xUnit Test Patterns: Refactoring Test Code},
edition = {1st edition.},
language = {eng},
author = {Meszaros, Gerard},
keywords = {Electronic books},
publisher = {Addison-Wesley},
}

@article{Reichhart2007,
abstract = {With the success of agile methodologies more and more projects develop large test suites to ensure that the system is behaving as expected. Not only do tests ensure correctness, but they also offer a live documentation for the code. However, as the system evolves, the tests need to evolve as well to keep up with the system, and as the test suite grows larger, the effort invested into maintaining tests is a significant activity. In this context, the quality of tests becomes an important issue, as developers need to assess and understand the tests they have to maintain. In this paper we present TestLint, an approach together with an experimental tool for qualifying tests. We define a set of criteria to determine test quality, and we evaluate our approach on a large sample of unit tests found in open-source projects.},
author = {Reichhart, Stefan and G{\^{i}}rba, Tudor and Ducasse, St{\'{e}}phane},
doi = {10.5381/jot.2007.6.9.a12},
issn = {1660-1769},
journal = {The Journal of Object Technology},
number = {9},
pages = {231},
title = {{Rule-based Assessment of Test Quality.}},
volume = {6},
year = {2007}
}

@article{VanRompaey2007,
abstract = {As a testing method, white box testing has been demonstrated to be very efficient in early defect detection. However, white box testing introduces test co-evolution as an additional burden to software development. To mitigate the effects of co-evolution, tests should be written in a manner that makes them easy to change. Fortunately, we are able to concretely express what a good test is by exploiting the specific principles underlying white box testing. Analogous to the concept of code smells, violations of these principles are termed test smells. In this paper, we present a formal description of test smells, and propose metrics to support their detection. We validate the feasibility of detecting two test smells, General Fixture and Eager Test, by comparison with human evaluation. We demonstrate the effectiveness of the detection in the case the assessment is agreed upon by evaluators. For the General Fixture, a qualitative investigation showed that an ambiguous test smell definition prohibits the detection by metrics and suggests disentangling its definition. On the bright side, test evolvability can be more concretely expressed than general evolvability due to the exploitation of the specific principles underlying white box testing. In particular, adherence to a rigid setup-stimulate-verify-teardown cycle has been reported an essential characteristic of evolvable tests. In this article, we propose to incorporate structural characteristics of tests in the definition of test smells, thereby providing an objective means to detect test evolution obstacles. We validate the feasibility of detecting test evolution obstacles using such test smells, thereby contributing the first step to the mitigation of the cost of test co-evolution.},
author = {{Van Rompaey}, Bart and {Du Bois}, Bart and Demeyer, Serge and Rieger, Matthias},
doi = {10.1109/TSE.2007.70745},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Maintainability,Maintenance engineering,Performance evaluation,Quality assurance,Test design,Testing},
month = {12},
number = {12},
pages = {800--817},
title = {{On The Detection of Test Smells: A Metrics-Based Approach for General Fixture and Eager Test}},
volume = {33},
year = {2007}
}

@incollection{Baker2008,
author = {Baker, Paul and Dai, Zhen Ru and Grabowski, Jens and {Haugen, Oystein Schieferdecker}, Ina and Williams, Clay},
booktitle = {Model-Driven Testing},
pages = {87--95},
publisher = {Springer},
title = {{Data-driven testing}},
year = {2008}
}


@article{Memon2008,
abstract = {Although graphical user interfaces (GUIs) constitute a large part of the software being developed today and are typically created using rapid prototyping, there are no effective regression testing techniques for GUIs. The needs of GUI regression testing differ from those of traditional software. When the structure of a GUI is modified, test cases from the original GUI's suite are either reusable or unusable on the modified GUI. Because GUI test case generation is expensive, our goal is to make the unusable test cases usable, thereby helping to retain the suite's event coverage. The idea of reusing these unusable ( obsolete ) test cases has not been explored before. This article shows that a large number of test cases become unusable for GUIs. It presents a new GUI regression testing technique that first automatically determines the usable and unusable test cases from a test suite after a GUI modification, then determines the unusable test cases that can be repaired so that they can execute on the modified GUI, and finally uses repairing transformations to repair the test cases. This regression testing technique along with four repairing transformations has been implemented. An empirical study for four open-source applications demonstrates that (1) this approach is effective in that many of the test cases can be repaired, and is practical in terms of its time performance, (2) certain types of test cases are more prone to becoming unusable, and (3) certain types of “dominator” events, when modified, make a large number of test cases unusable.},
author = {Memon, Atif M.},
doi = {10.1145/1416563.1416564},
isbn = {1049-331X},
issn = {1049-331X},
journal = {ACM Transactions on Software Engineering and Methodology},
mendeley-groups = {Software Testing/Maintenance,Software Testing/Gui Testing},
month = {11},
number = {2},
pages = {1--36},
title = {{Automatically repairing event sequence-based GUI test suites for regression testing}},
volume = {18},
year = {2008}
}



@Inbook{Moonen2008,
author="Moonen, Leon and van Deursen, Arie and Zaidman, Andy and Bruntink, Magiel",
title="On the Interplay Between Software Testing and Evolution and its Effect on Program Comprehension",
bookTitle="Software Evolution",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="173--202",
abstract="We know software evolution to be inevitable if the system is to survive in the long-term. Equally well-understood is the necessity of having a good test suite available in order to (1) ensure the quality of the current state of the software system and (2) to ease future change. In that light, this chapter explores the interplay that exists between software testing and software evolution, because as tests ease software evolution by offering a safety net against unwanted change, they can equally be experienced as a burden because they are subject to the very same forces of software evolution themselves.In particular, in this chapter, we describe how typical refactorings of production code can invalidate tests, how test code can (structurally) be improved by applying specialized test refactorings. Building upon these concepts, we introduce ``test-driven refactoring'', or refactorings of production code that are induced by the (re)structuring of the tests. We also report on typical source code design metrics that can serve as indicators for testability. To conclude, we present a research agenda that contains pointers to---as yet---unexplored research topics in the domain of testing.",
isbn="978-3-540-76440-3",
doi="10.1007/978-3-540-76440-3_8",
}

@inproceedings{Myers2008,
abstract = {Designers are skilled at sketching and prototyping the look of interfaces, but to explore various behaviors (what the interface does in response to input) typically requires programming using Javascript, ActionScript for Flash, or other languages. In our survey of 259 designers, 86\% reported that the behavior is more difficult to prototype than the appearance. Often (78\% of the time), designing the behavior requires collaborating with developers, but 76\% of designers reported that communicating the behavior to developers was more difficult than the appearance. Other results include that annotations such as arrows and paragraphs of text are used on top of sketches and storyboards to explain behaviors, and designers want to explore multiple versions of behaviors, but today's tools make this difficult. The results provide new ideas for future tools. {\textcopyright} 2008 IEEE.},
author = {Myers, Brad and Park, Sun Young and Nakano, Yoko and Mueller, Greg and Ko, Andrew},
booktitle = {2008 IEEE Symposium on Visual Languages and Human-Centric Computing},
doi = {10.1109/VLHCC.2008.4639081},
isbn = {978-1-4244-2528-0},
issn = {1943-6092},
month = {9},
pages = {177--184},
publisher = {IEEE},
title = {{How designers design and program interactive behaviors}},
year = {2008}
}

@inproceedings{VanRompaey2008,
abstract = {In order to remain effective, test suites have to co-evolve alongside the production system. As such, quantifying the amount of changes in test code should be a part of effort estimation models for maintenance activities. In this paper, we verify to which extent (i) production code size, (ii) coverage measurements; and (iii) testability metrics predict the size of test code changes between two releases. For three Java and one C++ system, the size of production code changes appears to be the best predictor. We subsequently use this predictor to construct, calibrate and validate an estimation model using the historical release data. We demonstrate that is feasible to obtain a reliable prediction model, provided that at least 5 to 10 releases are available.},
author = {{Van Rompaey}, Bart and Demeyer, Serge},
booktitle = {Proceedings of 15th Working Conference on Reverse Engineering},
doi = {10.1109/WCRE.2008.29},
isbn = {978-0-7695-3429-9},
issn = {10951350},
month = {10},
pages = {269--278},
publisher = {IEEE},
title = {{Estimation of Test Code Changes Using Historical Release Data}},
year = {2008}
}


@inproceedings{Tang2008,
abstract = {This paper presents an adaptive framework of keyword-driven automation testing to support the conversion of the keyword-based test cases into different kinds of test scripts automatically to be executed by different test applications under different test environments (such as GUI environment, database environment, etc.). XML is used to describe the keyword-based commands for the test case. An engine is provided in this framework to parse the XML file and dispatch the command sequences to the different test drivers according to the driver type pre-defined in the command. The test driver is responsible for dispatching the commands to the corresponding test applications to generate the test scripts automatically according to the keywords in the commands. The test scripts will be executed by the test applications on the system-under-test under different test environments. All the test results will be recorded into a log repository for generating all kinds of the test reports.},
author = {{Jingfan Tang} and {Xiaohua Cao} and Ma, Albert},
booktitle = {2008 IEEE International Conference on Automation and Logistics},
doi = {10.1109/ICAL.2008.4636415},
isbn = {978-1-4244-2502-0},
keywords = {Adaptive,Automation testing,Keyword driven},
month = {9},
pages = {1631--1636},
publisher = {IEEE},
title = {{Towards adaptive framework of keyword driven automation testing}},
year = {2008}
}

@article{Brooks2009,
abstract = {To date we have developed and applied numerous model-based GUI testing techniques; however, we are unable to provide definitive improvement schemes to real-world GUI test planners, as our data was derived from open source applications, small compared to industrial systems. This paper presents a study of three industrial GUI-based software systems developed at ABB, including data on classified defects detected during late-phase testing and customer usage, test suites, and source code change metrics. The results show that (1) 50\% of the defects found through the GUI are categorized as data access and handling, control flow and sequencing, correctness, and processing defects, (2) system crashes exposed defects 12-19\% of the time, and (3) GUI and non-GUI components are constructed differently, in terms of source code metrics.},
author = {Brooks, Penelope and Robinson, Brian and Memon, Atif M.},
doi = {10.1109/ICST.2009.11},
isbn = {9780769536019},
journal = {Proceedings - 2nd International Conference on Software Testing, Verification, and Validation, ICST 2009},
pages = {11--20},
publisher = {IEEE},
title = {{An initial characterization of industrial graphical user interface systems}},
year = {2009}
}


@inproceedings{Dalvi2009,
abstract = {On script-generated web sites, many documents share com- mon HTML tree structure, allowing wrappers to effectively extract information of interest. Of course, the scripts and thus the tree structure evolve over time, causing wrappers to break repeatedly, and resulting in a high cost of maintaining wrappers. In this paper, we explore a novel approach: we use temporal snapshots of web pages to develop a tree-edit model of HTML, and use this model to improve wrapper construction. We view the changes to the tree structure as suppositions of a series of edit operations: deleting nodes, inserting nodes and substituting labels of nodes. The tree structures evolve by choosing these edit operations stochas- tically. Our model is attractive in that the probability that a
source tree has evolved into a target tree can be estimated efficiently—in quadratic time in the size of the trees—making it a potentially useful tool for a variety of tree-evolution problems. We give an algorithm to learn the probabilistic model from training examples consisting of pairs of trees, and apply this algorithm to collections of web-page snap- shots to derive HTML-specific tree edit models. Finally, we describe a novel wrapper-construction framework that takes the tree-edit model into account, and compare the quality of resulting wrappers to that of traditional wrappers on syn- thetic and real HTML document examples.
Categories},
address = {New York, New York, USA},
author = {Dalvi, Nilesh and Bohannon, Philip and Sha, Fei},
booktitle = {Proceedings of the 35th SIGMOD international conference on Management of data - SIGMOD '09},
keywords = {all or part of,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,probabilistic tree-edit model,provided that copies are,this work for,wrappers,xpath},
pages = {335-347},
publisher = {ACM Press},
title = {{Robust web extraction}},
year = {2009}
}

@article{Fu2009,
abstract = {Since manual black-box testing of GUI-based APplications (GAPs) is tedious and laborious, test engineers create test scripts to automate the testing process. These test scripts interact with GAPs by performing actions on their GUI objects. Unlike conventional languages that require programmers to declare types of variables explicitly, test script statements reference GUI objects using their properties (e.g., location, color, size, etc). The absence of type information exacerbates the process of understanding test scripts, making maintenance and evolution of these scripts expensive and prohibitive, thus obliterating benefits of test automation. We offer a novel approach for Type Inference of GUI Object References (TIGOR) in test scripts. TIGOR makes types of GUI objects explicit in the source code of scripts, enabling test engineers to reason more effectively about the interactions between operations in complex test scripts and GUI objects that these operations reference. We describe our implementation and give an algorithm for automatically inferring types of GUI objects. We built a tool and evaluated it on different GAPs. Our experience suggests that TIGOR is practical and efficient, and it yields appropriate types of GUI objects.},
author = {Fu, Chen and Grechanik, Mark and Xie, Qing},
doi = {10.1109/ICST.2009.12},
isbn = {9780769536019},
journal = {Proceedings of the 2nd International Conference on Software Testing, Verification, and Validation, ICST 2009},
pages = {1--10},
publisher = {IEEE},
title = {{Inferring types of references to GUI objects in test scripts}},
year = {2009}
}


@inproceedings{Grechanik2009,
abstract = {Since manual black-box testing of GUI-based APplications (GAPs) is tedious and laborious, test engineers create test scripts to automate the testing process. These test scripts interact with GAPs by performing actions on their GUI objects. As GAPs evolve, testers should fix their corresponding test scripts so that they can reuse them to test successive releases of GAPs. Currently, there are two main modes of maintaining test scripts: tool-based and manual. In practice, there is no consensus what approach testers should use to maintain test scripts. Test managers make their decisions ad hoc, based on their personal experience and perceived benefits of the tool-based approach versus the manual. In this paper we describe a case study with forty five professional programmers and test engineers to experimentally assess the tool-based approach for maintaining GUIdirected test scripts versus the manual approach. Based on the results of our case study and considering the high cost of the programmers' time and the lower cost of the time of test engineers, and considering that programmers often modify GAP objects in the process of developing software we recommend organizations to supply programmers with testing tools that enable them to fix test scripts faster so that these scripts can unit test software. The other side of our recommendation is that experienced test engineers are likely to be as productive with the manual approach as with the tool-based approach, and we consequently recommend that organizations do not need to provide each tester with an expensive tool license to fix test scripts.},
author = {Grechanik, Mark and Xie, Qing and Fu, Chen},
booktitle = {2009 IEEE International Conference on Software Maintenance},
doi = {10.1109/ICSM.2009.5306345},
isbn = {978-1-4244-4897-5},
issn = {1063-6773},
month = {9},
pages = {9--18},
publisher = {IEEE},
title = {{Experimental assessment of manual versus tool-based maintenance of GUI-directed test scripts}},
year = {2009}
}

@inproceedings{Grechanik2009b,
abstract = {Since manual black-box testing of GUI-based APplications (GAPs) is tedious and laborious, test engineers create test scripts to automate the testing process. These test scripts interact with GAPs by performing actions on their GUI objects. As GAPs evolve, testers should fix their corresponding test scripts so that they can reuse them to test successive releases of GAPs. Currently, there are two main modes of maintaining test scripts: tool-based and manual. In practice, there is no consensus what approach testers should use to maintain test scripts. Test managers make their decisions ad hoc, based on their personal experience and perceived benefits of the tool-based approach versus the manual. In this paper we describe a case study with forty five professional programmers and test engineers to experimentally assess the tool-based approach for maintaining GUIdirected test scripts versus the manual approach. Based on the results of our case study and considering the high cost of the programmers' time and the lower cost of the time of test engineers, and considering that programmers often modify GAP objects in the process of developing software we recommend organizations to supply programmers with testing tools that enable them to fix test scripts faster so that these scripts can unit test software. The other side of our recommendation is that experienced test engineers are likely to be as productive with the manual approach as with the tool-based approach, and we consequently recommend that organizations do not need to provide each tester with an expensive tool license to fix test scripts.},
author = {Grechanik, Mark and Xie, Qing and Fu, Chen},
booktitle = {2009 IEEE International Conference on Software Maintenance},
doi = {10.1109/ICSM.2009.5306345},
isbn = {978-1-4244-4897-5},
issn = {1063-6773},
month = {9},
pages = {9--18},
publisher = {IEEE},
title = {{Experimental assessment of manual versus tool-based maintenance of GUI-directed test scripts}},
year = {2009}
}


@inproceedings{Khomh2009,
abstract = {The presence of code and design smells can have a severe impact on the quality of a program. Consequently, their detection and correction have drawn the attention of both researchers and practitioners who have proposed various approaches to detect code and design smells in programs. However, none of these approaches handle the inherent uncertainty of the detection process. We propose a Bayesian approach to manage this uncertainty. First, we present a systematic process to convert existing state-of-the-art detection rules into a probabilistic model. We illustrate this process by generating a model to detect occurrences of the Blob antipattern. Second, we present results of the validation of the model: we built this model on two open-source programs, GanttProject v1.10.2 and Xerces v2.7.0, and measured its accuracy. Third, we compare our model with another approach to show that it returns the same candidate classes while ordering them to minimise the quality analysts' effort. Finally, we show that when past detection results are available, our model can be calibrated using machine learning techniques to offer an improved, context-specific detection.},
author = {Khomh, Foutse and Vaucher, St\'{e}phane and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Sahraoui, Houari},
title = {A Bayesian Approach for the Detection of Code and Design Smells},
year = {2009},
isbn = {9780769538280},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/QSIC.2009.47},
booktitle = {Proceedings of the 2009 Ninth International Conference on Quality Software},
pages = {305–314},
numpages = {10},
keywords = {software quality, bayesian belief networks, code smells, design smells},
series = {QSIC ’09}
}

@inproceedings{Mesbah2009,
abstract = {AJAX-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing AJAX applications automatically, based on a crawler to infer a flow graph for all (client-side) user interface states. We identify AJAX-specific faults that can occur in such states (related to DOM validity, error messages, discoverability, back-button compatibility, etc.) as well as DOM-tree invariants that can serve as oracle to detect such faults. We implemented our approach in ATUSA, a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe two case studies evaluating the fault revealing capabilities, scalability, required manual effort and level of automation of our approach. {\textcopyright} 2009 IEEE.},
author = {Mesbah, Ali and van Deursen, Arie},
booktitle = {2009 IEEE 31st International Conference on Software Engineering},
doi = {10.1109/ICSE.2009.5070522},
isbn = {978-1-4244-3453-4},
issn = {02705257},
pages = {210--220},
publisher = {IEEE},
title = {{Invariant-based automatic testing of AJAX user interfaces}},
year = {2009}
}

@article{Roy2009,
abstract = {Over the last decade many techniques and tools for software clone detection have been proposed. In this paper, we provide a qualitative comparison and evaluation of the current state-of-the-art in clone detection techniques and tools, and organize the large amount of information into a coherent conceptual framework. We begin with background concepts, a generic clone detection process and an overall taxonomy of current techniques and tools. We then classify, compare and evaluate the techniques and tools in two different dimensions. First, we classify and compare approaches based on a number of facets, each of which has a set of (possibly overlapping) attributes. Second, we qualitatively evaluate the classified techniques and tools with respect to a taxonomy of editing scenarios designed to model the creation of Type-1, Type-2, Type-3 and Type-4 clones. Finally, we provide examples of how one might use the results of this study to choose the most appropriate clone detection tool or technique in the context of a particular set of goals and constraints. The primary contributions of this paper are: (1) a schema for classifying clone detection techniques and tools and a classification of current clone detectors based on this schema, and (2) a taxonomy of editing scenarios that produce different clone types and a qualitative evaluation of current clone detectors based on this taxonomy.},
author = {Roy, Chanchal K. and Cordy, James R. and Koschke, Rainer},
doi = {10.1016/j.scico.2009.02.007},
journal = {Science of Computer Programming},
keywords = {Clone detection,Comparison,Scenario-based evaluation,Software clone},
month = {5},
number = {7},
pages = {470--495},
publisher = {Elsevier B.V.},
title = {{Comparison and evaluation of code clone detection techniques and tools: A qualitative approach}},
volume = {74},
year = {2009}
}

@inproceedings{Cunha2010,
abstract = {Nowadays, the usage of graphical user interfaces (GUIs) in order to ease the interaction with software applications is preferred over command line interfaces. Despite recent advances in software testing, GUIs are still tested in a complete ad-hoc, manual fashion, with little support from (industrial) testing tools. Automating the process of testing GUIs has additional challenges when compared to command-line applications. This paper presents an approach for GUI (semi-automated) testing which uses knowledge of the common behaviour of a GUI. To do so, the most common aspects in a GUI are identified and then a suite of test cases is automatically generated and executed. To validate our approach, we have run it against well known web-based applications, such as GMail. {\textcopyright} 2010 IEEE.},
author = {Cunha, Marco and Paiva, Ana C. R. and Ferreira, Hugo Sereno and Abreu, Rui},
booktitle = {2010 2nd International Conference on Software Technology and Engineering},
doi = {10.1109/ICSTE.2010.5608882},
isbn = {978-1-4244-8667-0},
keywords = {Graphical user interfaces,Patterns,Software testing},
month = {10},
pages = {202--206},
publisher = {IEEE},
title = {{PETTool: A pattern-based GUI testing tool}},
volume = {1},
year = {2010}
}

@inproceedings{Huang2010,
abstract = {Recent advances in automated functional testing of Graphical User Interfaces (GUIs) rely on deriving graph models that approximate all possible sequences of events that may be executed on the GUI, and then use the graphs to generate test cases (event sequences) that achieve a specified coverage goal. However, because these models are only approximations of the actual event flows, the generated test cases may suffer from problems of infeasibility, i.e., some events may not be available for execution causing the test case to terminate prematurely. In this paper we develop a method to automatically repair GUI test suites, generating new test cases that are feasible. We use a genetic algorithm to evolve new test cases that increase our test suite's coverage while avoiding infeasible sequences. We experiment with this algorithm on a set of synthetic programs containing different types of constraints and for test sequences of varying lengths. Our results suggest that we can generate new test cases to cover most of the feasible coverage and that the genetic algorithm outperforms a random algorithm trying to achieve the same goal in almost all cases.},
author = {Huang, Si and Cohen, Myra B. and Memon, Atif M.},
booktitle = {2010 Third International Conference on Software Testing, Verification and Validation},
doi = {10.1109/ICST.2010.39},
isbn = {978-1-4244-6435-7},
pages = {245--254},
publisher = {IEEE},
title = {{Repairing GUI Test Suites Using a Genetic Algorithm}},
year = {2010}
}


@book{Humble2010,
  abstract = {Getting software released to users is often a painful, risky, and time-consuming process. This groundbreaking new book sets out the principles and technical practices that enable rapid, incremental delivery of high quality, valuable new functionality to users. Through automation of the build, deployment, and testing process, and improved collaboration between developers, testers, and operations, delivery teams can get changes released in a matter of hours---sometimes even minutes---no matter what the size of a project or the complexity of its code base. Jez Humble and David Farley begin by presenting the foundations of a rapid, reliable, low-risk delivery process. Next, they introduce the deployment pipeline, an automated process for managing all changes, from check-in to release. Finally, they discuss the 'ecosystem' needed to support continuous delivery, from infrastructure, data and configuration management to governance. The authors introduce state-of-the-art techniques, including automated infrastructure management and data migration, and the use of virtualization. For each, they review key issues, identify best practices, and demonstrate how to mitigate risks.},
  address = {Upper Saddle River, NJ},
  author = {Humble, Jez and Farley, David G.},
  isbn = {978-0-321-60191-9},
  keywords = {01841 105 safari book software development build process},
  publisher = {Addison-Wesley},
  title = {Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation},
  year = 2010
}

@article{Lamkanfi2010,
author = {Lamkanfi, Ahmed and Demeyer, Serge},
journal = {Proceedings of the 9th Belgian-Netherlands Software EvoLution Seminar)},
keywords = {evolution,impact analysis,software traceability,test cases},
pages = {1--4},
title = {{Studying the Co-evolution of Application Code and Test Cases}},
volume = {2010},
year = {2010}
}


@incollection{Memon2010,
abstract = {Despite the ubiquity of software applications that employ a graphical-user interface (GUI) front-end, functional system testing of these applications has remained, until recently, an understudied research area. During “GUI testing,” test cases, modeled as sequences of user input events, are created and executed on the software by exercising the GUI's widgets. Because each possible sequence of user events may potentially be a test case and today's GUIs offer enormous flexibility to end-users, in principle, GUI testing requires a prohibitively large number of test cases. Any practical test-case generation technique must sample the vast GUI input space. Existing techniques are largely manual, and hence extremely resource intensive. Several new automated model-based techniques have been developed in the past decade. All these techniques develop, either manually or automatically, a model of the GUI and employ it to generate test cases. This chapter presents the first detailed taxonomy of these techniques. A small GUI application is used as a running example to demonstrate each technique and illustrate its relative strengths and weaknesses. {\textcopyright} 2010 Elsevier Inc.},
author = {Memon, Atif M. and Nguyen, Bao N.},
booktitle = {Advances in Computers},
doi = {10.1016/S0065-2458(10)80003-8},
edition = {1},
issn = {00652458},
pages = {121--162},
publisher = {Elsevier Inc.},
title = {{Advances in Automated Model-Based System Testing of Software Applications with a GUI Front-End}},
volume = {80},
year = {2010}
}

@inproceedings{Shewchuk2010,
abstract = {Most of the modern software systems are being maintained and evolved through numerous versions. Thus, effective co-maintenance and co-evolution of their test suites along with their source code becomes an important and challenging issue. In this context, issues such as the types and extent of required maintenance activities on test suites, change in size/complexity, fault and cost effectiveness of multi-version functional test suites are among the most important issues. To study, analyze and get insights into co-maintenance and co-evolution of functional test suites with software systems w.r.t. the above issues, we have performed a case study on a functional GUI test suite of a popular open source project (jEdit). We chose as our test tool the IBM Rational Functional Tester, one of the leading commercial functional testing tools. The case study reveals interesting practical/empirical insights into the subject, e.g., developing a functional test suite in an earlier version and maintaining it might be a cost effective approach.},
address = {Redwood City, San Francisco Bay, CA, USA},
author = {Shewchuk, Yuri and Garousi, Vahid},
booktitle = {Proceedings of the 22nd International Conference on Software Engineering {\&} Knowledge Engineering (SEKE'2010)},
isbn = {1891706268},
keywords = {functional testing,evolution,test maintenance and,tool evaluation},
pages = {489--494},
title = {{Experience with Maintenance of a Functional GUI Test Suite using IBM Rational Functional Tester}},
year = {2010}
}

@inproceedings{Daniel2011,
abstract = {To improve the overall user experience, graphical user interfaces (GUIs) of successful software systems evolve continuously. While the evolution is beneficial for end users, it creates several problems for developers and testers. Developers need to manually change the GUI code. Testers need to manually inspect and repair highly fragile test scripts. This is time-consuming and error-prone. The state-of-the-art tools for automatic GUI test repair use a black-box approach: they try to infer the changes between two GUI versions and then apply these changes to the test scripts. However, inferring these changes is challenging. We propose a white-box approach where the GUI changes are automated and knowledge about them is reused to repair the test cases appropriately. We use GUI refactorings as a means to encode the evolution of the GUIs. We envision a smart IDE that will record these refactorings precisely as they happen and will use them to change the GUI code and to repair test cases. We illustrate our approach through an example, discuss challenges that should be overcome to turn our vision into reality, and present a research agenda to address these challenges.},
address = {New York, New York, USA},
author = {Daniel, Brett and Luo, Qingzhou and Mirzaaghaei, Mehdi and Dig, Danny and Marinov, Darko and Pezz{\`{e}}, Mauro},
booktitle = {2011 International Workshop on End-to-End Test Script Engineering, ETSE 2011 - Proceedings},
doi = {10.1145/2002931.2002937},
isbn = {9781450308083},
keywords = {Automated GUI testing,GUI maintenance,GUI refactoring,Graphical user interfaces},
pages = {38--41},
publisher = {ACM Press},
title = {{Automated GUI refactoring and test script repair (position paper)}},
year = {2011}
}


@inproceedings{Choudhary2011,
abstract = {Web applications tend to evolve quickly, resulting in errors and fail- ures in test automation scripts that exercise them. Repairing such scripts to work on the updated application is essential for maintain- ing the quality of the test suite. Updating such scripts manually is a time consuming task, which is often difficult and is prone to errors if not performed carefully. In this paper, we propose a tech- nique to automatically suggest repairs for such web application test scripts. Our technique is based on differential testing and compares the behavior of the test case on two successive versions of the web application: first version in which the test script runs successfully and the second version in which the script results in an error or fail- ure. By analyzing the difference between these two executions, our technique suggests repairs that can be applied to repair the scripts. To evaluate our technique, we implemented it in a tool calledWA- TER and exercised it on real web applications with test cases. Our experiments show thatWATER can suggest meaningful repairs for practical test cases, many of which correspond to those made later by developers themselves.},
address = {New York, New York, USA},
author = {Choudhary, Shauvik Roy and Zhao, Dan and Versee, Husayn and Orso, Alessandro},
booktitle = {Proceedings of the First International Workshop on End-to-End Test Script Engineering - ETSE '11},
keywords = {test repair, web testing},
pages = {24--29},
publisher = {ACM Press},
title = {{WATER}},
year = {2011}
}

@book{Goldstein2011,
title = {HTML5 \& CSS3 for the Real World},
author = {Goldstein, Alexis and Lazais, Louis and Weyl, Estelle},
year = {2011},
publisher = {Sitepoint}
}

@inproceedings{Gupta2011,
abstract = {Applications, once developed, need to be maintained and tested as they undergo frequent changes. Test automation plays a significant role in testing activity, as it saves time and provides better utilization of resources. Test automation itself comes with many challenges such as mapping of user specifications to test-cases, test-case generation, maintenance of test-cases and test-scripts. In this paper, we propose a model-driven approach for test automation to provide end-to-end assistance in test case generation and automation, with focus on re-usability and maintainability. Functional specifications of system are mapped to test-cases for traceability which ensures better test automation process. Functional specifications of system are used as an input to design process models, which are used for automatic generation of test-cases. Process models consist of flows of different tasks in specified sequence. By recording the individual tasks, test-scripts for all the test-cases are generated. The test-cases and test-scripts can be modified and maintained using user friendly user-interface (UI) to provide better control to test designer and ease the load of tester. In this paper, we also present a case study performed on JBilling application [18] to evaluate our approach.},
address = {New York, New York, USA},
author = {Gupta, Priya and Surve, Prafullakumar},
booktitle = {Proceedings of the First International Workshop on End-to-End Test Script Engineering - ETSE '11},
doi = {10.1145/2002931.2002932},
isbn = {9781450308083},
keywords = {model based approach,test case maintenance,test execution,test scripts,test-automation,test-case generation},
pages = {1--7},
publisher = {ACM Press},
title = {{Model based approach to assist test case creation, execution, and maintenance for test automation}},
year = {2011}
}

@article{Montoto2011,
abstract = {Web automation applications are widely used for different purposes such as B2B integration, automated testing of web applications or technology and business watch. One crucial part in web automation applications is for them to easily generate and reproduce navigation sequences. This problem is specially complicated in the case of the new breed of AJAX-based websites. Although recently some tools have also addressed the problem, they show some limitations either in usability or their ability to deal with complex websites. In this paper, we propose a set of new techniques to build an automatic web navigation system able to deal with these complexities. Our main contributions are: a new method for recording navigation sequences able to scale to a wider range of events, an algorithm to identify in a change-resilient manner the target element of a user action, and a novel method to detect when the effects caused by a user action (including the effects of scripting code and AJAX requests) have finished. In addition, we have also tested our approach with a high number of real web sources and have compared it with other relevant web automation tools obtaining very good results. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Montoto, Paula and Pan, Alberto and Raposo, Juan and Bellas, Fernando and L{\'{o}}pez, Javier},
issn = {0169023X},
journal = {Data {\&} Knowledge Engineering},
keywords = {Web automation,Web integration,[28.8] Technologies of DBs/mediators and wrappers,[30.3] Web web-based information systems,[8.3] Data mining/web-based information},
month = {3},
pages = {269--283},
publisher = {Elsevier B.V.},
title = {{Automated browsing in AJAX websites}},
volume = {70},
year = {2011}
}

@inproceedings{Satopaa2011,
abstract = {Computer systems often reach a point at which the relative cost to increase some tunable parameter is no longer worth the corresponding performance benefit. These "knees" typically represent beneficial points that system designers have long selected to best balance inherent trade-offs. While prior work largely uses ad hoc, system-specific approaches to detect knees, we present Kneedle, a general approach to on line and off line knee detection that is applicable to a wide range of systems. We define a knee formally for continuous functions using the mathematical concept of curvature and compare our definition against alternatives. We then evaluate Kneedle's accuracy against existing algorithms on both synthetic and real data sets, and evaluate its performance in two different applications. {\textcopyright} 2011 IEEE.},
author = {Satopaa, Ville and Albrecht, Jeannie and Irwin, David and Raghavan, Barath},
booktitle = {2011 31st International Conference on Distributed Computing Systems Workshops},
doi = {10.1109/ICDCSW.2011.20},
isbn = {978-1-4577-0384-3},
keywords = {Congestion control,Curvature,Knee detection,MapReduce,System behavior},
month = {6},
pages = {166--171},
publisher = {IEEE},
title = {{Finding a "Kneedle" in a Haystack: Detecting Knee Points in System Behavior}},
url = {http://ieeexplore.ieee.org/document/5961514/},
year = {2011}
}

@article{Zaidman2011,
abstract = {Many software production processes advocate rigorous development testing alongside functional code writing, which implies that both test code and production code should co-evolve. To gain insight in the nature of this co-evolution, this paper proposes three views (realized by a tool called TeMo) that combine information from a software project's versioning system, the size of the various artifacts and the test coverage reports. We validate these views against two open source and one industrial software project and evaluate our results both with the help of log messages, code inspections and the original developers of the software system. With these views we could recognize different co-evolution scenarios (i.e., synchronous and phased) and make relevant observations for both developers as well as test engineers.},
author = {Zaidman, Andy and {Van Rompaey}, Bart and van Deursen, Arie and Demeyer, Serge},
doi = {10.1007/s10664-010-9143-7},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Co-evolution,Software evolution,Software repository mining,Software testing,Test coverage},
month = {6},
number = {3},
pages = {325--364},
title = {{Studying the co-evolution of production and test code in open source and industrial developer test processes through repository mining}},
volume = {16},
year = {2011}
}

@article{Chen2012,
abstract = {Testing the GUI (Graphical User Interface) of a software application is typically accomplished by developing a GUI test script composed of sequences of events and assertions. A GUI test script is in a sense similar to the source code of a program, since events and assertions are like source-code statements, which are executed line by line. Therefore, like source code, a GUI test script may have bad smells, and refactoring is an effective technique that can eradicate bad smells, making the script better and easier to maintain. This paper studies the bad smells a GUI test script may have and the refactoring methods that can be applied to remove the bad smells. A total of 11 bad smells are identified and 16 refactoring methods are proposed. The refactoring methods have been implemented in a GUI testing tool, called GTT, to support the automatic refactoring of GUI test scripts.},
author = {Chen, Woei Kae and Wang, Jung Chi},
doi = {10.1109/SNPD.2012.10},
isbn = {9780769547619},
journal = {Proceedings - 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, SNPD 2012},
keywords = {GUI testing,refactoring,smell,test script},
pages = {289--294},
title = {{Bad smells and refactoring methods for GUI test scripts}},
year = {2012}
}

@inproceedings{Gross2012,
abstract = {Modern test case generation techniques can automatically achieve high code coverage. If they operate on the unit level, they run the risk of generating inputs infeasible in reality, which, when causing failures, are painful to identify and eliminate. Running a unit test generator on five open source Java programs, we found that all of the 181 reported failures were false failuresthat is, indicating a problem in the generated test case rather than the program. By generating test cases at the GUI level, our EXSYST prototype can avoid such false alarms by construction. In our evaluation, it achieves higher coverage than search-based test generators at the unit level; yet, every failure can be shown to be caused by a real sequence of input events. Whenever a system interface is available, we recommend considering search-based system testing as an alternative to avoid false failures.},
address = {New York, New York, USA},
author = {Gross, Florian and Fraser, Gordon and Zeller, Andreas},
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis - ISSTA 2012},
doi = {10.1145/2338965.2336762},
isbn = {9781450314541},
keywords = {Test case generation; system testing; GUI testing; test coverage},
pages = {67},
publisher = {ACM Press},
title = {{Search-based system testing: high coverage, no false alarms}},
year = {2012}
}


@inproceedings{Hametner2012,
abstract = {In the field of industrial automation systems software becomes an important factor because engineers tend to move the realization of functional requirements from hardware to software components. The main reason for this is that software components allow increasing product flexibility. As a consequence software complexity increases rapidly and requires systematic, automation-supported and agile testing approaches. Thus, systematic and agile testing are key challenges in industrial control software development to ensure and improve systems quality. Further different implementation standards, i.e., IEC 61131-3 and IEC 61499, arise additional challenges in constructing and testing industrial automation systems software. This paper presents an agile and keyword-driven test approach with focus on testing implementations based on both important industrial standards and illustrates the applicability of the purposed approach in a sample implementation, i.e., a High Speed Pick and Place unit. Main results show the applicability of keyword-driven testing based on a defined subset of keywords (common for IEC 61131-3 and IEC 61499) and thus enable agile and automation-supported testing more effective and efficient. {\textcopyright} 2012 IEEE.},
author = {Hametner, Reinhard and Winkler, Dietmar and Zoitl, Alois},
booktitle = {IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society},
doi = {10.1109/IECON.2012.6389298},
isbn = {978-1-4673-2421-2},
keywords = {IEC 61131-3,IEC 61499,Industrial Control Applications,Keyword-driven Test,Test Automation},
month = {10},
pages = {3727--3732},
publisher = {IEEE},
title = {{Agile testing concepts based on keyword-driven testing for industrial automation systems}},
year = {2012}
}

@inproceedings{Hurdugaci2012,
abstract = {Unit and integration tests can be invaluable during software maintenance as they help to understand pieces of code, they help with quality assurance and they build up confidence amongst developers. Unfortunately then, previous research has shown that unit tests do not always co-evolve nicely with the production code, thus leaving the software vulnerable. This paper presents TestNForce, a tool that helps developers to identify the unit tests that need to be altered and executed after a code change, thereby reducing the effort needed to keep the unit tests in sync with the changes to the production code. In order to evaluate TestNForce, we perform a user study that evaluates the adequacy, usefulness and completeness of TestNForce.},
author = {Hurdugaci, Victor and Zaidman, Andy},
booktitle = {Proceedings of 16th European Conference on Software Maintenance and Reengineering},
doi = {10.1109/CSMR.2012.12},
isbn = {978-0-7695-4666-7},
issn = {15345351},
month = {3},
pages = {11--20},
publisher = {IEEE},
title = {{Aiding Software Developers to Maintain Developer Tests}},
year = {2012}
}


@inproceedings{Issa2012,
abstract = {Graphical User Interface (GUI) testing literature emphasizes testing a system's functionality through its GUI, rather than testing visual aspects of the GUI itself. In this paper we introduce the notion of visual testing as a subset of GUI testing. To explore visual testing, we have conducted a study of defects in four open source systems. We found that visual defects represent between 16\% and 33\% of reported defects in those systems. Two categories of visual defects are identified with six subcategories within each of them. Other findings are also reported that are aimed at motivating the importance and the need for systematically conducting visual testing among researchers and practitioners. {\textcopyright} 2012 IEEE.},
author = {Issa, Ayman and Sillito, Jonathan and Garousi, Vahid},
booktitle = {Proceedings of IEEE International Symposium on Web Systems Evolution, WSE},
doi = {10.1109/WSE.2012.6320526},
isbn = {9781467330558},
issn = {21606153},
keywords = {graphical user interface,visual defect,visual testing},
pages = {11--15},
publisher = {IEEE},
title = {{Visual testing of Graphical User Interfaces: An exploratory study towards systematic definitions and approaches}},
year = {2012}
}

@article{Mesbah2012,
abstract = {Ajax-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing Ajax applications automatically, based on a crawler to infer a state-flow graph for all (client-side) user interface states. We identify Ajax-specific faults that can occur in such states (related to, e.g., DOM validity, error messages, discoverability, back-button compatibility) as well as DOM-tree invariants that can serve as oracles to detect such faults. Our approach, called Atusa, is implemented in a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe three case studies, consisting of six subjects, evaluating the type of invariants that can be obtained for Ajax applications as well as the fault revealing capabilities, scalability, required manual effort, and level of automation of our testing approach.},
author = {Mesbah, Ali and van Deursen, A. and Roest, Danny},
doi = {10.1109/TSE.2011.28},
isbn = {9781424434527},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Ajax,Automated testing,web applications},
month = {1},
number = {1},
pages = {35--53},
title = {{Invariant-Based Automatic Testing of Modern Web Applications}},
volume = {38},
year = {2012}
}


@article{Pinto2012,
abstract = {Test suites, once created, rarely remain static. Just like the application they are testing, they evolve throughout their lifetime. Test obsolescence is probably the most known reason for test-suite evolution---test cases cease to work because of changes in the code and must be suitably repaired. Repairing existing test cases manually, however, can be extremely time consuming, especially for large test suites, which has motivated the recent development of automated test-repair techniques. We believe that, for developing effective repair techniques that are applicable in real-world scenarios, a fundamental prerequisite is a thorough understanding of how test cases evolve in practice. Without such knowledge, we risk to develop techniques that may work well for only a small number of tests or, worse, that may not work at all in most realistic cases. Unfortunately, to date there are no studies in the literature that investigate how test suites evolve. To tackle this problem, in this paper we present a technique for studying test-suite evolution, a tool that implements the technique, and an extensive empirical study in which we used our technique to study many versions of six real-world programs and their unit test suites. This is the first study of this kind, and our results reveal several interesting aspects of test-suite evolution. In particular, our findings show that test repair is just one possible reason for test-suite evolution, whereas most changes involve refactorings, deletions, and additions of test cases. Our results also show that test modifications tend to involve complex, and hard-to-automate, changes to test cases, and that existing test-repair techniques that focus exclusively on assertions may have limited practical applicability. More generally, our findings provide initial insight on how test cases are added, removed, and modified in practice, and can guide future research efforts in the area of test-suite evolution.},
author = {Pinto, Leandro Sales and Sinha, Saurabh and Orso, Alessandro},
doi = {10.1145/2393596.2393634},
isbn = {978-1-4503-1614-9},
journal = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
keywords = {test-suite evolution,test-suite maintenance,unit testing},
pages = {33:1--33:11},
title = {{Understanding Myths and Realities of Test-suite Evolution}},
volume = {1},
year = {2012}
}

@inproceedings{Thummalapenta2012,
abstract = {Mention “test case”, and it conjures up the image of a script or a program that exercises a system under test. In industrial practice, however, test cases often start out as steps described in natural language. These are essentially directions a human tester needs to follow to interact with an application, exercising a given scenario. Since tests need to be executed repeatedly, such manual tests then have to go through test automation to create scripts or programs out of them. Test automation can be expensive in programmer time. We describe a technique to automate test automation. The input to our technique is a sequence of steps written in natural language, and the output is a sequence of procedure calls with accompanying parameters that can drive the application without human intervention. The technique is based on looking at the natural language test steps as consisting of segments that describe actions on targets, except that there can be ambiguity in identifying segments, in identifying the action in a segment, as well as in the specification of the target of the action. The technique resolves this ambiguity by backtracking, until it can synthesize a successful sequence of calls. We present an evaluation of our technique on professionally created manual test cases for two open-source web applications as well as a proprietary enterprise application. Our technique could automate over 82\% of the steps contained in these test cases with no human intervention, indicating that the technique can reduce the cost of test automation quite effectively.},
author = {Thummalapenta, Suresh and Sinha, Saurabh and Singhania, Nimit and Chandra, Satish},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
doi = {10.1109/ICSE.2012.6227131},
isbn = {978-1-4673-1066-6},
month = {6},
pages = {881--891},
publisher = {IEEE},
title = {{Automating test automation}},
year = {2012}
}


@article{Utting2012,
abstract = {Model-based testing (MBT) relies on models of a system under test and/or its environment to derive test cases for the system. This paper discusses the process of MBT and defines a taxonomy that covers the key aspects of MBT approaches. It is intended to help with understanding the characteristics, similarities and differences of those approaches, and with classifying the approach used in a particular MBT tool. To illustrate the taxonomy, a description of how three different examples of MBT tools fit into the taxonomy is provided.},
author = {Utting, Mark and Pretschner, Alexander and Legeard, Bruno},
doi = {10.1002/stvr.456},
issn = {09600833},
journal = {Software Testing, Verification and Reliability},
keywords = {clustering,inheritance metrics,machine learning,object oriented paradigm,prediction,software fault,unsupervised learning},
month = {8},
number = {5},
pages = {297--312},
title = {{A taxonomy of model-based testing approaches}},
volume = {22},
year = {2012}
}

@inproceedings{Vliegendhart2012,
abstract = {Conducting a conventional experiment to test an application's user interface in a lab environment is a costly and time-consuming process. In this paper, we show that it is feasible to carry out A/B tests for a multimedia application through Amazon's crowdsourcing platform Mechanical Turk involving hundreds of workers at low costs. We let workers test user interfaces within a remote virtual machine that is embedded within the HIT and we show that technical issues that arise in this approach can be overcome.},
address = {New York, New York, USA},
author = {Vliegendhart, Raynor and Dolstra, Eelco and Pouwelse, Johan},
booktitle = {Proceedings of the ACM multimedia 2012 workshop on Crowdsourcing for multimedia - CrowdMM '12},
doi = {10.1145/2390803.2390813},
isbn = {9781450315890},
keywords = {A/B testing,Crowdsourcing,Usability study},
pages = {21},
publisher = {ACM Press},
title = {{Crowdsourced user interface testing for multimedia applications}},
year = {2012}
}


@inproceedings{Alegroth2013,
author = {Alegroth, Emil and Feldt, Robert and Olsson, Helena H},
booktitle = {2013 IEEE Sixth International Conference on Software Testing, Verification and Validation},
doi = {10.1109/ICST.2013.14},
isbn = {978-0-7695-4968-2},
keywords = {-visual gui testing,company saab ab,empirical,entirely by industrial practitioners,industrial case study,into vgt at the,nance,subdivision security and,test automation,test mainte-,transition,with the goal to},
month = {3},
pages = {56--65},
publisher = {IEEE},
title = {{Transitioning Manual System Test Suites to Automated Testing: An Industrial Case Study}},
year = {2013}
}

@article{Banerjee2013,
abstract = {Context GUI testing is system testing of a software that has a graphical-user interface (GUI) front-end. Because system testing entails that the entire software system, including the user interface, be tested as a whole, during GUI testing, test cases - modeled as sequences of user input events - are developed and executed on the software by exercising the GUI's widgets (e.g., text boxes and clickable buttons). More than 230 articles have appeared in the area of GUI testing since 1991. Objective In this paper, we study this existing body of knowledge using a systematic mapping (SM). Method The SM is conducted using the guidelines proposed by Petersen et al. We pose three sets of research questions. We define selection and exclusion criteria. From the initial pool of 230 articles, published in years 1991-2011, our final pool consisted of 136 articles. We systematically develop a classification scheme and map the selected articles to this scheme. Results We present two types of results. First, we report the demographics and bibliometrics trends in this domain, including: top-cited articles, active researchers, top venues, and active countries in this research area. Moreover, we derive the trends, for instance, in terms of types of articles, sources of information to derive test cases, types of evaluations used in articles, etc. Our second major result is a publicly-accessible repository that contains all our mapping data. We plan to update this repository on a regular basis, making it a "live" resource for all researchers. Conclusion Our SM provides an overview of existing GUI testing approaches and helps spot areas in the field that require more attention from the research community. For example, much work is needed to connect academic model-based techniques with commercially available tools. To this end, studies are needed to compare the state-of-the-art in GUI testing in academic techniques and industrial tools. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Banerjee, Ishan and Nguyen, Bao and Garousi, Vahid and Memon, Atif},
doi = {10.1016/j.infsof.2013.03.004},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Bibliometrics,GUI application,Paper repository,Systematic mapping,Testing},
month = {10},
number = {10},
pages = {1679--1694},
publisher = {Elsevier B.V.},
title = {{Graphical user interface (GUI) testing: Systematic mapping and repository}},
volume = {55},
year = {2013}
}

@inproceedings{Hauptmann2013,
abstract = {Tests are central artifacts of software systems and play a crucial role for software quality. In system testing, a lot of test execution is performed manually using tests in natural language. However, those test cases are often poorly written without best practices in mind. This leads to tests which are not maintainable, hard to understand and inefficient to execute. For source code and unit tests, so called code smells and test smells have been established as indicators to identify poorly written code. We apply the idea of smells to natural language tests by defining a set of common Natural Language Test Smells (NLTS). Furthermore, we report on an empirical study analyzing the extent in more than 2800 tests of seven industrial test suites.},
address = {San Francisco, CA, USA},
author = {Hauptmann, Benedikt and Heinemann, Lars and Vaas, Rudolf and Braun, Peter},
booktitle = {35th International Conference on Software Engineering (ICSE)},
doi = {10.1109/ICSE.2013.6606682},
isbn = {978-1-4673-3076-3},
issn = {02705257},
keywords = {natural language,system testing,test smells},
month = {5},
pages = {1217--1220},
publisher = {IEEE},
title = {{Hunting for smells in natural language tests}},
year = {2013}
}

@article{Kan2013,
abstract = {Through reusing software test components, automated software testing generally costs less than manual software testing. There has been much research on how to develop the reusable test components, but few fall on how to estimate the reusability of test components for automated testing. The purpose of this paper is to present a method of minimum reusability estimation for automated testing based on the return on investment (ROI) model. Minimum reusability is a benchmark for the whole automated testing process. If the reusability in one test execution is less than the minimum reusability, some new strategies must be adopted in the next test execution to increase the reusability. Only by this way, we can reduce unnecessary costs and finally get a return on the investment of automated testing. {\textcopyright} 2013 Shanghai Jiaotong University and Springer-Verlag Berlin Heidelberg.},
author = {Kan, Hong Xing and Wang, Guo Qiang and Wang, Zong Dian and Ding, Shuai},
doi = {10.1007/s12204-013-1406-1},
issn = {10071172},
journal = {Journal of Shanghai Jiaotong University (Science)},
keywords = {automated software testing,manual software testing,mean maintenance costs multiplier,minimum reusability estimation,reusable software test components,threshold},
number = {3},
pages = {360--365},
title = {{A method of minimum reusability estimation for automated software testing}},
volume = {18},
year = {2013}
}

@inproceedings{Leotta2013,
abstract = {There are several approaches for automated func- tional web testing and the choice among them depends on a number of factors, including the tools used for web testing and the costs associated with their adoption. In this paper, we present an empirical cost/benefit analysis of two different categories of automated functional web testing approaches: (1) capture- replay web testing (in particular, using Selenium IDE); and, (2) programmable web testing (using Selenium WebDriver). On a set of six web applications, we evaluated the costs of applying these testing approaches both when developing the initial test suites from scratch and when the test suites are maintained, upon the release of a new software version. Results indicate that, on the one hand, the development of the test suites is more expensive in terms of time required (between 32{\%} and 112{\%}) when the programmable web testing approach is adopted, but on the other hand, test suite maintenance is less expensive when this approach is used (with a saving between 16{\%} and 51{\%}). We found that, in the majority of the cases, after a small number of releases (from one to three), the cumulative cost of programmable web testing becomes lower than the cost involved with capture-replay web testing and the cost saving gets amplified over the successive releases. Index Terms—Test Case Evolution, Test Case Repair, Empiri- cal Study, Selenium IDE, Selenium WebDriver.},
author = {Leotta, Maurizio and Clerissi, Diego and Ricca, Filippo and Tonella, Paolo},
booktitle = {2013 20th Working Conference on Reverse Engineering (WCRE)},
keywords = {Empirirical Study, Selenium IDE, Selenium WebDriver, Test Case Evolution, Test Case Repair},
month = {10},
pages = {272--281},
publisher = {IEEE},
title = {{Capture-replay vs. programmable web testing: An empirical assessment during test case evolution}},
year = {2013}
}

@inproceedings{Gomez2013,
abstract = {Touchscreen-based devices such as smartphones and tablets are gaining popularity, but their rich input capabilities pose new development and testing complications. To alleviate this problem, we present an approach and tool named Reran that permits record-and-replay for the Android smartphone platform. Existing GUI-level record-and-replay approaches are inadequate due to the expressiveness of the smartphone domain, in which applications support sophisticated GUI gestures, depend on inputs from a variety of sensors on the device, and have precise timing requirements among the various input events. We address these challenges by directly capturing the low-level event stream on the phone, which includes both GUI events and sensor events, and replaying it with microsecond accuracy. Moreover, Reran does not require access to app source code, perform any app rewriting, or perform any modifications to the virtual machine or Android platform. We demonstrate RERAN's applicability in a variety of scenarios, including (a) replaying 86 out of the Top-100 Android apps on Google Play; (b) reproducing bugs in popular apps, e.g., Firefox, Facebook, Quickoffice; and (c) fast-forwarding executions. We believe that our versatile approach can help both Android developers and researchers. {\textcopyright} 2013 IEEE.},
author = {Gomez, Lorenzo and Neamtiu, Iulian and Azim, Tanzirul and Millstein, Todd},
booktitle = {2013 35th International Conference on Software Engineering (ICSE)},
doi = {10.1109/ICSE.2013.6606553},
isbn = {978-1-4673-3076-3},
issn = {02705257},
keywords = {Google Android,Record-and-replay},
month = {5},
pages = {72--81},
publisher = {IEEE},
title = {{RERAN: Timing- and touch-sensitive record and replay for Android}},
year = {2013}
}

@InProceedings{Hao2013,
abstract="In software evolution, developers typically need to identify whether the failure of a test is due to a bug in the source code under test or the obsoleteness of the test code when they execute a test suite. Only after finding the cause of a failure can developers determine whether to fix the bug or repair the obsolete test. Researchers have proposed several techniques to automate test repair. However, test-repair techniques typically assume that test failures are always due to obsolete tests. Thus, such techniques may not be applicable in real world software evolution when developers do not know whether the failure is due to a bug or an obsolete test. To know whether the cause of a test failure lies in the source code under test or in the test code, we view this problem as a classification problem and propose an automatic approach based on machine learning. Specifically, we target Java software using the JUnit testing framework and collect a set of features that may be related to failures of tests. Using this set of features, we adopt the Best-first Decision Tree Learning algorithm to train a classifier with some existing regression test failures as training instances. Then, we use the classifier to classify future failed tests. Furthermore, we evaluated our approach using two Java programs in three scenarios (within the same version, within different versions of a program, and between different programs), and found that our approach can effectively classify the causes of failed tests.",

author={Hao, Dan and Lan, Tian and Zhang, Hongyu and Guo, Chao and Zhang, Lu},
title={Is This a Bug or an Obsolete Test?},
booktitle={Proceedings of European Conference on Object-Oriented Programming},
year={2013},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={602--628},
isbn={978-3-642-39038-8},
}


@inproceedings{Machiry2013,
abstract = {We present a system Dynodroid for generating relevant inputs to unmodified Android apps. Dynodroid views an app as an event-driven program that interacts with its environment by means of a sequence of events through the Android framework. By instrumenting the framework once and for all, Dynodroid monitors the reaction of an app upon each event in a lightweight manner, using it to guide the generation of the next event to the app. Dynodroid also allows interleaving events from machines, which are better at generating a large number of simple inputs, with events from humans, who are better at providing intelligent inputs. We evaluated Dynodroid on 50 open-source Android apps, and compared it with two prevalent approaches: users manually exercising apps, and Monkey, a popular fuzzing tool. Dynodroid, humans, and Monkey covered 55\%, 60\%, and 53\%, respectively, of each app's Java source code on average. Monkey took 20X more events on average than Dynodroid. Dynodroid also found 9 bugs in 7 of the 50 apps, and 6 bugs in 5 of the top 1, 000 free apps on Google Play. Copyright 2013 ACM.},
address = {New York, New York, USA},
author = {Machiry, Aravind and Tahiliani, Rohan and Naik, Mayur},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering - ESEC/FSE 2013},
doi = {10.1145/2491411.2491450},
isbn = {9781450322379},
keywords = {Android,GUI testing,Testing event-driven programs},
pages = {224},
publisher = {ACM Press},
title = {{Dynodroid: an input generation system for Android apps}},
year = {2013}
}

@article{Rattan2013,
title = "Software clone detection: A systematic review ",
journal = "Inf. Softw. Tech. ",
volume = 55,
number = 7,
pages = "1165 - 1199",
year = 2013,
issn = "0950-5849",
doi = "10.1016/j.infsof.2013.01.008",
author = "Dhavleesh Rattan and Rajesh Bhatia and Maninder Singh",
keywords = {"Software clone",  "Clone detection", "Systematic literature review", "Semantic clones", "Model based clone "}
}

@inproceedings{Thummalapenta2013,
abstract = {Test automation, which involves the conversion of manual test cases to executable test scripts, is necessary to carry out efficient regression testing of GUI-based applications. However, test automation takes significant investment of time and skilled effort. Moreover, it is not a one-time investment: as the application or its environment evolves, test scripts demand continuous patching. Thus, it is challenging to perform test automation in a cost-effective manner. At IBM, we developed a tool, called ATA [1], [2], to meet this challenge. ATA has novel features that are designed to lower the cost of initia test automation significantly. Moreover, ATA has the ability to patch scripts automatically for certain types of application or environment changes. How well does ATA meet its objectives in the real world? In this paper, we present a detailed case study in the context of a challenging production environment: an enterprise web application that has over 6500 manual test cases, comes in two variants, evolves frequently, and needs to be tested on multiple browsers in time-constrained and resource-constrained regression cycles. We measured how well ATA improved the efficiency in initial automation. We also evaluated the effectiveness of ATA's change-resilience along multiple dimensions: application versions, browsers, and browser versions. Our study highlights several lessons for test-automation practitioners as well as open research problems in test automation.},
address = {San Francisco, CA, USA},
author = {Thummalapenta, Suresh and Devaki, Pranavadatta and Sinha, Saurabh and Chandra, Satish and Gnanasundaram, Sivagami and Nagaraj, Deepa D. and Kumar, Sampath and Kumar, Sathish},
booktitle = {2013 35th International Conference on Software Engineering (ICSE)},
month = {5},
pages = {1002--1011},
publisher = {IEEE},
title = {{Efficient and change-resilient test automation: An industrial case study}},
year = {2013}
}

@inproceedings{Tsantalis2013,
abstract = {In this paper we present an empirical study on the refactoring activity in three well-known projects. We have studied five research questions that explore the different types of refactorings applied to different types of sources, the individual contribution of team members on refactoring activities, the alignment of refactoring activity with release dates and testing periods, and the motivation behind the applied refactorings. The studied projects have a history of 12, 7, and 6 years, respectively. We have found that there is very little variation in the types of refactorings applied on test code, since the majority of the refactorings focus on the reorganization and renaming of classes. Additionally, we have identified that the refactoring decision making and application is often performed by individual refactoring "managers". We have found a strong alignment between refactoring activity and release dates. Moreover, we found that the development teams apply a considerable amount of refactorings during testing periods. Finally, we have also found that in addition to code smell resolution the main drivers for applying refactorings are the introduction of extension points, and the resolution of backward compatibility issues.},
address = {Ontario, Canada},
author = {Tsantalis, Nikolaos and Guana, Victor and Stroulia, Eleni and Hindle, Abram},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {132--146},
publisher = {IBM Corp.},
title = {{A Multidimensional Empirical Study on Refactoring Activity}},
year = {2013}
}


@inproceedings{Zhang2013,
abstract = {A workflow is a sequence of UI actions to complete a specific task. In the course of a GUI application's evolution, changes ranging from a simple GUI refactoring to a complete rearchitecture can break an end-user's well-established workflow. It can be challenging to find a replacement workflow. To address this problem, we present a technique (and its tool implementation, called FlowFixer) that repairs a broken workflow. FlowFixer uses dynamic profiling, static analysis, and random testing to suggest a replacement UI action that fixes a broken workflow. We evaluated FlowFixer on 16 broken workflows from 5 realworld GUI applications written in Java. In 13 workflows, the correct replacement action was FlowFixer's first suggestion. In 2 workflows, the correct replacement action was FlowFixer's second suggestion. The remaining workflow was un-repairable. Overall, FlowFixer produced significantly better results than two alternative approaches.},
address = {New York, NY, USA},
author = {Zhang, Sai and L{\"{u}}, Hao and Ernst, Michael D.},
booktitle = {Proceedings of the International Symposium on Software Testing and Analysis},
doi = {10.1145/2483760.2483775},
isbn = {9781450321594},
keywords = {Dynamic analysis,GUI applications,workflows},
month = {7},
pages = {45--55},
publisher = {ACM},
title = {{Automatically repairing broken workflows for evolving GUI applications}},
year = {2013}
}



@book{Bosch2014,
abstract = {This book provides essential insights on the adoption of modern software engineering practices at large companies producing software-intensive systems, where hundreds or even thousands of engineers collaborate to deliver on new systems and new versions of already deployed ones. It is based on the findings collected and lessons learned at the Software Center (SC), a unique collaboration between research and industry, with Chalmers University of Technology, Gothenburg University and Malm{\"{o}} University as academic partners and Ericsson, AB Volvo, Volvo Car Corporation, Saab Electronic Defense Systems, Grundfos, Axis Communications, Jeppesen (Boeing) and Sony Mobile as industrial partners. The 17 chapters present the "Stairway to Heaven" model, which represents the typical evolution path companies move through as they develop and mature their software engineering capabilities. The chapters describe theoretical frameworks, conceptual models and, most importantly, the industrial experiences gained by the partner companies in applying novel software engineering techniques. The book's structure consists of six parts. Part I describes the model in detail and presents an overview of lessons learned in the collaboration between industry and academia. Part II deals with the first step of the Stairway to Heaven, in which R{\&}D adopts agile work practices. Part III of the book combines the next two phases, i.e., continuous integration (CI) and continuous delivery (CD), as they are closely intertwined. Part IV is concerned with the highest level, referred to as "R{\&}D as an innovation system," while Part V addresses a topic that is separate from the Stairway to Heaven and yet critically important in large organizations: organizational performance metrics that capture data, and visualizations of the status of software assets, defects and teams. Lastly, Part VI presents the perspectives of two of the SC partner companies. The book is intended for practitioners and professionals in the software-intensive systems industry, providing concrete models, frameworks and case studies that show the specific challenges that the partner companies encountered, their approaches to overcoming them, and the results. Researchers will gain valuable insights on the problems faced by large software companies, and on how to effectively tackle them in the context of successful cooperation projects.},
author = {Bosch, Jan},
booktitle = {Continuous software engineering},
pages = {1--226},
publisher = {Springer International Publishing},
title = {{Continuous Software Engineering}},
year = {2014}
}

@inproceedings{Christophe2014,
abstract = {Functional testing requires executing particular sequences of user actions. Test automation tools enable scripting user actions such that they can be repeated more easily. SELENIUM, for instance, enables testing web applications through scripts that interact with a web browser and assert properties about its observable state. However, little is known about how common such tests are in practice. We therefore present a cross-sectional quantitative study of the prevalence of SELENIUM-based tests among open-source web applications, and of the extent to which such tests are used within individual applications. Automating functional tests also brings about the problem of maintaining test scripts. As the system under test evolves, its test scripts are bound to break. Even less is known about the way test scripts change over time. We therefore also present a longitudinal quantitative study of whether and for how long test scripts are maintained, as well as a longitudinal qualitative study of the kind of changes they undergo. To the former's end, we propose two new metrics based on whether a commit to the application's version repository touches a test file. To the latter's end, we propose to categorize the changes within each commit based on the elements of the test upon which they operate. As such, we are able to identify the elements of a test that are most prone to change.},
author = {Christophe, Laurent and Stevens, Reinout and {De Roover}, Coen and {De Meuter}, Wolfgang},
booktitle = {2014 IEEE International Conference on Software Maintenance and Evolution},
keywords = {Functional testing,Selenium,Test automation},
month = {9},
pages = {141--150},
publisher = {IEEE},
title = {{Prevalence and Maintenance of Automated Functional Tests for Web Applications}},
year = {2014}
}


@inproceedings{Falleri2014,
address = {New York, New York, USA},
author = {Falleri, Jean-R{\'{e}}my and Morandat, Flor{\'{e}}al and Blanc, Xavier
and Martinez, Matias and Monperrus, Martin},
booktitle = {Proceedings of the 29th ACM/IEEE international conference on
Automated software engineering - ASE '14},
doi = {10.1145/2642937.2642982},
isbn = {9781450330138},
keywords = {ast,program comprehension,software evolution,tree differencing},
pages = {313--324},
publisher = {ACM Press},
title = {{Fine-grained and accurate source code differencing}},
year = {2014}
}

@inproceedings{Femmer2014,
abstract = {Bad requirements quality can have expensive consequences during the software development lifecycle. Especially, if it- erations are long and feedback comes late - The faster a problem is found, the cheaper it is to fix. We propose to detect issues in requirements based on re- quirements (bad) smells by applying a light-weight static requirements analysis. This light-weight technique allows for instant checks as soon as a requirement is written down. In this paper, we derive a set of smells, including automatic smell detection, from the natural language criteria of the ISO/IEC/IEEE 29148 standard. We evaluated the approach with 336 requirements and 53 use cases from 9 specifications that were written by the car manufacturer Daimler AG and the chemical business companyWacker Chemie AG, and discussed the results with their requirements and domain experts. While not all problems can be detected, the case study shows that lightweight smell analysis can uncover many practically relevant requirements defects. Based on these results and the discussion with our industry partners, we conclude that requirements smells can serve as an effcient supplement to traditional reviews or team discussions, in order to create fast feedback on requirements quality.},
address = {New York, New York, USA},
author = {Femmer, Henning and Fern{\'{a}}ndez, Daniel M{\'{e}}ndez and Juergens, Elmar and Klose, Michael and Zimmer, Ilona and Zimmer, J{\"{o}}rg},
booktitle = {Proceedings of the 1st International Workshop on Rapid Continuous Software Engineering - RCoSE 2014},
doi = {10.1145/2593812.2593817},
isbn = {9781450328562},
keywords = {Analytical quality assurance,Requirements engineering,Requirements smells},
pages = {10--19},
publisher = {ACM Press},
title = {{Rapid requirements checks with requirements smells: two case studies}},
year = {2014}
}

@inproceedings{Leotta2014,
abstract = {In the context of web regression testing, the main aging factor for a test suite is related to the continuous evolution of the underlying web application that makes the test cases broken. This rapid decay forces the quality experts to evolve the test ware. One of the major costs of test case evolution is due to the manual effort necessary to repair broken web page element locators. Locators are lines of source code identifying the web elements the test cases interact with. Web test cases rely heavily on locators, for instance to identify and fill the input portions of a web page (e.g., The form fields), to execute some computations (e.g., By locating and clicking on buttons) and to verify the correctness of the output (by locating the web page elements showing the results). In this paper we present ROBULA (ROBUst Locator Algorithm), a novel algorithm able to partially prevent and thus reduce the aging of web test cases by automatically generating robust XPath-based locators that are likely to work also when new releases of the web application are created. Preliminary results show that XPath locators produced by ROBULA are substantially more robust than absolute and relative locators, generated by state of the practice tools such as Fire Path. Fragility of the test suites is reduced on average by 56{\%} for absolute locators and 41{\%} for relative locators.},
author = {Leotta, Maurizio and Stocco, Andrea and Ricca, Filippo and Tonella, Paolo},
booktitle = {2014 IEEE International Symposium on Software Reliability Engineering Workshops},
keywords = {Robust locators,Test cases aging,Web testing},
month = {11},
pages = {449--454},
publisher = {IEEE},
title = {{Reducing Web Test Cases Aging by Means of Robust XPath Locators}},
year = {2014}
}

@incollection{Leotta2014b,
abstract = {Automation in Web testing has been successfully supported by DOM based tools that allow testers to program the interactions of their test cases with the Web application under test. More recently a new generation of visual tools has been proposed where a test case interacts with the Web application by recognising the images of the widgets that can be actioned upon and by asserting the expected visual appearance of the result. In this paper, we first discuss the inherent robustness of the locators created by following the visual and DOM-based approaches and we then compare empirically a visual and a DOM-based tool, taking into account both the cost for initial test suite development from scratch and the cost for test suite maintenance during code evolution. Since visual tools are known to be computationally demanding, we also measure the test suite execution time. Results indicate that DOM-based locators are generally more robust than visual ones and that DOM-based test cases can be developed from scratch and evolved at lower cost. Moreover, DOM-based test cases require a lower execution time. However, depending on the specific features of the Web application under test and its expected evolution, in some cases visual locators might be the best choice (e.g., when the visual appearance is more stable than the structure).},
author = {Leotta, Maurizio and Clerissi, Diego and Ricca, Filippo and Tonella, Paolo},
booktitle = {International Conference on Web Engineering},
doi = {10.1007/978-3-319-08245-5_19},
issn = {16113349},
pages = {322--340},
title = {{Visual vs. DOM-Based Web Locators: An Empirical Study}},
volume = {8541},
year = {2014}
}


@inproceedings{Luo2014,
abstract = {Regression testing is a crucial part of software development. It checks that software changes do not break existing func-tionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodified test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests— often called flaky tests—have non-deterministic outcomes. Such tests undermine the regression testing as they make it difficult to rely on test results. We present the first extensive study of flaky tests. We study in detail a total of 201 commits that likely fix flaky tests in 51 open-source projects. We classify the most com-mon root causes of flaky tests, identify approaches that could manifest flaky behavior, and describe common strategies that developers use to fix flaky tests. We believe that our insights and implications can help guide future research on the important topic of (avoiding) flaky tests.},
address = {New York, NY, USA},
author = {Luo, Qingzhou and Hariri, Farah and Eloussi, Lamyaa and Marinov, Darko},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
doi = {10.1145/2635868.2635920},
isbn = {9781450330565},
keywords = {empirical study,flaky tests,non-determinism},
month = {11},
pages = {643--653},
publisher = {ACM},
title = {{An empirical analysis of flaky tests}},
year = {2014}
}


@inproceedings{Mahmood2014,
abstract = {Proliferation of Android devices and apps has created a demand for applicable automated software testing techniques. Prior research has primarily focused on either unit or GUI testing of Android apps, but not their end-to-end system testing in a systematic manner. We present EvoDroid, an evolutionary approach for system testing of Android apps. EvoDroid overcomes a key shortcoming of using evolutionary techniques for system testing, i.e., the inability to pass on genetic makeup of good individuals in the search. To that end, EvoDroid combines two novel techniques: (1) an Android-specific program analysis technique that identifies the segments of the code amenable to be searched independently, and (2) an evolutionary algorithm that given information of such segments performs a step- wise search for test cases reaching deep into the code. Our experi- ments have corroborated EvoDroid's ability to achieve significantly higher code coverage than existing Android testing tools.},
address = {New York, New York, USA},
author = {Mahmood, Riyadh and Mirzaei, Nariman and Malek, Sam},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering - FSE 2014},
doi = {10.1145/2635868.2635896},
isbn = {9781450330565},
keywords = {android,evolutionary testing,program analysis},
pages = {599--609},
publisher = {ACM Press},
title = {{EvoDroid: segmented evolutionary testing of Android apps}},
year = {2014}
}

@inproceedings{Marsavina2014,
abstract = {Numerous software development practices suggest updating the test code whenever the production code is changed. However, previous studies have shown that co-evolving test and production code is generally a difficult task that needs to be thoroughly investigated. In this paper we perform a study that, following a mixed methods approach, investigates fine-grained co-evolution patterns of production and test code. First, we mine fine-grained changes from the evolution of 5 open-source systems. Then, we use an association rule mining algorithm to generate the co-evolution patterns. Finally, we interpret the obtained patterns by performing a qualitative analysis. The results show 6 co-evolution patterns and provide insights into their appearance along the history of the analyzed software systems. Besides providing a better understanding of how test code evolves, these findings also help identify gaps in the test code thereby assisting both researchers and developers.},
author = {Marsavina, Cosmin and Romano, Daniele and Zaidman, Andy},
booktitle = {Proceedings of the IEEE 14th International Working Conference on Source Code Analysis and Manipulation},
doi = {10.1109/SCAM.2014.28},
isbn = {978-1-4799-6148-1},
month = {9},
pages = {195--204},
publisher = {IEEE},
title = {{Studying Fine-Grained Co-evolution Patterns of Production and Test Code}},
year = {2014}
}


@inproceedings{Negara2014,
abstract = {Identifying repetitive code changes benefits developers, tool builders, and researchers. Tool builders can automate the popular code changes, thus improving the productivity of developers. Researchers can better understand the practice of code evolution, advancing existing code assistance tools and benefiting developers even further. Unfortunately, existing research either predominantly uses coarse-grained Version Control System (VCS) snapshots as the primary source of code evolution data or considers only a small subset of program transformations of a single kind - refactorings. We present the first approach that identifies previously unknown frequent code change patterns from a fine-grained sequence of code changes. Our novel algorithm effectively handles challenges that distinguish continuous code change pattern mining from the existing data mining techniques. We evaluated our algorithm on 1,520 hours of code development collected from 23 developers, and showed that it is effective, useful, and scales to large amounts of data. We analyzed some of the mined code change patterns and discovered ten popular kinds of high-level program transformations. More than half of our 420 survey participants acknowledged that eight out of ten transformations are relevant to their programming activities.},
address = {New York, NY, USA},
author = {Negara, Stas and Codoban, Mihai and Dig, Danny and Johnson, Ralph E.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
doi = {10.1145/2568225.2568317},
isbn = {9781450327565},
issn = {02705257},
keywords = {Code Changes,Data Mining,Program Transformation},
month = {5},
number = {1},
pages = {803--813},
publisher = {ACM},
title = {{Mining fine-grained code changes to detect unknown change patterns}},
year = {2014}
}



@article{Nguyen2014,
abstract = {Most of today's software applications feature a graphical user interface (GUI) front-end. System testing of these applications requires that test cases, mod- eled as sequences of GUI events, be generated and executed on the software. We term GUI testing as the process of testing a software application through its GUI. Re- searchers and practitioners agree that one must employ a variety of techniques (e.g., model-based, capture/replay, manually scripted) for effective GUI testing. Yet, the tools available today for GUI testing are limited in the techniques they support. In this paper, we describe an innovative tool called GUITAR that supports a wide va- riety of GUI testing techniques. The innovation lies in the architecture of GUITAR, which uses plug-ins to support flexibility and extensibility. Software developers and quality assurance engineers may use this architecture to create new toolchains, new workflows based on the toolchains, and plug in a variety of measurement tools to conduct GUI testing.We demonstrate these features of GUITAR via several carefully crafted case studies.},
author = {Nguyen, Bao N. and Robbins, Bryan and Banerjee, Ishan and Memon, Atif},
doi = {10.1007/s10515-013-0128-9},
issn = {0928-8910},
journal = {Automated Software Engineering},
keywords = {GUI testing,Test automation,Test generation},
month = {3},
number = {1},
pages = {65--105},
title = {{GUITAR: an innovative tool for automated testing of GUI-driven software}},
volume = {21},
year = {2014}
}


@inproceedings{Pirzadeh2014,
abstract = {Developing user interfaces is an important part of software development process for web-applications. Frequent and small changes are inevitable in the process of developing and refactoring a user interface (UI) to improve user experience while preserving the functionality. Those changes however have been shown to easily break a majority of automated UI-level tests. Maintaining and fixing those test cases then increases the overall cost of development and decreases the benefit of test automation. In this paper, we propose a new test framework for identifying and locating the elements of the UI in a way that cannot be impacted by small UI changes. Our approach is inspired by the way human interact with the UIs of web applications and how such interactions are described and communicated in natural language to others. Our framework could significantly reduce the cost of test maintenance by enabling software engineers create UI-level tests that are naturally resistant to UI changes.},
author = {Pirzadeh, Heidar and Shanian, Sara and Davari, Farzin},
booktitle = {Proceedings of the 9th International Conference on the Quality of Information and Communications Technology},
doi = {10.1109/QUATIC.2014.43},
isbn = {978-1-4799-6133-7},
keywords = {Automation,Quality,User Interface Level Test,Web Application},
month = {9},
pages = {268--273},
publisher = {IEEE},
title = {{A Novel Framework for Creating User Interface Level Tests Resistant to Refactoring of Web Applications}},
year = {2014}
}


@inproceedings{Yandrapally2014,
abstract = {Despite the seemingly obvious advantage of test automation, significant skepticism exists in the industry regarding its cost-benefit tradeoffs. Test scripts for web applications are fragile: even small changes in the page layout can break a number of tests, requiring the expense of re-automating them. Moreover, a test script created for one browser cannot be relied upon to run on a different web browser: it requires duplicate effort to create and maintain versions of tests for a variety of browsers. Because of these hidden costs, organizations often fall back to manual testing. We present a fresh solution to the problem of test-script fragility. Often, the root cause of test-script fragility is that, to identify UI elements on a page, tools typically record some metadata that depends on the internal representation of the page in a browser. Our technique eliminates metadata almost entirely. Instead, it identifies UI elements relative to other prominent elements on the page. The core of our technique automatically identifies a series of contextual clues that unambiguously identify a UI element, without recording anything about the internal representation. Empirical evidence shows that our technique is highly accurate in computing contextual clues, and outperforms existing techniques in its resilience to UI changes as well as browser changes.},
address = {New York, New York, USA},
author = {Yandrapally, Rahulkrishna and Thummalapenta, Suresh and Sinha, Saurabh and Chandra, Satish},
booktitle = {Proceedings of the International Symposium on Software Testing and Analysis},
doi = {10.1145/2610384.2610390},
isbn = {9781450326452},
keywords = {GUI test automation,Test-script fragility},
pages = {304--314},
publisher = {ACM Press},
title = {{Robust test automation using contextual clues}},
year = {2014}
}


@article{Zanoni2014,
author = {Zanoni, Marco and Perin, Fabrizio and Fontana, Francesca Arcelli and Viscusi, Gianluigi},
title = {Pattern detection for conceptual schema recovery in data-intensive systems},
journal = {Journal of Software: Evolution and Process},
volume = {26},
number = {12},
pages = {1172-1192},
keywords = {conceptual schema, design pattern detection, object-relational mapping, reverse engineering},
doi = {https://doi.org/10.1002/smr.1656},
abstract = {ABSTRACT In this paper, an approach for information systems reverse engineering is proposed and applied. The aim is to support a unified perspective to the reverse engineering process of both data and software. At the state of the art, indeed, many methods, techniques, and tools for software reverse engineering have been proposed to support program comprehension, software maintenance, and software evolution. Other approaches and tools have been proposed for data reverse engineering, with the aim, for example, to provide complete and up-to-date documentation of legacy databases. However, the two engineering communities often worked independently, and very few approaches addressed the reverse engineering of both data and software as information system's constituencies. Hence, a higher integration is needed to support a better co-evolution of databases and programs, in an environment often characterized by high availability of data and volatility of information flows. Accordingly, the approach we propose leverages the detection of object-relational mapping design patterns to build a conceptual schema of the software under analysis. Then, the conceptual schema is mapped to the domain model of the system, to support the design of the evolution of the information system itself. The approach is evaluated on two large-scale open-source enterprise applications. Copyright © 2014 John Wiley \& Sons, Ltd.},
year = {2014}
}


@article{Alegroth2015,
abstract = {In today's software development industry, high-level tests such as Graphical User Interface (GUI) based system and acceptance tests are mostly performed with manual practices that are often costly, tedious and error prone. Test automation has been proposed to solve these problems but most automation techniques approach testing from a lower level of system abstraction. Their suitability for high-level tests has therefore been questioned. High-level test automation techniques such as Record and Replay exist, but studies suggest that these techniques suffer from limitations, e.g. sensitivity to GUI layout or code changes, system implementation dependencies, etc. Visual GUI Testing (VGT) is an emerging technique in industrial practice with perceived higher flexibility and robustness to certain GUI changes than previous high-level (GUI) test automation techniques. The core of VGT is image recognition which is applied to analyze and interact with the bitmap layer of a system's front end. By coupling image recognition with test scripts, VGT tools can emulate end user behavior on almost any GUI-based system, regardless of implementation language, operating system or platform. However, VGT is not without its own challenges, problems and limitations (CPLs) but, like for many other automated test techniques, there is a lack of empirically-based knowledge of these CPLs and how they impact industrial applicability. Crucially, there is also a lack of information on the cost of applying this type of test automation in industry. This manuscript reports an empirical, multi-unit case study performed at two Swedish companies that develop safety-critical software. It studies their transition from manual system test cases into tests automated with VGT. In total, four different test suites that together include more than 300 high-level system test cases were automated for two multi-million lines of code systems. The results show that the transitioned test cases could find defects in the tested systems and that all applicable test cases could be automated. However, during these transition projects a number of hurdles had to be addressed; a total of 58 different CPLs were identified and then categorized into 26 types. We present these CPL types and an analysis of the implications for the transition to and use of VGT in industrial software development practice. In addition, four high-level solutions are presented that were identified during the study, which would address about half of the identified CPLs. Furthermore, collected metrics on cost and return on investment of the VGT transition are reported together with information about the VGT suites' defect finding ability. Nine of the identified defects are reported, 5 of which were unknown to testers with extensive experience from using the manual test suites. The main conclusion from this study is that even though there are many challenges related to the transition and usage of VGT, the technique is still valuable, flexible and considered cost-effective by the industrial practitioners. The presented CPLs also provide decision support in the use and advancement of VGT and potentially other automated testing techniques similar to VGT, e.g. Record and Replay.},
author = {Al{\'{e}}groth, Emil and Feldt, Robert and Ryrholm, Lisa},
journal = {Empirical Software Engineering},
keywords = {Challenges,Development cost,Industrial case study,Problems and Limitations,System and acceptance test automation,Visual GUI Testing},
month = {6},
number = {3},
pages = {694--744},
title = {{Visual GUI testing in practice: challenges, problemsand limitations}},
volume = {20},
year = {2015}
}

@article{Barr2015,
abstract = {Testing involves examining the behaviour of a system in order to discover potential faults. Given an input for a system, the challenge of distinguishing the corresponding desired, correct behaviour from potentially incorrect behavior is called the 'test oracle problem'. Test oracle automation is important to remove a current bottleneck that inhibits greater overall test automation. Without test oracle automation, the human has to determine whether observed behaviour is correct. The literature on test oracles has introduced techniques for oracle automation, including modelling, specifications, contract-driven development and metamorphic testing. When none of these is completely adequate, the final source of test oracle information remains the human, who may be aware of informal specifications, expectations, norms and domain specific information that provide informal oracle guidance. All forms of test oracles, even the humble human, involve challenges of reducing cost and increasing benefit. This paper provides a comprehensive survey of current approaches to the test oracle problem and an analysis of trends in this important area of software testing research and practice.},
author = {Barr, Earl T. and Harman, Mark and McMinn, Phil and Shahbaz, Muzammil and Yoo, Shin},
doi = {10.1109/TSE.2014.2372785},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Automatic testing,Test oracle,Testing formalism},
month = {5},
number = {5},
pages = {507--525},
publisher = {IEEE},
title = {{The Oracle Problem in Software Testing: A Survey}},
volume = {41},
year = {2015}
}


@article{Bavota2015,
abstract = {Bad code smells have been defined as indicators of potential problems in source code. Techniques to identify and mitigate bad code smells have been proposed and studied. Recently bad test code smells (test smells for short) have been put forward as a kind of bad code smell specific to tests such a unit tests. What has been missing is empirical investigation into the prevalence and impact of bad test code smells. Two studies aimed at providing this missing empirical data are presented. The first study finds that there is a high diffusion of test smells in both open-source and industrial software systems with 86 {\%} of JUnit tests exhibiting at least one test smell and six tests having six distinct test smells. The second study provides evidence that test smells have a strong negative impact on program comprehension and maintenance. Highlights from this second study include the finding that comprehension is 30 {\%} better in the absence of test smells.},
author = {Bavota, Gabriele and Qusef, Abdallah and Oliveto, Rocco and {De Lucia}, Andrea and Binkley, Dave},
doi = {10.1007/s10664-014-9313-0},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Controlled experiments,Mining software repositories,Test smells,Unit testing},
month = {8},
number = {4},
pages = {1052--1094},
title = {{Are test smells really harmful? An empirical study}},
volume = {20},
year = {2015}
}


@article{Cohen2015,
abstract = {We discuss a key problem in information extraction which deals with wrapper failures due to changing content templates. A good proportion of wrapper failures are due to HTML templates changing to cause wrappers to become incompatible after element inclusion or removal in a DOM (Tree representation of HTML). We perform a large-scale empirical analyses of the causes of shift and mathematically quantify the levels of domain difficulty based on entropy. We propose the XTreePath annotation method to captures contextual node information from the training DOM. We then utilize this annotation in a supervised manner at test time with our proposed Recursive Tree Matching method which locates nodes most similar in context recursively using the tree edit distance. The search is based on a heuristic function that takes into account the similarity of a tree compared to the structure that was present in the training data. We evaluate XTreePath using 117,422 pages from 75 diverse websites in 8 vertical markets. Our XTreePath method consistently outperforms XPath and a current commercial system in terms of successful extractions in a blackbox test. We make our code and datasets publicly available online.},
author = {Cohen, Joseph Paul and Ding, Wei and Bagherjeiran, Abraham},
journal = {arXiv: Information Retrieval},
month = {5},
title = {{XTreePath: A generalization of XPath to handle real world structural variation}},
year = {2015}
}


@inproceedings{Hauptmann2015,
abstract = {Automated system tests often have many clones, which make them complex to understand and costly to maintain. Unfortunately, removing clones is challenging as there are numerous possibilities of how to refactor them to reuse components such as subroutines. Additionally, clones often overlap partly which makes it particularly difficult to decide which parts to extract. If done wrongly, reuse potential is not leveraged optimally and structures between tests and reuse components will become unnecessarily complex. We present a method to support test engineers in extracting overlapping clones. Using grammar inference algorithms, we generate a refactoring proposal that demonstrates test engineers how overlapping clones can be extracted. Furthermore, we visualize the generated refactoring proposal to make it easily understandable for test engineers. An industrial case study demonstrates that our approach helps test engineers to gain information of the reuse potential of test suites and guides them to perform refactorings.},
author = {Hauptmann, Benedikt and Eder, Sebastian and Junker, Maximilian and Juergens, Elmar and Woinke, Volkmar},
booktitle = {Proceedings of IEEE 23rd International Conference on Program Comprehension},
doi = {10.1109/ICPC.2015.20},
isbn = {978-1-4673-8159-8},
keywords = {Automated Testing,Refactoring,Test Clones},
month = {5},
pages = {115--124},
publisher = {IEEE},
title = {{Generating Refactoring Proposals to Remove Clones from Automated System Tests}},
volume = {2015-August},
year = {2015}
}

@inproceedings{Kochhar2015,
abstract = {Smartphone applications (apps) have gained popularity recently. Millions of smartphone applications (apps) are available on different app stores which gives users plethora of options to choose from, however, it also raises concern if these apps are adequately tested before they are released for public use. In this study, we want to understand the test automation culture prevalent among app developers. Specifically, we want to examine the current state of testing of apps, the tools that are commonly used by app developers, and the problems faced by them. To get an insight on the test automation culture, we conduct two different studies. In the first study, we analyse over 600 Android apps collected from F- Droid, one of the largest repositories containing information about open-source Android apps. We check for the presence of test cases and calculate code coverage to measure the adequacy of testing in these apps. We also survey developers who have hosted their applications on GitHub to understand the testing practices followed by them. We ask developers about the tools that they use and ''pain points'' that they face while testing Android apps. For the second study, based on the responses from Android developers, we improve our survey questions and resend it to Windows app developers within Microsoft. We conclude that many Android apps are poorly tested - only about 14\% of the apps contain test cases and only about 9\% of the apps that have executable test cases have coverage above 40\%. Also, we find that Android app developers use automated testing tools such as JUnit, Monkeyrunner, Robotium, and Robolectric, however, they often prefer to test their apps manually, whereas Windows app developers prefer to use in-house tools such as Visual Studio and Microsoft Test Manager. Both Android and Windows app developers face many challenges such as time constraints, compatibility issues, lack of exposure, cumbersome tools, etc. We give suggestions to improve the test automation culture in the growing app community.},
author = {Kochhar, Pavneet Singh and Thung, Ferdian and Nagappan, Nachiappan and Zimmermann, Thomas and Lo, David},
booktitle = {Proceeding of the 8th IEEE International Conference on Software Testing, Verification and Validation (ICST)},
doi = {10.1109/ICST.2015.7102609},
isbn = {978-1-4799-7125-1},
keywords = {Android,App Developers,Microsoft,Test Automation Culture},
month = {4},
pages = {1--10},
publisher = {IEEE},
title = {{Understanding the Test Automation Culture of App Developers}},
year = {2015}
}



@inproceedings{Leotta2015,
abstract = {The main reason for the fragility of web test cases is the inability of web element locators to work correctly when the web page DOM evolves. Web elements locators are used in web test cases to identify all the GUI objects to operate upon and eventually to retrieve web page content that is compared against some oracle in order to decide whether the test case has passed or not. Hence, web element locators play an extremely important role in web testing and when a web element locator gets broken developers have to spend substantial time and effort to repair it. While algorithms exist to produce robust web element locators to be used in web test scripts, no algorithm is perfect and different algorithms are exposed to different fragilities when the software evolves. Based on such observation, we propose a new type of locator, named multi-locator, which selects the best locator among a candidate set of locators produced by different algorithms. Such selection is based on a voting procedure that assigns different voting weights to different locator generation algorithms. Experimental results obtained on six web applications, for which a subsequent release was available, show that the multi-locator is more robust than the single locators (about -30{\%} of broken locators w.r.t. the most robust kind of single locator) and that the execution overhead required by the multiple queries done with different locators is negligible (2-3{\%} at most).},
author = {Leotta, Maurizio and Stocco, Andrea and Ricca, Filippo and Tonella, Paolo},
booktitle = {Proceedings of IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)},
keywords = {Test Case Robustness,Testware Evolution,Web Element Locators,Web Testing,XPath Locators},
month = {4},
pages = {1--10},
publisher = {IEEE},
title = {{Using Multi-Locators to Increase the Robustness of Web Test Cases}},
year = {2015}
}

@inproceedings{Leotta2015b,
abstract = {Test scripts used for web testing rely on DOM locators, often expressed as XPaths, to identify the active web page elements and the web page data to be used in assertions. When the web application evolves, the major cost incurred for the evolution of the test scripts is due to broken locators, which fail to locate the target element in the new version of the software. We formulate the problem of automatically generating robust XPath locators as a graph exploration problem, for which we provide an optimal, greedy algorithm. Since such an algorithm has exponential time and space complexity, we present also a genetic algorithm.},
author = {Leotta, Maurizio and Stocco, Andrea and Ricca, Filippo and Tonella, Paolo},
booktitle = {2015 IEEE/ACM 8th International Workshop on Search-Based Software Testing},
doi = {10.1109/SBST.2015.16},
isbn = {978-1-4673-7079-0},
keywords = {DOM-based Locators,Fragile Test,Web Element Locators,Web Testing,XPath Locators Robustness},
month = {5},
pages = {36--39},
publisher = {IEEE},
title = {{Meta-heuristic Generation of Robust XPath Locators for Web Testing}},
year = {2015}
}


@article{Pandit2015,
abstract = {User Acceptance Testing (UAT) has widespread implications in the software community. It involves not only the end-user, but the Quality Assurance (QA) team, developers, business analysts and top level management. UAT is conducted with the aim of developing confidence of the user in the software product. UAT is generally performed manually and not preferred to be automated. UAT frameworks exist for Agile methodologies such as Scrum. We propose a UAT process model which adapts the generic agile process model. Hence, it is able to encompass every agile methodology. AgileUAT, aims at generation of exhaustive acceptance test cases in natural language, based on acceptance criteria. It indicates whether the acceptance criteria is fulfilled or not, as a percentage value. The tool illustrates traceability among epics, user stories, acceptance criteria and acceptance test cases. We explore several different templates for user stories and acceptance criteria. In the future, we aim to provide a direct mapping between the acceptance criteria and acceptance test cases based on permutations and combinations using decision tables.},
author = {Pandit, Pallavi and Tahiliani, Swati},
doi = {10.5120/21262-3533},
issn = {09758887},
journal = {International Journal of Computer Applications},
month = {6},
number = {10},
pages = {16--21},
title = {{AgileUAT: A Framework for User Acceptance Testing based on User Stories and Acceptance Criteria}},
volume = {120},
year = {2015}
}

@inproceedings{Tang2015,
abstract = {Context: Software source code is frequently changed for fixing revealed bugs. These bug-fixing changes might introduce unintended system behaviors, which are inconsistent with scenarios of existing regression test cases, and consequently break regression testing. For validating the quality of changes, regression testing is a required process before submitting changes during the development of software projects. Our pilot study shows that 48.7\% bug-fixing changes might break regression testing at first run, which means developers have to run regression testing at least a couple of times for 48.7\% changes. Such process can be tedious and time consuming. Thus, before running regression test suite, finding these changes and corresponding regression test cases could be helpful for developers to quickly fix these changes and improve the efficiency of regression testing. Goal: This paper proposes bug- fixing change impact prediction (BFCP), for predicting whether a bug-fixing change will break regression testing or not before running regression test cases, by mining software change histories. Method: Our approach employs the machine learning algorithms and static call graph analysis technique. Given a bug-fixing change, BFCP first predicts whether it will break existing regression test cases; second, if the change is predicted to break regression test cases, BFCP can further identify the might-be-broken test cases. Results: Results of experiments on 552 real bug-fixing changes from four large open source projects show that BFCP could achieve prediction precision up to 83.3\% recall up to 92.3\% and F-score up to 81.4\%. For identifying the might-be-broken test cases, BFCP could achieve 100\% recall.},
author = {Tang, Xinye and Wang, Song and Mao, Ke},
booktitle = {Proceedings of ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1109/ESEM.2015.7321218},
isbn = {978-1-4673-7899-4},
issn = {19493789},
keywords = {regression testing,source code change impact analysis,static program analysis},
month = {10},
pages = {1--10},
publisher = {IEEE},
title = {{Will This Bug-Fixing Change Break Regression Testing?}},
volume = {2015-Novem},
year = {2015}
}


@inproceedings{Vahabzadeh2015,
abstract = {Testing aims at detecting (regression) bugs in production code. However, testing code is just as likely to contain bugs as the code it tests. Buggy test cases can silently miss bugs in the production code or loudly ring false alarms when the production code is correct. We present the first empirical study of bugs in test code to characterize their prevalence and root cause categories. We mine the bug repositories and version control systems of 211 Apache Software Foundation (ASF) projects and find 5,556 test-related bug reports. We (1) compare properties of test bugs with production bugs, such as active time and fixing effort needed, and (2) qualitatively study 443 randomly sampled test bug reports in detail and categorize them based on their impact and root causes. Our results show that (1) around half of all the projects had bugs in their test code; (2) the majority of test bugs are false alarms, i.e., test fails while the production code is correct, while a minority of these bugs result in silent horrors, i.e., test passes while the production code is incorrect; (3) incorrect and missing assertions are the dominant root cause of silent horror bugs; (4) semantic (25\%), flaky (21\%), environment-related (18\%) bugs are the dominant root cause categories of false alarms; (5) the majority of false alarm bugs happen in the exercise portion of the tests, and (6) developers contribute more actively to fixing test bugs and test bugs are fixed sooner compared to production bugs. In addition, we evaluate whether existing bug detection tools can detect bugs in test code.},
author = {Vahabzadeh, Arash and Fard, Amin Milani and Mesbah, Ali},
booktitle = {2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
doi = {10.1109/ICSM.2015.7332456},
isbn = {978-1-4673-7532-0},
keywords = {Bugs,empirical study,test code},
month = {9},
pages = {101--110},
publisher = {IEEE},
title = {{An empirical study of bugs in test code}},
year = {2015}
}



@article{Alegroth2016,
abstract = {Context: Verification and validation (V{\&}V) activities make up 20-50{\%} of the total development costs of a software system in practice. Test automation is proposed to lower these V{\&}V costs but available research only provides limited empirical data from industrial practice about the maintenance costs of automated tests and what factors affect these costs. In particular, these costs and factors are unknown for automated GUI-based testing. Objective: This paper addresses this lack of knowledge through analysis of the costs and factors associated with the maintenance of automated GUI-based tests in industrial practice. Method: An empirical study at two companies, Siemens and Saab, is reported where interviews about, and empirical work with, Visual GUI Testing is performed to acquire data about the technique's maintenance costs and feasibility. Results: 13 factors are observed that affect maintenance, e.g. tester knowledge/experience and test case complexity. Further, statistical analysis shows that developing new test scripts is costlier than maintenance but also that frequent maintenance is less costly than infrequent, big bang maintenance. In addition a cost model, based on previous work, is presented that estimates the time to positive return on investment (ROI) of test automation compared to manual testing. Conclusions: It is concluded that test automation can lower overall software development costs of a project while also having positive effects on software quality. However, maintenance costs can still be considerable and the less time a company currently spends on manual testing, the more time is required before positive, economic, ROI is reached after automation.},
archivePrefix = {arXiv},
arxivId = {1602.01226},
author = {Al{\'{e}}groth, Emil and Feldt, Robert and Kolstr{\"{o}}m, Pirjo},
doi = {10.1016/j.infsof.2016.01.012},
eprint = {1602.01226},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Empirical,Industrial,Maintenance,Return on investment,Visual GUI Testing},
pages = {66--80},
title = {{Maintenance of automated test suites in industry: An empirical study on Visual GUI Testing}},
volume = {73},
year = {2016}
}

@inproceedings{Alegroth2016b,
abstract = {Technical debt (TD) is a concept used to describe a sub-optimal solution of a software artifact that negatively affects its comprehensibility, extendability and maintainability. As such, TD adversely affects the costs or quality associated with the artifact, which is also called interest. TD has through research been identified in all types of software artifacts, from architectural design to automated tests (Testware). However, research into testware technical debt (TTD) is limited and primarily focused on testing on lower level of system abstraction, i.e. unit-and integration tests, leaving a need for more TTD research on GUI-based testing. In this study we explore this gap in knowledge through an industrial case study at a Swedish avionics software development company. Four repositories are studied for the presence of TTD using expert interviews, semi-automated document analysis and automatic metric analysis. Results of the study provide initial support that the concept of TTD is applicable to GUI-based testware and show the presence of both TD items unique to GUI-based testware and items common to software. The implications of these results are that engineering best practices must be established for GUI-based testware to minimize TD interest.},
author = {Alegroth, Emil and Steiner, Marcello and Martini, Antonio},
booktitle = {2016 IEEE Ninth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
doi = {10.1109/ICSTW.2016.47},
isbn = {978-1-5090-3674-5},
keywords = {GUI-based testing,Industrial case study,Technical debt,Testware},
month = {4},
pages = {257--262},
publisher = {IEEE},
title = {{Exploring the Presence of Technical Debt in Industrial GUI-Based Testware: A Case Study}},
year = {2016}
}

@article{ArcelliFontana2016,
abstract = {Several code smell detection tools have been developed providing different results, because smells can be subjectively interpreted, and hence detected, in different ways. In this paper, we perform the largest experiment of applying machine learning algorithms to code smells to the best of our knowledge. We experiment 16 different machine-learning algorithms on four code smells (Data Class, Large Class, Feature Envy, Long Method) and 74 software systems, with 1986 manually validated code smell samples. We found that all algorithms achieved high performances in the cross-validation data set, yet the highest performances were obtained by J48 and Random Forest, while the worst performance were achieved by support vector machines. However, the lower prevalence of code smells, i.e., imbalanced data, in the entire data set caused varying performances that need to be addressed in the future studies. We conclude that the application of machine learning to the detection of these code smells can provide high accuracy ({\textgreater}96 {\%}), and only a hundred training examples are needed to reach at least 95 {\%} accuracy.},
author = {{Arcelli Fontana}, Francesca and M{\"{a}}ntyl{\"{a}}, Mika V. and Zanoni, Marco and Marino, Alessandro},
doi = {10.1007/s10664-015-9378-4},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Benchmark for code smell detection,Code smells detection,Machine learning techniques},
month = {6},
number = {3},
pages = {1143--1191},
publisher = {Empirical Software Engineering},
title = {{Comparing and experimenting machine learning techniques for code smell detection}},
volume = {21},
year = {2016}
}

@inproceedings{Coppola2016,
abstract = {Automated UI testing suffers from fragility due to continuous - although minor - changes in the UI of applications. Such fragility has been shown especially for the web domain, though no clear evidence is available for mobile applications. Our goal is to perform an exploratory assessment of the extent and causes of the fragiliy of UI automated tests for mobile applications. For this purpose, we analyzed a small test suite -that we developed using five different testing frameworks- for an Android application (K-9 Mail) and observed the changes induced in the tests by the evolution of the UI. We found that up to 75\% of code-based tests, and up to 100\% of image recognition tests, had to be adapted because of the changes induced by the evolution of the application between two different versions. In addition we identified the main causes of such fragility: changes of identifiers, text or graphics, removal or relocation of elements, activity ow variation, execution time variation, and usage of physical buttons. The preliminary assessment showed that the fragility of UI tests can be a relevant issue also for mobile applications. A few common causes were found that can be used as the basis for providing guidelines for fragility avoidance and repair.},
author = {Coppola, Riccardo and Raffero, Emanuele and Torchiano, Marco},
booktitle = {Proceedings of the 2nd International Workshop on User Interface Test Automation},
doi = {10.1145/2945404.2945406},
isbn = {9781450344128},
keywords = {Automated,Empirical,Fragility,Test,UI},
month = {7},
pages = {11--20},
publisher = {ACM},
title = {{Automated mobile UI test fragility: an exploratory assessment study on Android}},
year = {2016}
}

@article{Gao2016,
abstract = {System testing of a GUI-based application requires that test cases, consisting of sequences of user actions/events, be executed and the software's output be verified. To enable automated re-testing, such test cases are increasingly being coded as low-level test scripts, to be replayed automatically using test harnesses. Whenever the GUI changes-widgets get moved around, windows get merged-some scripts become unusable because they no longer encode valid input sequences. Moreover, because the software's output may have changed, their test oracles-assertions and checkpoints-encoded in the scripts may no longer correctly check the intended GUI objects. We present ScrIpT repAireR (SITAR), a technique to automatically repair unusable low-level test scripts. SITAR uses reverse engineering techniques to create an abstract test for each script, maps it to an annotated event-flow graph (EFG), uses repairing transformations and human input to repair the test, and synthesizes a new “repaired” test script. During this process, SITAR also repairs the reference to the GUI objects used in the checkpoints yielding a final test script that can be executed automatically to validate the revised software. SITAR amortizes the cost of human intervention across multiple scripts by accumulating the human knowledge as annotations on the EFG. An experiment using QTP test scripts suggests that SITAR is effective in that 41-89 percent unusable test scripts were repaired. Annotations significantly reduced human cost after 20 percent test scripts had been repaired.},
author = {Gao, Zebao and Chen, Zhenyu and Zou, Yunxiao and Memon, Atif M.},
doi = {10.1109/TSE.2015.2454510},
isbn = {0098-5589 VO - 42},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {GUI test script,GUI testing,human knowledge accumulation,test script repair},
month = {2},
number = {2},
pages = {170--186},
title = {{SITAR: GUI Test Script Repair}},
volume = {42},
year = {2016}
}



@article{Garousi2016,
abstract = {With the increasing importance, size, and complexity of automated test suites, the need exists for suitable methods and tools to develop, assess the quality of, and maintain test code (scripts) in parallel with regular production (application) code. A recent review paper called this subarea of software testing software test code engineering (STCE). This article summarizes STCE tools, techniques, and guidelines. It also presents specific quantitative examples in this area based on experience in projects and raises important issues practitioners and researchers must address to further advance this field.},
author = {Garousi, Vahid and Felderer, Michael},
doi = {10.1109/MS.2016.30},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {software engineering,software test code engineering,software testing,test automation,test code,test scripts},
month = {5},
number = {3},
pages = {68--75},
publisher = {IEEE},
title = {{Developing, Verifying, and Maintaining High-Quality Automated Test Scripts}},
volume = {33},
year = {2016}
}



@inproceedings{Hammoudi2016,
abstract = {Software engineers often use record/replay tools to enable the automated testing of web applications. Tests created in this manner can then be used to regression test new versions of the web applications as they evolve. Web application tests recorded by record/replay tools, however, can be quite brittle, they can easily break as applications change. For this reason, researchers have begun to seek approaches for automatically repairing record/replay tests. To date, however, there have been no comprehensive attempts to characterize the causes of breakagesin record/replay tests for web applications. In this work, wepresent a taxonomy classifying the ways in which record/replay tests for web applications break, based on an analysis of 453 versions of popular web applications for which 1065 individual test breakages were recognized. The resulting taxonomy can help direct researchers in their attempts to repair such tests. It can also help practitioners by suggesting best practices when creating tests or modifying programs, and can help researchers with other tasks such as test robustness analysis and IDE design.},
author = {Hammoudi, Mouna and Rothermel, Gregg and Tonella, Paolo},
booktitle = {2016 IEEE International Conference on Software Testing, Verification and Validation},
keywords = {Record/replay testing,test breakages,test repair,web applications},
month = {4},
pages = {180--190},
publisher = {IEEE},
title = {{Why do Record/Replay Tests of Web Applications Break?}},
year = {2016}
}

@inproceedings{Hammoudi2016b,
abstract = {Software engineers use record replay tools to capture use case scenarios that can serve as regression tests for web applications. Such tests, however, can be brittle in the face of code changes. Thus, researchers have sought automated approaches for repairing broken record/replay tests. To date, such approaches have operated by directly analyzing differences between the releases of web applications. Often, however, intermediate versions or commits exist between releases, and these represent finer-grained sequences of changes by which new releases evolve. In this paper, we present WATERFALL, an incremental test repair approach that applies test repair techniques iteratively across a sequence of fine-grained versions of a web application. The results of an empirical study on seven web applications show that our approach is substantially more effective than a coarse-grained approach (209\% overall), while maintaining an acceptable level of overhead.},
address = {New York, NY, USA},
author = {Hammoudi, Mouna and Rothermel, Gregg and Stocco, Andrea},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
doi = {10.1145/2950290.2950294},
isbn = {9781450342186},
keywords = {Record/replay tests,Test case repair,Web applications},
month = {11},
pages = {751--762},
publisher = {ACM},
title = {{WATERFALL: an incremental approach for repairing record-replay tests of web applications}},
volume = {13-18-Nove},
year = {2016}
}


@article{Leotta2016,
abstract = {Automated test scripts are used with success in many web development projects, so as to automatically verify key functionalities of the web application under test, reveal possible regressions and run a large number of tests in short time. However, the adoption of automated web testing brings advantages but also novel problems, among which the test code fragility problem. During the evolution of the web application, existing test code may easily break and testers have to correct it. In the context of automated DOM-based web testing, one of the major costs for evolving the test code is the manual effort necessary to repair broken web page element locators – lines of source code identifying the web elements (e.g. form fields and buttons) to interact with. In this work, we present ROBULA+, a novel algorithm able to generate robust XPath-based locators – locators that are likely to work correctly on new releases of the web application. We compared ROBULA+ with several state of the practice/art XPath locator generator tools/algorithms. Results show that XPath locators produced by ROBULA+ are by far the most robust. Indeed, ROBULA+ reduces the locators' fragility on average by 90{\%} w.r.t. absolute locators and by 63{\%} w.r.t. Selenium IDE locators. Copyright},
author = {Leotta, Maurizio and Stocco, Andrea and Ricca, Filippo and Tonella, Paolo},
journal = {Journal of Software: Evolution and Process},
keywords = {DOM selector,maintenance effort reduction,rbust XPath locator,test cases fragility,web testing},
month = {3},
number = {3},
pages = {177--204},
title = {{Robula+: an algorithm for generating robust XPath locators for web testing}},
volume = {28},
year = {2016}
}


@inproceedings{Mao2016,
abstract = {We introduce Sapienz, an approach to Android testing that uses multi-objective search-based testing to automatically explore and optimise test sequences, minimising length, while simultaneously maximising coverage and fault revelation. Sapienz combines random fuzzing, systematic and search-based exploration, exploiting seeding and multi-level instrumentation. Sapienz significantly outperforms (with large effect size) both the state-of-the-art technique Dynodroid and the widely-used tool, Android Monkey, in 7/10 experiments for coverage, 7/10 for fault detection and 10/10 for fault-revealing sequence length. When applied to the top 1,000 Google Play apps, Sapienz found 558 unique, previously unknown crashes. So far we have managed to make contact with the developers of 27 crashing apps. Of these, 14 have confirmed that the crashes are caused by real faults. Of those 14, six already have developer-confirmed fixes.},
address = {New York, NY, USA},
author = {Mao, Ke and Harman, Mark and Jia, Yue},
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
doi = {10.1145/2931037.2931054},
isbn = {9781450343909},
keywords = {android,search-based software testing,test generation},
month = {7},
pages = {94--105},
publisher = {ACM},
title = {{Sapienz: multi-objective automated testing for Android applications}},
year = {2016}
}


@inproceedings{Tufano2016,
abstract = {Test smells have been defined as poorly designed tests and, as reported by recent empirical studies, their presence may negatively affect comprehension and maintenance of test suites. Despite this, there are no available automated tools to support identification and repair of test smells. In this paper, we firstly investigate developers' perception of test smells in a study with 19 participants. The results show that developers generally do not recognize (potentially harmful) test smells, highlighting that automated tools for identifying such smells are much needed. However, to build effective tools, deeper insights into the test smells phenomenon are required. To this aim, we conducted a large-scale empirical investigation aimed at analyzing (i) when test smells occur in source code, (ii) what their survivability is, and (iii) whether their presence is associated with the presence of design problems in production code (code smells). The results indicate that test smells are usually introduced when the corresponding test code is committed in the repository for the first time, and they tend to remain in a system for a long time. Moreover, we found various unexpected relationships between test and code smells. Finally, we show how the results of this study can be used to build e.ective automated tools for test smell detection and refactoring.},
address = {Singapore, Singapore},
author = {Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and {Di Penta}, Massimiliano and Oliveto, Rocco and {De Lucia}, Andrea and Poshyvanyk, Denys},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering - ASE 2016},
doi = {10.1145/2970276.2970340},
isbn = {9781450338455},
keywords = {Mining Software Repositories,Software Evolution,Test Smells},
pages = {4--15},
publisher = {ACM Press},
title = {{An empirical investigation into the nature of test smells}},
year = {2016}
}


@online{W3C2016,
url={https://www.w3.org/TR/1999/REC-xpath-19991116/},
title={XML Path Language (XPath)},
author={World Wide Web Consortium},
year={1999},
month={11},
urldate={2021-01-16}
}


@inproceedings{Aldalur2017,
abstract = {Web locators uniquely identify elements on the Web Content. They are heavily used in different scenarios, from Web harvesting to Web testing and browser extensions. Locators' Achilles heel is their fragility upon Website upgrades. This work tackles locator fragility in the context of browser extensions. We introduce regenerative locator, i.e. traditional structure-based locators which are supplemented with contingency data from the target node. The aim: keeping browser extensions up and running for as long as possible. Eight case studies are analysed by considering real Website upgrades taken from Wayback Machine. Figures indicate a 70{\%} success in regenerating broken locators without interrupting extension functioning.},
address = {New York, NY, USA},
author = {Aldalur, Inigo and Diaz, Oscar},
booktitle = {Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
month = {6},
pages = {45--50},
publisher = {ACM},
title = {{Addressing web locator fragility}},
year = {2017}
}


@inproceedings{Bowes2017,
abstract = {Background: Test quality is a prerequisite for achieving production system quality. While the concept of quality is multidimensional, most of the effort in testing context hasbeen channelled towards measuring test effectiveness. Objective: While effectiveness of tests is certainly important, we aim to identify a core list of testing principles that also address other quality facets of testing, and to discuss how they can be quantified as indicators of test quality. Method: We have conducted a two-day workshop with our industry partners to come up with a list of relevant principles and best practices expected to result in high quality tests. We then utilised our academic and industrial training materials together with recommendations in practitioner oriented testing books to refine the list. We surveyed existing literature for potential metrics to quantify identified principles. Results: We have identified a list of 15 testing principles to capture the essence of testing goals and best practices from quality perspective. Eight principles do not map toexisting test smells and we propose metrics for six of those. Further, we have identified additional potential metrics for the seven principles that partially map to test smells. Conclusion: We provide a core list of testing principles along with a discussion of possible ways to quantify them for assessing goodness of tests. We believe that our work would be useful for practitioners in assessing the quality of their tests from multiple perspectives including but not limited to maintainability, comprehension and simplicity.},
author = {Bowes, David and Hall, Tracy and Petric, Jean and Shippey, Thomas and Turhan, Burak},
booktitle = {2017 IEEE/ACM 8th Workshop on Emerging Trends in Software Metrics (WETSoM)},
doi = {10.1109/WETSoM.2017.2},
isbn = {978-1-5386-2807-2},
issn = {23270969},
keywords = {metrics,test quality,unit testing},
month = {5},
pages = {9--14},
publisher = {IEEE},
title = {{How Good Are My Tests?}},
year = {2017}
}

@inproceedings{Coppola2017,
author = {Coppola, Riccardo and Morisio, Maurizio and Torchiano, Marco},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
doi = {10.1145/3127005.3127008},
isbn = {9781450353052},
keywords = {automated software testing, GUI testing, mobile development, software evolution, software maintenance},
month = {11},
pages = {22--32},
publisher = {ACM},
title = {{Scripted GUI Testing of Android Apps}},
year = {2017}
}


@article{Femmer2017,
abstract = {Bad requirements quality can cause expensive consequences during the software development lifecycle, especially if iterations are long and feedback comes late. We aim at a light-weight static requirements analysis approach that allows for rapid checks immediately when requirements are written down. We transfer the concept of code smells to requirements engineering as Requirements Smells. To evaluate the benefits and limitations, we define Requirements Smells, realize our concepts for a smell detection in a prototype called Smella and apply Smella in a series of cases provided by three industrial and a university context. The automatic detection yields an average precision of 59{\%} at an average recall of 82{\%} with high variation. The evaluation in practical environments indicates benefits such as an increase of the awareness of quality defects. Yet, some smells were not clearly distinguishable. Lightweight smell detection can uncover many practically relevant requirements defects in a reasonably precise way. Although some smells need to be defined more clearly, smell detection provides a helpful means to support quality assurance in requirements engineering, for instance, as a supplement to reviews.},
author = {Femmer, Henning and {M{\'{e}}ndez Fern{\'{a}}ndez}, Daniel and Wagner, Stefan and Eder, Sebastian},
doi = {10.1016/j.jss.2016.02.047},
eprint = {1611.08847},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Automatic defect detection,Requirements Smells,Requirements engineering},
month = {1},
pages = {190--213},
publisher = {Elsevier Inc.},
title = {{Rapid quality assurance with Requirements Smells}},
volume = {123},
year = {2017}
}

@article{Labuschagne2017,
abstract = {Software defects cost time and money to diagnose and fix. Consequently, developers use a variety of techniques to avoid introducing defects into their systems. However, these tech- niques have costs of their own; the benefit of using a technique must outweigh the cost of applying it. In this paper we investigate the costs and benefits of auto- mated regression testing in practice. Specifically, we studied 61 projects that use Travis CI, a cloud-based continuous integration tool, in order to examine real test failures that were encountered by the developers of those projects. We determined how the developers resolved the failures they encountered and used this information to classify the failures as being caused by a flaky test, by a bug in the system under test, or by a broken or obsolete test. We consider that test failures caused by bugs represent a benefit of the test suite, while failures caused by broken or obsolete tests represent a test suite maintenance cost. We found that 18{\%} of test suite executions fail and that 13{\%} of these failures are flaky. Of the non-flaky failures, only 74{\%} were caused by a bug in the system under test; the remaining 26{\%} were due to incorrect or obsolete tests. In addition, we found that, in the failed builds, only 0.38{\%} of the test case executions failed and 64{\%} of failed builds contained more than one failed test. Our findings contribute to a wider understanding of the unforeseen costs that can impact the overall cost effectiveness of regression testing in practice. They can also inform re- search into test case selection techniques, as we have provided an approximate empirical bound on the practical value that could be extracted from such techniques. This value appears to be large, as the 61 systems under study contained nearly 3 million lines of test code and yet over 99{\%} of test case executions could have been eliminated with a perfect oracle.},
author = {Labuschagne, Adriaan and Inozemtseva, Laura and Holmes, Reid},
doi = {10.1145/3106237.3106288},
isbn = {9781450351058},
journal = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {821--830},
title = {{Measuring the cost of regression testing in practice: a study of Java projects using continuous integration}},
year = {2017}
}

@article{Lavoie2017,
abstract = {Context: This paper presents a novel experiment focused on detecting and analyzing clones in test suites written in TTCN-3, a standard telecommunication test script language, for different industrial projects. Objective: This paper investigates frequencies, types, and similarity distributions of TTCN-3 clones in test scripts from three industrial projects in telecommunication. We also compare the distribution of clones in TTCN-3 test scripts with the distribution of clones in C/C++ and Java projects from the telecommunication domain. We then perform a statistical analysis to validate the significance of differences between these distributions. Method: Similarity is computed using CLAN, which compares metrics syntactically derived from script fragments. Metrics are computed from the Abstract Syntax Trees produced by a TTCN-3 parser called Titan developed by Ericsson as an Eclipse plugin. Finally, clone classification of similar script pairs is computed using the Longest Common Subsequence algorithm on token types and token images. Results: This paper presents figures and diagrams reporting TTCN-3 clone frequencies, types, and similarity distributions. We show that the differences between the distribution of clones in test scripts and the distribution of clones in applications are statistically significant. We also present and discuss some lessons that can be learned about the transferability of technology from this study. Conclusion: About 24{\%} of fragments in the test suites are cloned, which is a very high proportion of clones compared to what is generally found in source code. The difference in proportion of Type-1 and Type-2 clones is statistically significant and remarkably higher in TTCN-3 than in source code. Type-1 and Type-2 clones represent 82.9{\%} and 15.3{\%} of clone fragments for a total of 98.2{\%}. Within the projects this study investigated, this represents more and easier potential re-factoring opportunities for test scripts than for code.},
author = {Lavoie, Thierry and M{\'{e}}rineau, Mathieu and Merlo, Ettore and Potvin, Pascal},
doi = {10.1016/j.infsof.2017.01.008},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Clone detection,Telecommunications software,Test},
month = {7},
pages = {32--45},
publisher = {Elsevier B.V.},
title = {{A case study of TTCN-3 test scripts clone analysis in an industrial telecommunication setting}},
volume = {87},
year = {2017}
}

@inproceedings{Levin2017,
author={S. Levin and A. Yehudai},
booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
title={The Co-evolution of Test Maintenance and Code Maintenance through the Lens of Fine-Grained Semantic Changes},
year=2017,
pages={35-46},
doi={10.1109/ICSME.2017.9},
month={9},
}


@inproceedings{Silva2017,
abstract = {Refactoring is a well-known technique that is widely adopted by software engineers to improve the design and enable the evolution of a system. Knowing which refactoring operations were applied in a code change is a valuable information to understand software evolution, adapt software components, merge code changes, and other applications. In this paper, we present RefDiff, an automated approach that identifies refactorings performed between two code revisions in a git repository. RefDiff employs a combination of heuristics based on static analysis and code similarity to detect 13 well-known refactoring types. In an evaluation using an oracle of 448 known refactoring operations, distributed across seven Java projects, our approach achieved precision of 100\% and recall of 88\%. Moreover, our evaluation suggests that RefDiff has superior precision and recall than existing state-of-The-Art approaches.},
author = {Silva, Danilo and Valente, Marco Tulio},
booktitle = {2017 IEEE/ACM 14th International Conference on Mining Software Repositories},
doi = {10.1109/MSR.2017.14},
eprint = {1704.01544},
isbn = {978-1-5386-1544-7},
issn = {21601860},
keywords = {Git,Software evolution,Software repositories,refactoring},
month = {5},
pages = {269--279},
publisher = {IEEE},
title = {{RefDiff: Detecting Refactorings in Version Histories}},
url = {http://ieeexplore.ieee.org/document/7962377/},
year = {2017}
}


@inproceedings{Alegroth2018,
abstract = {Continuous integration (CI) is growing in industrial popularity, spurred on by market trends towards faster delivery and higher quality software. A key facilitator of CI is automated testing that should be executed, automatically, on several levels of system abstraction. However, many systems lack the interfaces required for automated testing. Others lack test automation coverage of the system under test's (SUT) graphical user interface (GUI) as it is shown to the user. One technique that shows promise to solve these challenges is Visual GUI Testing (VGT), which uses image recognition to stimulate and assert the SUT's behavior. Research has presented the technique's applicability and feasibility in industry but only limited support, from an academic setting, that the technique is applicable in a CI environment. In this paper we presents a study from an industrial design research study with the objective to help bridge the gap in knowledge regarding VGT's applicability in a CI environment in industry. Results, acquired from interviews, observations and quantitative analysis of 17.567 test executions, collected over 16 weeks, show that VGT provides similar benefits to other automated test techniques for CI. However, several significant drawbacks, such as high costs, are also identified. The study concludes that, although VGT is applicable in an industrial CI environment, its severe challenges require more research and development before the technique becomes efficient in practice.},
author = {Alegroth, Emil and Karlsson, Arvid and Radway, Alexander},
booktitle = {2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)},
keywords = {continuous integration,design research,empirical,industrial study,visual gui testing},
month = {4},
pages = {172--181},
publisher = {IEEE},
title = {{Continuous Integration and Visual GUI Testing: Benefits and Drawbacks in Industrial Practice}},
year = {2018}
}

@inproceedings{Eladawy2018,
abstract = {The importance of test automation in the software industry has received a growing attention in recent years and it is continuously increasing. Unfortunately, the maintenance of the test cases is a major problem that faces those who use test automation. This problem becomes bigger when dealing with Graphical User Interface (GUI) tests.In this paper, an algorithm is introduced to maintain GUI tests for web applications. The Genetic algorithm is adopted to automatically repair the locators used to select elements from web pages. The algorithm is evaluated using several applications and results show that the proposed algorithm improves the repair percentage to 87\% of the used locators compared to the previous result which was 73\%.},
author = {Eladawy, Hadeel Mohamed and Mohamed, Amr E. and Salem, Sameh A.},
booktitle = {2018 13th International Conference on Computer Engineering and Systems (ICCES)},
keywords = {DOM selectors,GUI testing,XPath,XPath locator,genetic algorithm,web testing},
month = {12},
pages = {327--331},
publisher = {IEEE},
title = {{A New Algorithm for Repairing Web-Locators using Optimization Techniques}},
year = {2018}
}


@article{Garousi2018,
abstract = {As a type of anti-pattern, test smells are defined as poorly designed tests and their presence may negatively affect the quality of test suites and production code. Test smells are the subject of active discussions among practitioners and researchers, and various guidelines to handle smells are constantly offered for smell prevention, smell detection, and smell correction. Since there is a vast grey literature as well as a large body of research studies in this domain, it is not practical for practitioners and researchers to locate and synthesize such a large literature. Motivated by the above need and to find out what we, as the community, know about smells in test code, we conducted a ‘multivocal' literature mapping (classification) on both the scientific literature and also practitioners' grey literature. By surveying all the sources on test smells in both industry (120 sources) and academia (46 sources), 166 sources in total, our review presents the largest catalogue of test smells, along with the summary of guidelines/techniques and the tools to deal with those smells. This article aims to benefit the readers (both practitioners and researchers) by serving as an “index” to the vast body of knowledge in this important area, and by helping them develop high-quality test scripts, and minimize occurrences of test smells and their negative consequences in large test automation projects.},
author = {Garousi, Vahid and K{\"{u}}{\c{c}}{\"{u}}k, Barış},
doi = {10.1016/j.jss.2017.12.013},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Automated testing,Multivocal literature mapping,Software testing,Survey,Systematic mapping,Test anti-patterns,Test automation,Test scripts,Test smells},
month = {4},
pages = {52--81},
title = {{Smells in software test code: A survey of knowledge in industry and academia}},
volume = {138},
year = {2018}
}


@techreport{Katalon2018,
abstract = {Test automation is an essential part of modern software development lifecycles with Agile and DevOps. However, it accounts for a small percentage of test activities performed by software testing community. There are certainly challenges being faced by the community. So, understanding them is an important step to better adopt test automation in organizations. We carried out a study surveying over 2,000 software professionals about the challenges and problems faced in applying test automation. Of over 100 automation tools being used, open-source and free ones like Selenium and Katalon Studio are dominant. Functional and regression testing are the most common types adopting automation. The survey identifies the most striking challenges in applying test automation perceived by professionals with the top two being the frequently changing requirements and the lack of experienced automation resources. As implementing test automation strategies usually involves multiple tools, the difficulty in tool integration is also cited as a top challenge. The survey shows that the cost of commercial tools is the top concern when it comes to the challenges or problems with the existing automation tools. When the application under test changes, test scripts generated and maintained by tools are broken, which is the second highest challenge. The reliability of automation tools is not a key concern for many respondents.},
author = {Katalon},
number = {5},
pages = {16},
title = {{The most striking problems in test automation : A survey}},
year = {2018}
}

@article{Leotta2018,
author = {Leotta, Maurizio and Stocco, Andrea and Ricca, Filippo and Tonella, Paolo},
doi = {10.1002/stvr.1665},
journal = {Software Testing, Verification and Reliability},
keywords = {DOM-based testing,Selenium WebDriver,Sikuli,Web testing,test automation,visual testing},
month = {6},
number = {4},
pages = {30},
title = {Pesto: Automated migration of DOM-based Web tests towards the visual approach}},
volume = {28},
year = {2018}
}

@inproceedings{Raffaillac2018,
abstract = {This paper introduces a new GUI framework based on the Entity-Component-System model (ECS), where interactive elements (Enti-ties) can acquire any data (Components). Behaviors are managed by continuously running processes (Systems) which select entities by the components they possess. This model facilitates the handling and reuse of behaviors. It allows to define the interaction modalities of an application globally, by formulating them as a set of Systems. We present Polyphony, an experimental toolkit implementing this approach, detail our interpretation of the ECS model in the context of GUIs, and demonstrate its use with a sample application. CCS CONCEPTS • Human-centered computing → User interface programming; User interface toolkits;},
author = {Raffaillac, Thibault and Huot, St{\'{e}}phane},
booktitle = {Proceedings of the 30th on l'Interaction Homme-Machine},
isbn = {9781450360784},
keywords = {Human-centered computing, User interface, User interface toolkits},
pages = {42--51},
publisher = {ACM},
title = {{Application du mod{\`{e}}le Entit{\'{e}}-Composant-Syst{\`{e}}me {\`{a}} la programmation d'interactions Applying the Entity-Component-System Model to Interaction Programming}},
year = {2018}
}


@inproceedings{Palomba2018,
abstract = {Software testing is a key activity to control the reliability of production code. Unfortunately, the effectiveness of test cases can be threatened by the presence of faults. Recent work showed that static indicators can be exploited to identify test-related issues. In particular test smells, i.e., sub-optimal design choices applied by developers when implementing test cases, have been shown to be related to test case effectiveness. While some approaches for the automatic detection of test smells have been proposed so far, they generally suffer of poor performance: As a consequence, current detectors cannot properly provide support to developers when diagnosing the quality of test cases. In this paper, we aim at making a step ahead toward the automated detection of test smells by devising a novel textual-based detector, coined TASTE (Textual AnalySis for Test smEll detection), with the aim of evaluating the usefulness of textual analysis for detecting three test smell types, General Fixture, Eager Test, and Lack of Cohesion of Methods. We evaluate TASTE in an empirical study that involves a manually-built dataset composed of 494 test smell instances belonging to 12 software projects, comparing the capabilities of our detector with those of two code metrics-based techniques proposed by Van Rompaey et al. and Greiler et al. Our results show that the structural-based detection applied by existing approaches cannot identify most of the test smells in our dataset, while TASTE is up to 44{\%} more effective. Finally, we find that textual and structural approaches can identify different sets of test smells, thereby indicating complementarity.},
author = {Palomba, Fabio and Zaidman, Andy and {De Lucia}, Andrea},
booktitle = {2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
doi = {10.1109/ICSME.2018.00040},
isbn = {978-1-5386-7870-1},
keywords = {Empirical Studies,Mining Software Repositories,Test smells},
month = {sep},
pages = {311--322},
publisher = {IEEE},
title = {{Automatic Test Smell Detection Using Information Retrieval Techniques}},
year = {2018}
}


@inproceedings{Rwemalika2018,
abstract = {Agile methodologies enable companies to drastically increase software release pace and reduce time-to-market. In a rapidly changing environment, testing becomes a cornerstone of the software development process, guarding the system code base from the insertion of faults. To cater for this, many companies are migrating manual end-to-end tests to automated ones. This migration introduces several challenges to the practitioners. These challenges relate to difficulties in the creation of the automated tests, their maintenance and the evolution of the test code base. In this position paper, we discuss our preliminary results on such challenges and present two potential solutions to these problems, focusing on keyword-driven end-to-end tests. Our solutions leverage existing software artifacts, namely the test suite and an automatically-created model of the system under test, to support the evolution of keyword-driven test suites.},
author = {Rwemalika, Renaud and Kintis, Marinos and Papadakis, Mike and {Le Traon}, Yves},
booktitle = {CEUR Workshop Proceedings},
title = {{Can we automate away the main challenges of end-to-end testing?}},
volume = {2361},
year = {2018}
}



@inproceedings{Stocco2018,
abstract = {Web tests are prone to break frequently as the application under test evolves, causing much maintenance effort in practice. To detect the root causes of a test breakage, developers typically inspect the test's interactions with the application through the GUI. Existing automated test repair techniques focus instead on the code and entirely ignore visual aspects of the application. We propose a test repair technique that is informed by a visual analysis of the application. Our approach captures relevant visual information from tests execution and analyzes them through a fast image processing pipeline to visually validate test cases as they re-executed for regression purposes. Then, it reports the occurrences of breakages and potential fixes to the testers. Our approach is also equipped with a local crawling mechanism to handle non-trivial breakage scenarios such as the ones that require to repair the test's workflow. We implemented our approach in a tool called Vista. Our empirical evaluation on 2,672 test cases spanning 86 releases of four web applications shows that Vista is able to repair, on average, 81\% of the breakages, a 41\% increment with respect to existing techniques.},
address = {Lake Buena Vista, FL, USA},
author = {Stocco, Andrea and Yandrapally, Rahulkrishna and Mesbah, Ali},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
doi = {10.1145/3236024.3236063},
isbn = {9781450355735},
keywords = {computer vision,image analysis, test repair, web testing},
month = {10},
pages = {503--514},
publisher = {ACM},
title = {{Visual web test repair}},
year = {2018}
}


@inproceedings{Vidacs2018,
abstract = {Many modern software systems come with automated tests. While these tests help to maintain code quality by providing early feedback after modifications, they also need to be maintained. In this paper, we replicate a recent pattern mining experiment to find patterns on how production and test code co-evolve over time. Understanding co-evolution patterns may directly affect the quality of tests and thus the quality of the whole system. The analysis takes into account fine grained changes in both types of code. Since the full list of fine grained changes cannot be perceived, association rules are learned from the history to extract co-change patterns. We analyzed the occurrence of 6 patterns throughout almost 2500 versions of a Java system and found that patterns are present, but supported by weaker links than in previously reported. Hence we experimented with weighting methods and investigated the composition of commits.},
author = {Vidacs, Laszlo and Pinzger, Martin},
booktitle = {Proceedings of IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation},
doi = {10.1109/MALTESQUE.2018.8368456},
isbn = {978-1-5386-5920-5},
keywords = {change analysis,co-evolution patterns,machine learning,software evolution,testing},
month = {3},
pages = {31--36},
publisher = {IEEE},
title = {{Co-evolution analysis of production and test code by learning association rules of changes}},
year = {2018}
}




@inproceedings{Zheng2018,
abstract = {Due to the rapid iteration of Web applications, there are some broken test cases in regression tests. The main reason for the appearance of broken test cases is the failure of element location in the new web page. The element locators in the test cases come from various Web element locating tools, which are used to identify the elements to be convenient for testers to operate them and eventually to test the Web application. Therefore, the Web element locating tools play an essential role in web testing. At present, there are some Web element locating tools, which are supported by a single locating algorithm or multiple locating algorithms. Moreover, the Multi-Locators supported by multiple algorithms are obviously more robust than the one supported by a single algorithm. However, when synthesizing all locating algorithm to generate Multi-Locators, a better method can be selected in assigning weights to each algorithm. Based on this observation, we propose a method to optimize Multi-Locators. In assigning weight to each algorithm, it chooses a weight distribution method based on machine learning, named Learned Weights. Through experimental comparison, it is shown that the locating tool supported by algorithm based on machine learning is more robust than these existing locating tools.},
author = {Zheng, Yu and Huang, Song and Hui, Zhan-wei and Wu, Ya-Ning},
booktitle = {2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
keywords = {Learned Weights,Web Element Locating Tool,Web Locator,Web Testing},
month = {7},
pages = {172--174},
publisher = {IEEE},
title = {{A Method of Optimizing Multi-Locators Based on Machine Learning}},
year = {2018}
}

@article{Alenezi2019,
abstract = {The software system evolves and changes with the time, so the test suite must be maintained according to code changes. Maintaining test cases manually is an expensive and time-consuming activity, especially for large test suites, which has motivated the recent development of automated test-repair techniques. Several researchers indicate that software evolution shows a direct impact on test suites evolution, as they have strong relationships and they should be evolved concurrently. This article aims to provide statistical evidence of having this significant relationship between the code production and its associated test suites. Seven systems along with releases are collected and eight metrics were calculated to be used in this study. The result shows how the systems under study are evolving and have a high impact on their test suites, although two metrics provide a negative significant relationship.},
author = {Alenezi, Mamdouh and Akour, Mohammed and {Al Sghaier}, Hiba},
doi = {10.35940/ijitee.A4967.119119},
issn = {22783075},
journal = {International Journal of Innovative Technology and Exploring Engineering},
keywords = {Complexity,Size,Software evolution,Software testing},
number = {1},
pages = {2737--2739},
title = {{The impact of co-evolution of code production and test suites through software releases in open source software systems}},
volume = {9},
year = {2019}
}


@inproceedings{Biagiola2019,
abstract = {Existing web test generators derive test paths from a navigational model of the web application, completed with either manually or randomly generated input values. However, manual test data selec- tion is costly, while random generation often results in infeasible input sequences, which are rejected by the application under test. Random and search-based generation can achieve the desired level of model coverage only after a large number of test execution at- tempts, each slowed down by the need to interact with the browser during test execution. In this work, we present a novel web test generation algorithm that pre-selects the most promising candi- date test cases based on their diversity from previously generated tests. As such, only the test cases that explore diverse behaviours of the application are considered for in-browser execution. We have implemented our approach in a tool called DIG. Our empirical eval- uation on six real-world web applications shows that DIG achieves higher coverage and fault detection rates significantly earlier than crawling-based and search-based web test generators.},
address = {Tallinn, Estonia},
author = {Biagiola, Matteo and Stocco, Andrea and Ricca, Filippo and Tonella, Paolo},
booktitle = {Proceedings of the 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
doi = {10.1145/3338906.3338970},
isbn = {9781450355728},
keywords = {diversity,page object,test generation,web testing},
number = {1},
pages = {142--153},
publisher = {ACM Press},
title = {{Diversity-based web test generation}},
year = {2019}
}

@incollection{Canny2019,
abstract = {Testing interactive systems is known to be a complex task that cannot be exhaustive. Indeed, the infinite number of combination of user input and the complexity of information presentation exceed the practical limits of exhaustive and analytical approach to testing [31]. Most interactive software testing techniques are produced by applying and tuning techniques from the field of software testing to try to address the specificities of interactive applications. When some elements cannot be taken into account by the software testing technique, they are usually ignored. In this paper we propose to follow an opposite approach, starting from a generic architecture for interactive systems (including both software and hardware elements) for identifying in a systematic way, testing problems and testing needs. This architecture-driven approach makes it possible to identify how software testing knowledge and techniques can support interactive systems testing but also where the interactive systems engineering community should invest in order to test their idiosyncrasies too.},
author = {Canny, Alexandre and Bouzekri, Elodie and Martinie, C{\'{e}}lia and Palanque, Philippe},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-05909-5_10},
isbn = {9783030059088},
issn = {16113349},
keywords = {Architecture-driven testing,Interactive system testing},
pages = {164--186},
title = {{Rationalizing the Need of Architecture-Driven Testing of Interactive Systems}},
volume = {11262 LNCS},
year = {2019}
}

@article{Coppola2019,
abstract = {Android applications do not seem to be tested as thoroughly as desktop ones. In particular, graphical user interface (GUI) testing appears generally limited. Like web-based applications, mobile apps suffer from GUI test fragility, i.e., GUI test classes failing or needing updates due to even minor modifications in the GUI or in the application under test. The objective of our study is to estimate the adoption of GUI testing frameworks among Android open-source applications, the quantity of modifications needed to keep test classes up to date, and their amount due to GUI test fragility. We introduce a set of 21 metrics to measure the adoption of testing tools and the evolution of test classes and test methods, and to estimate the fragility of test suites. We computed our metrics for six GUI testing frameworks, none of which achieved a significant adoption among Android projects hosted on GitHub. When present, GUI test methods associated with the considered tools are modified often, and a relevant portion (70\% on average) of those modifications is induced by GUI-related fragilities. On average, for the projects considered, more than 7\% of the total modified lines of code between consecutive releases belong to test classes developed with the analyzed testing frameworks. The measured percentage was higher on average than the one required by other generic test code, based on the JUnit testing framework. Fragility of GUI tests constitutes a relevant concern, probably an obstacle for developers to adopt test automation. This first evaluation of the fragility of Android scripted GUI testing can constitute a benchmark for developers and testers leveraging the analyzed test tools and the basis for the definition of a taxonomy of fragility causes and guidelines to mitigate the issue.},
author = {Coppola, Riccardo and Morisio, Maurizio and Torchiano, Marco},
doi = {10.1109/TR.2018.2869227},
issn = {0018-9529},
journal = {IEEE Transactions on Reliability},
keywords = {Mobile computing,software engineering,software maintenance,software metrics,software testing},
month = {3},
number = {1},
pages = {67--90},
publisher = {IEEE},
title = {{Mobile GUI Testing Fragility: A Study on Open-Source Android Applications}},
volume = {68},
year = {2019}
}

@article{Coppola2019b,
abstract = {Evidence from empirical studies suggests that mobile applications are not thoroughly tested as their desktop counterparts. In particular, GUI testing is generally limited. Like web-based applications, mobile apps suffer from GUI testing fragility, i.e., GUI test classes failing or needing interventions because of modifications in the AUT or in its GUI arrangement and definition. The objective of our study is to examine the diffusion of test classes created with a set of popular GUI Automation Frameworks for Android apps, the amount of changes required to keep test classes up to date, and the amount of code churn in existing test suites, along with the underlying modifications in the AUT that caused such modifications. We defined 12 metrics to characterize the evolution of test classes and test methods, and a taxonomy of 28 possible causes for changes to test code. To perform our experiments, we selected six widely used open-source GUI Automation Frameworks for Android apps. We evaluated the diffusion of the tools by mining the GitHub repositories featuring them, and computed our set of metrics on the projects. Applying the Grounded Theory technique, we then manually analyzed diff files of test classes written with the selected tools, to build from the ground up a taxonomy of causes for modifications of test code. We found that none of the considered GUI automation frameworks achieved a major diffusion among open-source Android projects available on GitHub. For projects featuring tests created with the selected frameworks, we found that test suites had to be modified often – specifically, about 8\% of developers' modified LOCs belonged to test code and that a relevant portion (around 50\% on average) of those modifications were induced by modifications in GUI definition and arrangement. Test code written with GUI automation fromeworks proved to need significant interventions during the lifespan of a typical Android open-source project. This can be seen as an obstacle for developers to adopt this kind of test automation. The evaluations and measurements of the maintainance needed by test code wrtitten with GUI automation frameworks, and the taxonomy of modification causes, can serve as a benchmark for developers, and the basis for the formulation of actionable guidelines and the development of automated tools to help mitigating the issue.},
author = {Coppola, Riccardo and Morisio, Maurizio and Torchiano, Marco and Ardito, Luca},
doi = {10.1007/s10664-019-09722-9},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Automated software testing,GUI testing,Mobile development,Software evolution,Software maintenance},
mendeley-groups = {Software Testing/Maintenance},
month = {10},
number = {5},
pages = {3205--3248},
title = {{Scripted GUI testing of Android open-source apps: evolution of test code and fragility causes}},
volume = {24},
year = {2019}
}


@inproceedings{Jimenez2019,
abstract = {Previous work on vulnerability prediction assume that predictive models are trained with respect to perfect labelling information (includes labels from future, as yet undiscovered vulnerabilities). In this paper we present results from a comprehensive empirical study of 1,898 real-world vulnerabilities reported in 74 releases of three security-critical open source systems (Linux Kernel, OpenSSL and Wiresark). Our study investigates the effectiveness of three previously proposed vulnerability prediction approaches, in two settings: with and without the unrealistic labelling assumption. The results reveal that the unrealistic labelling assumption can profoundly mis- lead the scientific conclusions drawn; suggesting highly effective and deployable prediction results vanish when we fully account for realistically available labelling in the experimental methodology. More precisely, MCC mean values of predictive effectiveness drop from 0.77, 0.65 and 0.43 to 0.08, 0.22, 0.10 for Linux Kernel, OpenSSL and Wiresark, respectively. Similar results are also obtained for precision, recall and other assessments of predictive efficacy. The community therefore needs to upgrade experimental and empirical methodology for vulnerability prediction evaluation and development to ensure robust and actionable scientific findings.},
address = {New York, NY, USA},
author = {Jimenez, Matthieu and Rwemalika, Renaud and Papadakis, Mike and Sarro, Federica and {Le Traon}, Yves and Harman, Mark},
booktitle = {Proceedings of the 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Machine Learning,Prediction Modelling,Software Vulnerabilities},
month = {8},
pages = {695--705},
publisher = {ACM},
title = {{The importance of accounting for real-world labelling when predicting software vulnerabilities}},
year = {2019}
}




@inproceedings{Kirinuki2019,
abstract = {Test automation tools such as Selenium are commonly used for automating end-to-end tests, but when developers update the software, they often need to modify the test scripts accordingly. However, the costs of modifying these test scripts are a big obstacle to test automation because of the scripts' fragility. In particular, locators in test scripts are prone to change. Some prior methods tried to repair broken locators by using structural clues, but these approaches usually cannot handle radical changes to page layouts. In this paper, we propose a novel approach called COLOR (correct locator recommender) to support repairing broken locators in accordance with software updates. COLOR uses various properties as clues obtained from screens (i.e., attributes, texts, images, and positions). We examined which properties are reliable for recommending locators by examining changes between two release versions of software, and the reliability is adopted as the weight of a property. Our experimental results obtained from four open source web applications show that COLOR can present the correct locator in rst place with a 77{\%}-93{\%} accuracy and is more robust against page layout changes than structure-based approaches.},
author = {Kirinuki, Hiroyuki and Tanno, Haruto and Natsukawa, Katsuyuki},
booktitle = {2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
month = {2},
number = {4},
pages = {310--320},
publisher = {IEEE},
title = {{COLOR: Correct Locator Recommender for Broken Test Scripts using Various Clues in Web Application}},
volume = {36},
year = {2019}
}

@inproceedings{Li2019,
abstract = {Repairing broken tests in evolving software systems is an expensive and challenging task. One of the main challenges for test repair, in particular, is preserving the intent of the original tests in the repaired ones. To address this challenge, we propose a technique for test repair that models and considers the intent of a test when repairing it. Our technique first uses a search-based approach to generate repair candidates for the broken test. It then computes, for each candidate, its likelihood of preserving the original test intent. To do so, the technique characterizes such intent using the path conditions generated during a dynamic symbolic execution of the tests. Finally, the technique reports the best candidates to the developer as repair recommendations. We implemented and evaluated our technique on a benchmark of 91 broken tests in 4 open-source programs. Our results are promising, in that the technique was able to generate intentpreserving repair candidates for over 79\% of those broken tests and rank the intent-preserving candidates as the first choice of repair recommendations for almost 70\% of the broken tests.},
author = {Li, Xiangyu and D'Amorim, Marcelo and Orso, Alessandro},
booktitle = {2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)},
doi = {10.1109/ICST.2019.00030},
isbn = {978-1-7281-1736-2},
keywords = {Search-based software engineering,Software testing,Test case repair,Test intent characterization},
month = {4},
pages = {217--227},
publisher = {IEEE},
title = {{Intent-Preserving Test Repair}},
year = {2019}
}


@inproceedings{Rwemalika2019,
abstract = {Many companies rely on software testing to verify that their software products meet their requirements. However, test quality and, in particular, the quality of end-to-end testing is relatively hard to achieve. The problem becomes challenging when software evolves, as end-to-end test suites need to adapt and conform to the evolved software. Unfortunately, end-to-end tests are particularly fragile as any change in the application interface, e.g., application flow, location or name of graphical user interface elements, necessitates a change in the tests. This paper presents an industrial case study on the evolution of Keyword-Driven test suites, also known as Keyword-Driven Testing (KDT). Our aim is to demonstrate the problem of test maintenance, identify the benefits of Keyword-Driven Testing and overall improve the understanding of test code evolution (at the acceptance testing level). This information will support the development of automatic techniques, such as test refactoring and repair, and will motivate future research. To this end, we identify, collect and analyze test code changes across the evolution of industrial KDT test suites for a period of eight months. We show that the problem of test maintenance is largely due to test fragility (most commonly-performed changes are due to locator and synchronization issues) and test clones (over 30{\%} of keywords are duplicated). We also show that the better test design of KDT test suites has the potential for drastically reducing (approximately 70{\%}) the number of test code changes required to support software evolution. To further validate our results, we interview testers from BGL BNP Paribas and report their perceptions on the advantages and challenges of keyword-driven testing.},
address = {Xi'an, China},
author = {Rwemalika, Renaud and Kintis, Marinos and Papadakis, Mike and {Le Traon}, Yves and Lorrach, Pierre},
booktitle = {Proceedings of the 12th IEEE Conference on Software Testing, Validation and Verification},
keywords = {acceptance testing,end-to-end testing,keyword-driven testing,test clone,test code evolution},
month = {4},
pages = {335--345},
publisher = {IEEE},
title = {{On the Evolution of Keyword-Driven Test Suites}},
year = {2019}
}


@inproceedings{Rwemalika2019b,
author = {Rwemalika, Renaud and Kintis, Marinos and Papadakis, Mike and Le Traon, Yves and Lorrach, Pierre},
title = {Ukwikora: Continuous Inspection for Keyword-Driven Testing},
year = {2019},
publisher = {Association for Computing Machinery},
address = {Beijing, China},
abstract = {Automation of acceptance test suites becomes necessary in the context of agile software development practices, which require rapid feedback on the quality of code changes. To this end, companies try to automate their acceptance tests as much as possible. Unfortunately, the growth of the automated test suites, by several automation testers, gives rise to potential test smells, i.e., poorly designed test code, being introduced in the test code base, which in turn may increase the cost of maintaining the code and creating new one. In this paper, we investigate this problem in the context of our industrial partner, BGL BNP Paribas, and introduce Ukwikora, an automated tool that statically analyzes acceptance test suites, enabling the continuous inspection of the test code base. Ukwikora targets code written in the Robot Framework syntax, a popular framework for writing Keyword-Driven tests. Ukwikora has been successfully deployed at BGL BNP Paribas, detecting issues otherwise unknown to the automation testers, such as the presence of duplicated test code, dead test code and dependency issues among the tests. The success of our case study reinforces the need for additional research and tooling for acceptance test suites.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {402–405},
numpages = {4},
keywords = {Keyword-DrivenTesting, Clones, Test Smell, Continuous Inspection},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{Rwemalika2019c,
abstract = {Software bugs constitute a frequent and common issue of software development. To deal with this problem, modern software development methodologies introduce dedicated quality assurance procedures. At the same time researchers aim at developing techniques capable of supporting the early discovery and fix of bugs. One important factor that guides such research attempts is the characteristics of software bugs and bug fixes. In this paper, we present an industrial study on the characteristics and differences between pre-release bugs, i.e. bugs detected during software development, and post-release bugs, i.e. bugs that escaped to production. Understanding such differences is of paramount importance as it will improve our understanding on the testing and debugging support that practitioners require from the research community, on the validity of the assumptions of several research techniques, and, most importantly, on the reasons why bugs escape to production. To this end, we analyze 37 industrial projects from BGL BNP Paribas and document the differences between pre-release bugs and post-release bugs. Our findings suggest that post-release bugs are more complex to fix, requiring developers to modify several source code files, written in different programming languages, and configuration files, as well. We also find that approximately 82\% of the post-release bugs involve code additions and can be characterized as 'omission' bugs. Finally, we conclude the paper with a discussion on the implications of our study and provide guidance to future research directions.},
address = {Cleveland, USA},
author = {Rwemalika, Renaud and Kintis, Marinos and Papadakis, Mike and {Le Traon}, Yves and Lorrach, Pierre},
booktitle = {Proceedings of the 35th IEEE International Conference on Software Maintenance and Evolution (ICSME)},
keywords = {Empirical study,bug fix,industrial study,post release,pre release},
month = {9},
pages = {92--102},
publisher = {IEEE},
title = {{An Industrial Study on the Differences between Pre-Release and Post-Release Bugs}},
year = {2019}
}



@inproceedings{Canny2020,
abstract = {The testing of applications with a Graphical User Interface (GUI) is a complex activity because of the infinity of possible event sequences. In the field of GUI Testing, model-based approaches based on reverse engineering of GUI application have been proposed to generate test cases. Unfortunately, evidences show that these techniques do not support some of the features of modern GUI applications. These features include dynamic widgets instantiation or advanced interaction techniques (e.g. multitouch). In this paper, we propose to build models of the applications from requirements, as it is standard practice in Model-Based Testing. To do so, we identified ICO (Interactive Cooperative Object) as one of the modelling techniques allowing the description of complex GUI behavior. We demonstrate that this notation is suitable for generating test cases targeting complex GUI applications in a process derived from the standard ModelBased Testing process.},
author = {Canny, Alexandre and Palanque, Philippe and Navarre, David},
booktitle = {2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
doi = {10.1109/ICSTW50294.2020.00029},
isbn = {978-1-7281-1075-2},
keywords = {GUI Testing,Model-Based Testing,User Interface Description Languages},
month = {10},
pages = {95--104},
publisher = {IEEE},
title = {{Model-Based Testing of GUI Applications Featuring Dynamic Instanciation of Widgets}},
year = {2020}
}

@article{Chen2020,
abstract = {In crowdsourced mobile application testing, crowd workers perform test tasks for developers and submit test reports to report the observed abnormal behaviors. These test reports usually provide important information to improve the quality of software. However, due to the poor expertise of workers and the inconvenience of editing on mobile devices, some test reports usually lack necessary information for understanding and reproducing the revealed bugs. Sometimes developers have to spend a significant part of available resources to handle the low-quality test reports, thus severely reducing the inspection efficiency. In this paper, to help developers determine whether a test report should be selected for inspection within limited resources, we issue a new problem of test report quality assessment. Aiming to model the quality of test reports, we propose a new framework named TERQAF. First, we systematically summarize some desirable properties to characterize expected test reports and define a set of measurable indicators to quantify these properties. Then, we determine the numerical values of indicators according to the contained contents of test reports. Finally, we train a classifier by using logistic regression to predict the quality of test reports. To validate the effectiveness of TERQAF, we conduct extensive experiments over five crowdsourced test report datasets. Experimental results show that TERQAF can achieve 85.18\% in terms of Macro-average Precision (MacroP), 75.87\% in terms of Macro-average Recall (MacroR), and 80.01\% in terms of Macro-average F-measure (MacroF) on average in test report quality assessment. Meanwhile, the empirical results also demonstrate that test report quality assessment can help developers handle test reports more efficiently.},
author = {Chen, Xin and Jiang, He and Li, Xiaochen and Nie, Liming and Yu, Dongjin and He, Tieke and Chen, Zhenyu},
doi = {10.1007/s10664-019-09793-8},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Crowdsourced testing,Desirable properties,Natural language processing,Quality indicators,Test report quality},
month = {3},
number = {2},
pages = {1382--1418},
publisher = {Empirical Software Engineering},
title = {{A systemic framework for crowdsourced test report quality assessment}},
volume = {25},
year = {2020}
}


@inproceedings{Ghamizi2020,
abstract = {The rapid spread of the Coronavirus SARS-2 is a major challenge that led almost all governments worldwide to take drastic measures to respond to the tragedy. Chief among those measures is the massive lockdown of entire countries and cities, which beyond its global economic impact has created some deep social and psychological tensions within populations. While the adopted mitigation measures (including the lockdown) have generally proven useful, policymakers are now facing a critical question: how and when to lift the mitigation measures? A carefully-planned exit strategy is indeed necessary to recover from the pandemic without risking a new outbreak. Classically, exit strategies rely on mathematical modeling to predict the effect of public health interventions. Such models are unfortunately known to be sensitive to some key parameters, which are usually set based on rules-of-thumb. In this paper, we propose to augment epidemiological forecasting with actual data-driven models that will learn to fine-tune predictions for different contexts (e.g., per country). We have therefore built a pandemic simulation and forecasting toolkit that combines a deep learning estimation of the epidemiological parameters of the disease in order to predict the cases and deaths, and a genetic algorithm component searching for optimal trade-offs/policies between constraints and objectives set by decision-makers. Replaying pandemic evolution in various countries, we experimentally show that our approach yields predictions with much lower error rates than pure epidemiological models in 75\% of the cases and achieves a 95\% R2 score when the learning is transferred and tested on unseen countries. When used for forecasting, this approach provides actionable insights into the impact of individual measures and strategies.},
address = {New York, NY, USA},
author = {Ghamizi, Salah and Rwemalika, Renaud and Cordy, Maxime and Veiber, Lisa and Bissyand{\'{e}}, Tegawend{\'{e}} F. and Papadakis, Mike and Klein, Jacques and {Le Traon}, Yves},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
keywords = {covid19,deep learning,exit strategies,pandemic,prediction,search-based optimization,seir},
month = {8},
pages = {3434--3442},
publisher = {ACM},
title = {{Data-driven Simulation and Optimization for Covid-19 Exit Strategies}},
year = {2020}
}



@inproceedings{Kim2020,
abstract = {Test smell as analogous to code smell is a poor design choice in the implementation of test code. Recently, the concept of test smell has become the utmost interest of researchers and practitioners. Surveys show that developers' are aware of test smells and their potential consequences in the software system. However, there is limited empirical evidence for how developers address test smells during software evolution. Thus, in this paper, we study 2 research questions: (RQ1) How do test smells evolve? (RQ2) What is the motivation for removing test smells? Our result shows that Assertion Roulette, Conditional Test Logic and Unknown tests have a high rate of churns, the feature addition and improvement motivate refactoring, but test smell persists, implicating sub-optimal practice. In our study, we hope to fill the gap between academia and industry by providing evidence of sub-optimal practice in the way developers address test smells, and how it may be detrimental to the software.},
address = {New York, NY, USA},
author = {Kim, Dong Jae},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
doi = {10.1145/3377812.3382176},
isbn = {9781450371223},
issn = {02705257},
keywords = {Evolution,Refactoring,Software Quality,Test Smell},
month = {6},
number = {i},
pages = {149--151},
publisher = {ACM},
title = {{An empirical study on the evolution of test smell}},
year = {2020}
}

@inproceedings{Long2020,
abstract = {Record-and-replay tools are important for quality assurance of Web applications by capturing user case scenarios and executing them automatically when needed. However, the tests generated by existing techniques are brittle, and often lead to test breakages as the dynamic behavior and frequent updates of modern Web applications. In this paper, we propose WebRR, a self-replay enhanced robust record-and-replay technique for Web applications testing. The novelty of WebRR is that, it introduces a new self-replay mechanism in the recording phase, which checks the captured event from the record module online, and generates multiple locators (including DOM locators, visual locator and proximity locators) automatically, to improve the robustness of generated test cases. During the replay, it combines multiple locators and new local workflow repair technique to repair test breakages, and can improve the resilience of generated tests to frequent updates of the applications. We applied our approach to 3 enterprise Web applications, which are deployed in a large power grid company of China. The experimental results show that WebRR is effective, and substantially improve the robustness of end-to-end web tests that are generated using record-and-replay technique.},
address = {New York, NY, USA},
author = {Long, Zhenyue and Wu, Guoquan and Chen, Xiaojiang and Chen, Wei and Wei, Jun},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
doi = {10.1145/3368089.3417069},
isbn = {9781450370431},
keywords = {End-to-end test,Record-and-replay,Web application},
month = {11},
pages = {1498--1508},
publisher = {ACM},
title = {{WebRR: self-replay enhanced robust record/replay for web application testing}},
year = {2020}
}


@article{Pan2020,
abstract = {Graphical User Interface (GUI) testing is widely used to test mobile apps. As mobile apps are frequently updated and need repeated testing, to reduce the test cost, their test cases are often coded as scripts to enable automated execution using test harnesses/tools. When those mobile apps evolve, many of the test scripts, however, may become broken due to changes made to the app GUIs. While it is desirable that the broken scripts get repaired, doing it manually can be preventively expensive if the number of tests need repairing is large. We propose in this paper a novel approach named METER to repairing broken GUI test scripts automatically when mobile apps evolve. METER leverages computer vision techniques to infer GUI changes between two versions of a mobile app and uses the inferred changes to guide the repair of GUI test scripts. Since METER only relies on screenshots to repair GUI tests, it is applicable to apps targeting open or closed source mobile platforms. In experiments conducted on 22 Android apps and 6 iOS apps, repairs produced by METER helped preserve 63.7\% and 38.8\% of all the test actions broken by the GUI changes, respectively},
author = {Pan, Minxue and Xu, Tongtong and Pei, Yu and Li, Zhong and Zhang, Tian and Li, Xuandong},
doi = {10.1109/TSE.2020.3007664},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {c},
pages = {1--20},
title = {{GUI-Guided Test Script Repair for Mobile Apps}},
volume = {5589},
year = {2020}
}



@inproceedings{Peruma2020,
abstract = {The test code, just like production source code, is subject to bad design and programming practices, also known as smells. The presence of test smells in a software project may affect the quality, maintainability, and extendability of test suites making them less effective in finding potential faults and quality issues in the project's production code. In this paper, we introduce tsDetect, an automated test smell detection tool for Java software systems that uses a set of detection rules to locate existing test smells in test code. We evaluate the effectiveness of tsDetect on a benchmark of 65 unit test files containing instances of 19 test smell types. Results show that tsDetect achieves a high detection accuracy with an average precision score of 96\% and an average recall score of 97\%. tsDetect is publicly available, with a demo video, at: https://testsmells.github.io/},
address = {New York, NY, USA},
author = {Peruma, Anthony and Almalki, Khalid and Newman, Christian D. and Mkaouer, Mohamed Wiem and Ouni, Ali and Palomba, Fabio},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
doi = {10.1145/3368089.3417921},
isbn = {9781450370431},
keywords = {Detection Tool,Software Quality,Test Smells},
month = {11},
pages = {1650--1654},
publisher = {ACM},
title = {{tsDetect: an open-source test smells detection tool}},
year = {2020}
}

@article{DiMartino2021,
abstract = {Exploratory testing and fully automated testing tools represent two viable and cheap alternatives to traditional test-case-based approaches for graphical user interface (GUI) testing of Android apps. The former can be executed by capture and replay tools that directly translate execution scenarios registered by testers in test cases, without requiring preliminary test-case design and advanced programming/testing skills. The latter tools are able to test Android GUIs without tester intervention. Even if these two strategies are widely employed, to the best of our knowledge, no empirical investigation has been performed to compare their performance and obtain useful insights for a project manager to establish an effective testing strategy. In this paper, we present two experiments we carried out to compare the effectiveness of exploratory testing approaches using a capture and replay tool (Robotium Recorder) against three freely available automatic testing tools (AndroidRipper, Sapienz, and Google Robo). The first experiment involved 20 computer engineering students who were asked to record testing executions, under strict temporal limits and no access to the source code. Results were slightly better than those of fully automated tools, but not in a conclusive way. In the second experiment, the same students were asked to improve the achieved testing coverage by exploiting the source code and the coverage obtained in the previous tests, without strict temporal constraints. The results of this second experiment showed that students outperformed the automated tools especially for long/complex execution scenarios. The obtained findings provide useful indications for deciding testing strategies that combine manual exploratory testing and automated testing.},
author = {{Di Martino}, Sergio and Fasolino, Anna Rita and Starace, Luigi Libero Lucio and Tramontana, Porfirio},
doi = {10.1002/stvr.1754},
issn = {0960-0833},
journal = {Software Testing, Verification and Reliability},
keywords = {Android app testing,GUI testing,automatic input generation,capture and replay,testing effectiveness},
month = {5},
number = {3},
title = {{Comparing the effectiveness of capture and replay against automatic input generation for Android graphical user interface testing}},
volume = {31},
year = {2021}
}


@inproceedings{Ricca2021,
abstract = {This paper provides the results of a survey of the grey lit- erature concerning best practices for end-to-end web test automation. We analyzed more than 2,400 sources (e.g., blog posts, white-papers, user manuals, GitHub repositories) looking for guidelines by IT profes- sionals on how to develop and maintain web test code. Ultimately, we filtered 142 relevant documents from which we extracted a taxonomy of guidelines divided into technical tips (i.e., concerning the development, maintenance, and execution of web tests), and business-level tips (i.e, concerning the planning and management of testing teams, design, and process). The paper concludes with distilling the ten most cited best practices for developing good quality automated web tests.},
address = {Bolzano, Italy},
author = {Ricca, Filippo and Stocco, Andrea},
booktitle = {Proceedings of 47th International Conference on Current Trends in Theory and Practice of Computer Science 2021},
doi = {10.1007/978-3-030-67731-2_35},
isbn = {9783030677305},
issn = {16113349},
keywords = {Best Practices,Grey Literature,Web Test Automation},
pages = {472--485},
title = {{Web Test Automation: Insights from the Grey Literature}},
year = {2021}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ONLINE RESOURCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@online{W3C2004,
url={https://www.w3.org/TR/DOM-Level-3-Core/introduction.html},
title={What is the Document Object Model?},
author={World Wide Web Consortium},
year={2004},
month={4},
addendum = {(accessed: 15.01.2021)}
}

@online{Jain2007,
  author = {Naresh Jain},
  title = {Patterns and Anti-Patterns: Acceptance Testing with FitNesse},
  year = 2007,
  url = {https://blogs.agilefaqs.com/2007/08/25/patterns-and-anti-patterns-acceptance-testing-with-fitnesse/},
  urldate = {2020-07-02}
}

@online{Archer2010,
  author = {Matt Archer},
  title = {How test automation with Selenium can fail},
  year = 2010,
  url = {https://mattarcherblog.wordpress.com/2010/11/29/how-test-automation-with-selenium-or-watir-can-fail/},
  urldate = {2020-07-02}
}

@online{Evangelisti2012,
  author = {Augusto Evangelisti},
  title = {How to transform bad Acceptance tests into Awesome ones},
  year = 2012,
  url = {https://mysoftwarequality.wordpress.com/2012/12/14/how-to-transform-bad-acceptance-tests-into-awesome-ones/},
  urldate = {2020-07-02}
}

@online{Clayton2014,
  author = {Josh Clayton},
  title = {Acceptance Tests at a Single Level of Abstraction},
  year = 2014,
  url = {https://thoughtbot.com/blog/acceptance-tests-at-a-single-level-of-abstraction},
  urldate = {2020-07-02}
}

@online{Kirkbride2014,
  author = {Jim Kirkbride},
  title = {Testing Anti-Patterns},
  year = 2014,
  url = {https://medium.com/jameskbride/testing-anti-patterns-b5ffc1612b8b},
  urldate = {2020-07-02}
}

@online{Klarck2014,
  author = {Pekka Klarck},
  title = {Robot Framework Dos And Don'ts},
  year = 2014,
  url = {https://slideshare.net/pekkaklarck/robot-framework-dos-and-donts},
  urldate = {2020-07-02}
}

@online{W3C2014,
url={https://www.w3.org/WAI/GL/wiki/Using_ARIA_landmarks_to_identify_regions_of_a_page},
title={Using ARIA landmarks to identify regions of a page},
author={World Wide Web Consortium},
year={2014},
month={1},
urldate={2021-01-16}
}

@online{Buwalda2015,
  author = {Hans Buwalda},
  title = {Test Design for Automation: Anti-Patterns},
  year = 2015,
  url = {https://www.techwell.com/techwell-insights/2015/09/test-design-automation-anti-patterns},
  urldate = {2020-07-02}
}

@online{Scott2015,
  author = {Alister Scott},
  title = {Five automated acceptance test anti-patterns},
  year = 2015,
  url = {https://alisterbscott.com/2015/01/20/five-automated-acceptance-test-anti-patterns/},
  urldate = {2020-07-02}
}

@online{England2016,
  author = {Theo England},
  title = {Cucumber anti-patterns (part one)},
  year = 2016,
  url = {https://cucumber.io/blog/bdd/cucumber-antipatterns-part-one/},
  urldate = {2020-07-03}
}

@online{Gawinecki2016,
  author = {Maciej Gawinecki},
  title = {Anti-patterns in test automation},
  year = 2016,
  url = {http://nomoretesting.com/blog/2016/05/23/anti-patterns/},
  urldate = {2020-07-02}
}

@online{Renaudin2016,
  author = {Josiah Renaudin},
  title = {Software Testing Anti Patterns},
  year = 2016,
  url = {https://www.slideshare.net/JosiahRenaudin/antipatterns-for-automated-testing},
  urldate = {2020-07-03}
}

@online{Advolodkin2018,
  author = {Nikolay Advolodkin},
  title = {Top 17 Automated Testing Best Practices (Supported By Data)},
  year = 2018,
  url = {https://ultimateqa.com/automation-patterns-antipatterns/},
  urldate = {2020-07-02}
}

@online{W3C2018,
url={https://drafts.csswg.org/selectors-3/},
title={Selectors Level 3 - W3C Candidate Recommendation},
author={World Wide Web Consortium},
year={2018},
month={1},
urldate={2021-01-21}
}

@online{Dharmender2017,
  author = {Kumar Dharmender},
  title = {Automation Testing: Anti-patterns},
  year = 2017,
  url = {https://alisterbscott.com/2015/01/20/five-automated-acceptance-test-anti-patterns/},
  urldate = {2020-07-02}
}

@online{Knight2017a,
  author = {Andrew Knight},
  title = {BDD 101: WRITING GOOD GHERKIN},
  year = 2017,
  url = {https://automationpanda.com/2017/01/30/bdd-101-writing-good-gherkin/},
  urldate = {2020-07-02}
}

@online{Knight2017b,
  author = {Andrew Knight},
  title = {SHOULD GHERKIN STEPS USE FIRST-PERSON OR THIRD-PERSON?},
  year = 2017,
  url = {https://automationpanda.com/2017/01/18/should-gherkin-steps-use-first-person-or-third-person/},
  urldate = {2020-07-02}
}

@online{StackExchange2017,
  author       = {StackExchange},
  title        = {What are anti-patterns in test automation?},
  year         = {2017},
  url          = {https://sqa.stackexchange.com/questions/8508/what-are-anti-patterns-in-test-automation},
  urldate      = {2021-04-06}
}

@online{Cripsin2018,
  author = {Lisa Cripsin},
  title = {Keep Your Automated Testing Simple and Avoid Anti-Patterns},
  year = 2018,
  url = {https://www.mabl.com/blog/keep-your-automated-testing-simple},
  urldate = {2020-07-02}
}

@online{Kapelonis2018,
  author = {Kostis Kapelonis},
  title = {Software Testing Anti-patterns},
  year = 2018,
  url = {http://blog.codepipes.com/testing/software-testing-antipatterns.html},
  urldate = {2020-07-02}
}

@online{Vocke2018,
  author = {Ham Vocke},
  title = {The Practical Test Pyramid},
  year = 2018,
  url = {https://martinfowler.com/articles/practical-test-pyramid.html#End-to-endTests},
  urldate = {2021-03-26}
}

@online{Bushnev2019,
  author = {Yuri Bushnev},
  title = {Top 15 UI Test Automation Best Practices},
  year = 2019,
  url = {https://www.blazemeter.com/blog/top-15-ui-test-automation-best-practices-you-should-follow},
  urldate = {2020-07-03}
}

@online{Buwalda2019,
  author = {Hans Buwalda},
  title = {8 Test Automation Anti-Patterns (And How to Avoid Them)},
  year = 2019,
  url = {https://dzone.com/articles/8-test-automation-anti-patterns-and-how-to-avoid-t},
  urldate = {2020-07-03}
}

@online{Goldberg2019,
  author = {Yoni Goldberg},
  title = {Node.js and JavaScript Testing Best Practices (2020)},
  year = 2019,
  url = {https://yonigoldberg.medium.com/yoni-goldberg-javascript-nodejs-testing-best-practices-2b98924c9347},
  urldate = {2020-07-02}
}

@online{Grigorik2019,
url={https://developers.google.com/web/fundamentals/performance/critical-rendering-path/render-tree-construction},
title={Render-tree Construction, Layout, and Paint},
author={Grigorik, Ilya},
year={2019},
month={2},
urldate={2021-01-28}
}

@online{Jones2019,
  author = {Andrew Knight},
  title = {BDD 101: WRITING GOOD GHERKIN},
  year = 2019,
  url = {https://techbeacon.com/app-dev-testing/7-ways-tidy-your-test-code},
  urldate = {2021-05-04}
}

@online{Morlion2019,
  author = {Peter Morlion},
  title = {Software Testing Anti Patterns},
  year = 2019,
  url = {https://www.enov8.com/blog/software-testing-anti-patterns/},
  urldate = {2020-07-02}
}

@online{Siminiuc2019,
  author = {Alex Siminiuc},
  title = {What are the anti patterns of automation with selenium?},
  year = 2019,
  url = {https://www.quora.com/What-are-the-anti-patterns-of-automation-with-selenium},
  urldate = {2020-07-02}
}

@online{Sciamanna2019,
  author = {Anthony Sciamanna},
  title = {What are the anti patterns of automation with selenium?},
  year = 2019,
  url = {https://anthonysciamanna.com/2019/10/20/avoiding-automated-testing-pitfalls.html},
  urldate = {2020-07-02}
}

@online{Shay2019,
  author = {Liraz Shay},
  title = {BDD Cucumber Features Best Practices},
  year = 2019,
  url = {https://www.linkedin.com/pulse/bdd-cucumber-features-best-practices-liraz-shay/},
  urldate = {2020-07-02}
}

@online{Battat2020,
  author = {Battat, Michael},
  title = {How Do You Simplify End-To-End Test Maintenance?},
  year = 2020,
  url = {https://dzone.com/articles/how-do-you-simplify-end-to-end-test-maintenance-au},
  urldate = {2021-05-05}
}

@online{Pais2020,
  author = {Manuel Pais},
  title = {Anti-Patterns in Software Testing},
  year = 2020,
  url = {https://www.softwaretestingmagazine.com/knowledge/anti-patterns-in-software-testing/},
  urldate = {2020-07-02}
}

@online{Sheth2020,
  author = {Sheth, Himanshu},
  title = {16 Selenium Best Practices For Efficient Test Automation},
  year = 2020,
  url = {https://www.lambdatest.com/blog/selenium-best-practices-for-web-testing/},
  urldate = {2021-05-04}
}

@online{Temov2020,
  author = {Jane Temov},
  title = {Want to speed end-to-end testing? Don't send in the clones},
  year = 2020,
  url = {https://techbeacon.com/app-dev-testing/want-speed-end-end-testing-dont-send-clones},
  urldate = {2020-07-02}
}

@online{Google2020,
url={https://developer.android.com/studio/test/monkey},
title={UI/Application Exerciser Monkey},
author={Google},
year={2020},
month={8},
urldate={2021-05-15}
}

@online{MDN2020,
url={https://developer.mozilla.org/en-US/docs/Web/HTML/Element},
title={HTML elements reference},
author={MDN contributors},
year={2020},
month={12},
urldate={2021-01-16}
}

@online{RobotFramework2020,
author = {Robot RobotFramework},
title = {Introduction},
year = 2020,
url = {http://robotframework.org/},
urldate = {2021-02-15}
}


@online{WHATWG2021,
url={https://html.spec.whatwg.org/},
title={HTML Living Standard},
author={Web Hypertext Application Technology Working Group},
year={2021},
month={1},
urldate={2021-01-16}
}

@online{Selenium2021,
url={https://www.selenium.dev/documentation/en/getting_started_with_webdriver/locating_elements/},
title={Locating elements},
author={Software Freedom Conservancy},
year={2021},
month={1},
urldate={2021-01-19}
}