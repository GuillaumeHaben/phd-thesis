\chapter{Related Work}
\label{chap:related-work}

\chapterPage{
This chapter discusses the existing work related to the contribution of the dissertation. We present an overview on the existing literature on \gls{suit} maintenance and the existing work aiming at reducing it.
}

As introduced in Chapter~\ref{chap:introduction}, \gls{suit} scripts suffer from a high maintenance cost arising from the natural evolution of the \gls{sut}. In this chapter, we present the related work addressing specifically the problem of test evolution. Additionally, we present the work addressing the main cause leading to test evolution, namely, test fragility. Finally, we end this tour of the literature with approaches proposed to either address the problem of test fragility by making the test more robust or proposing to automatically repair the test. During this overview, we address the shortcomings of the existing work and position our contributions accordingly.

\section{Test code evolution}
\label{sec:related-evolution}

In the literature, we find a large amount of work tackling the problem of test code evolution. Most studies present in the literature address the issue by analyzing each version of a test suite and extracting change patterns between two consecutive versions. These patterns are extracted with the help of fine-grained change extraction tools such as \emph{ChangeDistiller} \cite{Fluri2007} or by performing  a coarser-grained level such as in the work of \textcite{Zaidman2011} which restrict their analysis to the file level. The rules that are used to classify the changes to a particular pattern either originate from prior (expert) knowledge \cite{Marsavina2014}, are inferred from the analysis of changes occurring in the test code base \cite{Negara2014, Labuschagne2017} or reflect the influence of patterns observed in the production code on the evolution of test code \cite{VanRompaey2008}. Thus, the analysis of the evolution of test suites takes the form of mining changes between versions and categorizing them into patterns with the aim of aiding testers to reduce the maintenance cost of their test suites either by automatic maintenance \cite{Hurdugaci2012} or by exposing sub-optimal processes \cite{Labuschagne2017}.

In particular, a line of research has been dedicated to understanding the relationship (co-evolution) between the evolution and the maintenance of test code and production code \cite{Lamkanfi2010, Zaidman2011, Marsavina2014, Levin2017, Vidacs2018, Alenezi2019}. The goal of this body of work is to perceive which are the type of production code evolution that trigger changes in test code. Other studies, take the test suite in isolation and perform systematic analysis of patterns to evaluate their root causes \cite{Pinto2012}. To this end, the analysis of bad practices present in the test code, \emph{i.e.} test smells, is an active field of research with the detection of those anti-patterns \cite{VanDeursen2001, Bowes2017, Tufano2016}, the analysis of their impact and diffusion \cite{Bavota2015, Tufano2016, Kim2020} and their automatic identification and removal \cite{VanRompaey2007, Reichhart2007, Peruma2020}.

Interestingly, many studies focus on the detection of indications of low quality of the test suite resulting in low bug detection capabilities or higher maintenance cost. However, historically, fewer studies focus on test evolution caused by incorrect behavior in the test code leading to potentially wrong signal which can originate from bugs in the test code \cite{Vahabzadeh2015}, obsolete test cases \cite{Hao2013, Tang2015} or non-deterministic behavior either in the test, the \gls{sut} or the infrastructure. However, with the work of \textcite{Luo2014} which points out the prevalence of tests which only fails (test flakiness), the non-deterministic behavior of tests begins to suscitate great interest among researchers.

All the work presented so far focuses on unit tests, which may have very different characteristics than \gls{suit}s. One key difference lies in the black box interaction with the \gls{sut} making their maintenance more difficult \cite{Berner2005}. To account for these differences, researchers need to investigate the specific evolution of \gls{suit}s. For example, \textcite{Skoglund2004} conduct an exploratory study on the evolution of \gls{suit}s showing that their maintenance is more costly than unit tests. This high maintenance cost associated to \gls{suit}, which arises with each new version of the \gls{sut}, obliterates the benefits offered by test automation. To alleviate this problem we need a better understanding of the root causes of this fragility and to propose mitigation strategies.

To answer this question, not unlike the work presented earlier, studies investigate the co-evolution between production code and \gls{suit}s \cite{Shewchuk2010, Christophe2014}. Further work focuses on external factors influencing the maintenance of test suite \cite{Alegroth2013, Kan2013, Alegroth2016, Lavoie2017} providing insights on how to create and maintain \gls{suit} suites. Findings include the analysis of the frequency at which tests should be updated \cite{Alegroth2013, Alegroth2016}, which type of bad practices appear and should be avoided \cite{Chen2012, Lavoie2017} or on the contrary should be adopted \cite{Kan2013}.

Although much work has been conducted on unit test evolution, its root causes and its impact, work targeting \gls{suit}s evolution remains scarce. Because of the particularities in term of interaction with the code, structure of the test, and the portion of code exercised, \gls{suit} evolution can rapidly lead to a higher maintenance cost. Thus, focusing on these peculiarities of \gls{suit} and how they affect their evolution is necessary to address the problem of their high maintenance cost.

\section{The Fragility Problem}
\label{sec:related-fragility}

As described in Section~\ref{sec:related-evolution}, the literature on co-evolution, suggests that there is a strong impact from evolution in production code on its associated test code, thus, requiring its maintenance. The fragility problem contrast with the promise of test automation that once test scripts are created, they can be used for efficient regression testing cycles \cite{Yandrapally2014}.  It as been reported that up to 75\% of \gls{suit}s break during \gls{gui} regression testing \cite{Memon2003a, Grechanik2009, Coppola2016} following minor changes in the \gls{sut}. This major limitation constitutes a major obstacle for developer adoption where the diffusion of SUITs remains low (\emph{e.g.}, only 8\% \cite{Coppola2017, Coppola2019b} in Android Application) when compared to unit tests (\emph{e.g.}, only 14\% \cite{Kochhar2015} to 20\% \cite{Coppola2017, Coppola2019b} in Android Application) while some consider SUITs as a cost-ineffective investment and advocate for its replacement \cite{Vliegendhart2012, Chen2020}.

To increase their robustness, researchers first need to understand which changes are affecting SUITs. The literature proposes to classify changes originating from the \gls{sut} into three categories \cite{Choudhary2011, Yandrapally2014, Coppola2016}: (1) structural changes that change the representation of the \gls{gui} element that is being targeted (\eg\ node attribute are changed in a webpage), (2) content changes affecting the content of a particular \gls{gui} element the SUIT is interacting with, such as the label of a field, the text from a button or the content of an image (3) build changes which alter the behavior of the application as the apparition of modal boxes requiring user consent for the usage of cookies in all websites to remain compliant to the \gls{gdpr} imposed by the European Union. This classification later enabled researchers to focus on a subspace of the problem of test fragility to come up with more tailored solutions.

Moreover, studies have shown that different approaches may lead to different performances when accounting for fragility. \textcite{Leotta2013, Leotta2014} perform a comparison between Capture \& Replay and \gls{gui} Test Scripting showing that even though Capture \& Replay is cheaper to produce, because of the high fragility of the tests that are generated, the cumulative cost of maintaining \gls{gui} Test Scripts becomes lower. This results show that more than the nature of the changes in the \gls{sut}, the structure of the SUITs may condition their fragility. 

Indeed, the root cause of SUIT fragility often originates from their interaction with internal information of the \gls{sut} instead of solely relying on rendered elements \cite{Yandrapally2014}. As such, in a survey on smells in software test code conducted by \textcite{Garousi2018}, the authors show that fragility can be exacerbated by bad practices in test code, such as the failure to use proper synchronization point when interacting with the \gls{gui}. Other studies focus on the representation of the identifiers, one of the major challenges being to identify \gls{gui} elements through the use of locators \cite{Hammoudi2016} which allow to uniquely identify \gls{gui} elements. For example, in the domain of web testing, \textcite{Leotta2015} propose a fragility coefficient to qualify an XPath expression that uniquely describes a web element. Thus, the more specialized the XPath, the higher its fragility coefficient.

Thus, to reduce the impact of the SUIT fragility, the research community started investigating mitigation strategies. These strategies can be classified in two families: (1) strategies aiming at avoiding test breakage altogether, thus yielding tests more robust to changes and (2) repairing the test whenever it breaks. We present the work addressing these two strategies in the remaining of this section.

\section{Robust Test Code}
\label{sec:related-robust}

To limit maintenance induced by the fragility of \gls{suit}s a straightforward approach is to render the test more robust to \gls{sut} evolution. To that end, \textcite{Pirzadeh2014} propose a framework for creating \emph{resilient} \gls{suit}s which are resistant to specific \gls{sut} refactoring by avoiding to rely on unnecessary details. 

Often, the root cause of SUIT fragility is that, to identify \gls{gui} elements, metadata depending on its internal (structural) representation are recorded \cite{Daniel2011, Yandrapally2014, Hammoudi2016}. Thus, in the literature, we find a large amount of work focusing specifically on the generation of robust locators uniquely identifying a \gls{gui} element. For example, some studies propose to generate more robust locators relying on the presentation  layer  of the \gls{sut} \cite{Montoto2011, Leotta2014, Leotta2015, Leotta2016, Zheng2018}. However, these structural locators all suffer from the reliance on the internal representation of the \gls{gui}, therefore, leaking internal details of the \gls{sut}. To overcome this limitation, different strategies have been developed to minimise the impact of structural changes, such as the use of multiple locators \cite{Leotta2015, Zheng2018, Long2020} or the use of robustness heuristics estimates \cite{Montoto2011, Leotta2014, Leotta2015, Leotta2015b}.

In an effort to avoid altogether this limitation, \gls{vgt} \cite{Bosch2014}, uses computer vision to locate elements through the bitmap graphics shown to the user at runtime. Indeed, relying on computer vision allows \gls{vgt} locators to be more flexible under structural changes, thus, leading to more robust locators \cite{Leotta2014b}. Tools like  Automating Test Automation (ATA) \cite{Thummalapenta2012, Thummalapenta2013} or the approach proposed by \textcite{Yandrapally2014}, extract contextual clues from the \gls{gui} to augment the information contained in the locator and avoid to rely too heavily on its internal representation. However, while this technique is more adaptive to underlying structural changes than structural locators, minor changes in the \gls{gui} representation (\emph{e.g.} minor changes in the layout or in the color of \gls{gui} components) might break tests relying on the exact representation of \gls{gui} elements \cite{Aldalur2017, Alegroth2018}.

One common problem in all these approaches is that there is a limit to robustness because predicting \gls{sut} evolution is difficult \cite{Kirinuki2019}. Thus, in parallel to the creation of robust tests, researchers also investigate automatic repair of\gls{suit}s exhibiting breakage.

\section{Repairing Test Code}
\label{sec:related-repair}

Aware of the high maintenance cost, and the limitation to SUIT robustness, other approaches concentrate on techniques to automatically maintain \gls{gui} test scripts and compare their performance to manual maintenance \cite{Grechanik2009, Grechanik2009b}. If we cannot avoid test breakages from occurring, then why not automatically repair the test. To that end, some tools such as GUITAR \cite{Memon2008}, SITAR \cite{Gao2016} or METER \cite{Pan2020} propose model-based approaches to automatically repair SUITs when they exhibit a failure. By representing the test in a more abstract representation, these tools can detect unusable tests and then attempt to repair them by generating repair candidates. To validate the repair candidates some heuristics are used which ensure to some extent that the intent of the test is preserved. As such, if no candidates are available, the failure can be considered as fault revealing.

Besides model-based approaches, tools like WATER \cite{Choudhary2011} or WATERFALL \cite{Hammoudi2016b} exploit the differences between the old and the new representation layer of the \gls{sut} to propose repair candidates. Similarly, white-box techniques have been proposed to repair \gls{gui} test scripts using differences between versions. For example, FlowFixer \cite{Zhang2013} instruments the \gls{gui}-based application to generate an execution trace from the old application. Then, it randomly generates new event sequences to match the method coverage of the previous version of the test. Other studies \cite{Grechanik2009, Fu2009}, instead of comparing the traces, focus on the code underlying \gls{gui} change to guide the generation of new repair candidates.

Incidentally, we can observe an emerging schema. Using heuristics such as minimal deviation from the original graph\cite{Memon2008} or exercising previously executed methods \cite{Zhang2013}, all these techniques create new tests by generating event sequences using different strategies  to explore the search space (\emph{e.g.} genetic algorithms \cite{Huang2010}, exhaustively \cite{Memon2008} or randomly \cite{Zhang2013}). This guided search allows to preserve to some extent the original intent of the test that is being repaired. One major limitation to this kind of techniques comes from the oracle problem. When a new sequence of events is generated, even though heuristics are put in place to limit this effect, there is no guarantee that the intent of the test is preserved \cite{Li2019}. Aware of this limitation, the tool proposed in the work of \textcite{Gao2016} requires human intervention during the validation process of the new tests.

In an attempt to reduce this effect, researchers have redefined the search space and only target a subset of the breakages. Thus, in a similar fashion as in Section~\ref{sec:related-robust}, when generating repair candidates, many studies reduce the search space by only targeting one type of breakage, locator breakages. As such, different tools are proposed in the literature to leverage different properties of the page such as attribute properties in the passing tests \cite{Choudhary2011}, constants in the \gls{gui} representation \cite{Eladawy2018, Kirinuki2019} or relying on computer vision\cite{Stocco2018} to regenerate broken locators while preventing any oracle violation.

\section{Summary}
\label{sec:related-summary}

This chapter presented the work in the literature that is related to the contributions of this dissertation. Overall, this chapter shows that the work addressing the causes of the high maintenance associated with maintaining \gls{gui} test scripts remain scarce. Furthermore, where some promising work showcase improvement to tackle the fragility problem, further reducing the brittleness of \gls{gui} test scripts remains crucial for a larger adoption by practitioners. The next chapters present our contributions to fill the shortcomings in the literature regarding those problems.