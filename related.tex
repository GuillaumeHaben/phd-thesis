\chapter{Related Work}
\label{chap:related-work}

\section{Test code evolution}
\label{sec:related-evolution}

In the literature, we find a large amount of work tackling the problem of test code evolution. Most studies present in the literature address the issue by analyzing each version of a test suite and extracting change patterns between two consecutive versions. These patterns are extracted with the help of fine-grain change extraction tools such as \emph{ChangeDistiller} \cite{Fluri2007} or by performing  a coarser grain level such as in the work of \textcite{Zaidman2011} which restrict their analysis at the file level. The rules that are used to classify the changes to a particular pattern either originate from prior (expert) knowledge \cite{Marsavina2014}, are inferred from the analysis of changes occurring in the test code base \cite{Negara2014, Labuschagne2017} or reflect the influence of patterns observed in the production code on the evolution of test code \cite{VanRompaey2008}. Thus, the analysis of the evolution of test suites takes the form of mining changes between version and categorizing them into patterns with the aim of aiding testers to reduce the maintenance cost of their test suites either by automatic maintenance \cite{Hurdugaci2012} or by exposing sub-optimal processes \cite{Labuschagne2017}.

In particular, a line of research has been dedicated to understanding the relationship (co-evolution) between the evolution and the maintenance of test code and production code \cite{Lamkanfi2010, Zaidman2011, Marsavina2014, Levin2017, Vidacs2018, Alenezi2019}. The goal of this body of work is to perceive which are the type of production code evolution that trigger changes in test code. Other studies, take the test suite in isolation and perform systematic analysis of each of the patterns to evaluate their root causes \cite{Pinto2012}. To this end, the analysis of bad practices present in the test code, \emph{i.e.} test smells, is an active field of research with the detection of those anti-patterns \cite{VanDeursen2001, Bowes2017, Tufano2016}, the analysis of their impact and diffusion \cite{Bavota2015, Tufano2016, Kim2020} and their automatic identification and removal \cite{VanRompaey2007, Reichhart2007, Peruma2020}.

Interestingly, most studies focus on the detection of indication of low quality of the test suite resulting in low bug detection capabilities or higher maintenance cost. However, historically, fewer studies focus on evolution caused by incorrect behavior in the test code leading to potentially wrong signal which can originate from bugs in the test code \cite{Vahabzadeh2015} or obsolete test cases \cite{Hao2013, Tang2015}. To a certain extent, this trend could change with the work of \textcite{Luo2014} which pointed out the prevalence of test which only occasionally provide a wrong signal (flaky tests). While not directly in line with test code evolution, discoveries in the root causes of test failure might shed light on the issues causing test obsolescence or help explain test code evolution.

All the work presented so far focuses on unit tests, which may have very different characteristics than SUITs, one key difference being the way to interact with the SUT which is considered as a black box making their maintenance difficult to \cite{Berner2005}. To account for these differences, researchers need investigate the specific evolution of SUITs. For example, \textcite{Skoglund2004} conduct an exploratory study on the evolution of SUITs showing that their maintenance is more costly than at the unit level. This high maintenance cost associated to SUIT arising with each new version of the SUT obliterates the benefits offered by test automation. To alleviate this problem work needs to be conducted on understanding to propose mitigation strategies.

To answer this question, not unlike the work presented earlier, studies investigate the co-evolution between production code and SUITs \cite{Shewchuk2010, Christophe2014}. Further work focuses on external factors influencing the maintenance of test suite \cite{Alegroth2013, Kan2013, Alegroth2016, Lavoie2017} providing insights on how to create and maintain SUIT suites. Such findings include the analysis of the frequency at which tests should be updated \cite{Alegroth2013, Alegroth2016}, which type of bad practices appear and should be avoided \cite{Chen2012, Lavoie2017} or on the contrary adopted \cite{Kan2013}.

Although much work has been conducted on test evolution, its root causes and its impact, work targeting SUITs evolution is still scarce. Because of the particularities in term of interaction with the code, structure of the test, portion of the code exercised, SUIT evolution can rapidly lead to a higher maintenance cost. Thus, focusing on these peculiarities of SUIT and how they affect their evolution is necessary in order to address the problem of their high maintenance cost.

\section{The Fragility Problem}
\label{sec:related-fragility}

As described in Section~\ref{sec:related-evolution}, the literature on co-evolution, suggests that there is a strong impact from evolution in production code on its associated test code, thus, requiring its maintenance. The fragility of the test code is measured by its propensity to break following minor changes in the production code not affecting its functionalities \cite{Garousi2016, Coppola2019}. In the previous definition, by minor changes, we understand changes not presenting any trouble to a human. Moreover, we define a test breakage as the event that occurs when the test raises exceptions or errors that do not pertain to the presence of a bug \cite{Stocco2018}. For example, a test testing the login behavior of a system should not be impacted by the color of the \texttt{Login} button. The fragility problem contrast with the promise of test automation that once test scripts are created, they can be used for efficient regression testing cycles \cite{Yandrapally2014}.  It as been reported that up to 75\% of SUITs break during GUI regression testing \cite{Memon2003a, Grechanik2009, Coppola2016} following minor changes in the SUT. This major limitation constitutes a major obstacle for developer adoption where the diffusion of SUITs remains low (\emph{e.g.}, only 8\% \cite{Coppola2017, Coppola2019b} in Android Application) when compared to unit tests (\emph{e.g.}, only 14\% \cite{Kochhar2015} to 20\% \cite{Coppola2017, Coppola2019b} in Android Application) while some consider SUITs as a cost-ineffective investment and advocate for its replacement \cite{Vliegendhart2012, Chen2020}.

The literature proposes to classify changes originating from the SUT into three categories \cite{Choudhary2011, Yandrapally2014, Coppola2016}: (1) structural changes that change the representation of the web element that is being targeted, \emph{e.g.}, DOM node or node attribute are changed in a webpage (2) content changes affecting the content of a particular GUI element the SUIT is interacting with, such as the label of a field, the text from a button or the content of an image (3) build changes which alter the behavior of the application as the apparition of modal boxes requiring user consent for the usage of cookies in all websites to remain compliant to the General Data Protection Regulation (GDPR) imposed by the European Union.

Moreover, studies have shown that different approaches may lead to different performances when accounting for fragility. \textcite{Leotta2013, Leotta2014} perform a comparison between Capture \& Replay and GUI Test Scripting showing that even though Capture \& Replay is cheaper to produce, because of the high fragility of the tests that are generated, the cumulative cost of maintaining GUI Test Scripts becomes lower. This results show that more than the nature of the changes in the SUT, the structure of the SUITs may condition their fragility. 

Indeed, the root cause of SUIT fragility often originates from their interaction with internal information of the SUT instead of solely relying on rendered elements \textcite{Yandrapally2014}. As such, in a survey on smells in software test code conducted by \textcite{Garousi2018}, the authors show that fragility can be exacerbated by bad practices in test code, such as the failure to use proper synchronization point when interacting with the GUI. Other studies focused on the representation of the identifiers, one of the major challenges being to identify GUI elements through the use of locators \cite{Hammoudi2016} which allow to uniquely identify GUI elements. For example, in the domain of web testing, \textcite{Leotta2015} propose a fragility coefficient to qualify an XPath expression that uniquely describes a web element. Thus, the more specialized the XPath, the higher its fragility coefficient.

Thus, to reduce the impact of the SUIT fragility, the research community started investigating mitigation strategies. These strategies can be classified in two families: (1) strategies aiming at avoiding test breakage altogether, thus yielding tests more robust to changes and (2) repairing the test whenever it breaks. We discuss these two strategies in the remaining of this section.

\section{Robust Test Code}
\label{sec:related-robust}

To limit maintenance induced by the fragility of SUITs a straightforward approach is to render the test more robust to SUT evolution. To that end,\cite{Pirzadeh2014} propose a framework for creating \emph{resilient} SUITs which are resistant to specific SUT refactoring by avoiding to rely on unnecessary details. 

Often, the root cause of SUIT fragility is that, to identify UI elements, metadata depending on the internal representation of the presentation layer are recorded \cite{Daniel2011, Yandrapally2014, Hammoudi2016}. Thus, in the literature, we find a large amount of work focusing specifically on the generation of robust locators uniquely identifying a GUI element. For example, some studies propose to generate more robust locators relying on the presentation layer (structural) of the SUT \cite{Montoto2011, Leotta2014, Leotta2015, Leotta2016, Zheng2018}. However, these structural locators all suffer from the leakage of the internal representation of the presentation layer. To overcome this limitation, different strategies have been developed to minimise the impact of structural changes, such as the use of multiple locators \cite{Leotta2015, Zheng2018, Long2020} or the use of robustness heuristics estimates \cite{Montoto2011, Leotta2014, Leotta2015, Leotta2015b}.

To alleviate this limitation, Visual GUI Testing (VGT) \cite{Bosch2014}, uses computer vision to locate elements through the bitmap graphics shown to the user at runtime. Indeed, relying on computer vision allows third-generation locators to be more flexible under structural changes, thus, leading to more robust locators \cite{Leotta2014b}. Tools like  Automating Test Automation (ATA) \cite{Thummalapenta2012, Thummalapenta2013} or the approach proposed by \textcite{Yandrapally2014} extract contextual clues from the GUI to augment the information contained in the locator and avoid to rely too heavily on the structure of the DOM. However, while this technique is more adaptive to underlying structural changes than structural locators, minor changes in the GUI representation (\emph{e.g.} minor changes in the layout or in the color of GUI components) might break tests relying on the exact representation of GUI elements \cite{Aldalur2017, Alegroth2018}.

On common problem in all these approaches is that there is a limit to robustness because predicting SUT evolution is difficult \cite{Kirinuki2019}. Thus, in parallel to the creation of robust tests, researchers also investigates automatic repair of SUITs exhibiting breakage.

\section{Repairing Test Code}
\label{sec:related-repair}

The other approach present in the literature to tackle the fragility problem is to repair the test as they exhibit breakages. If we cannot avoid test breakages from occurring, then why not automatically repair the test. To that end, some tools such as GUITAR \cite{Memon2008}, SITAR \cite{Gao2016} or METER \cite{Pan2020} propose model-based approaches to automatically repair SUITs when they exhibit a failure. By representing the test in a more abstract representation, these tools can detect unusable tests and then attempt to repair them by generating repair candidates . To validate test candidates some heuristics are used which guarantee to some extent that the intent of the test is preserved. As such, if no candidate are available, the failure can be considered as fault revealing.

Besides model-based approaches, there exist white-box have been proposed to repair GUI test scripts. For example, FlowFixer \cite{Zhang2013} instruments the GUI-based application to generate an execution trace from the old application. Then, FlowFixer randomly generates new actions in order to match similar methods in the new version of the application. Instead of comparing the traces, other studies \cite{Grechanik2009, Fu2009} focus on the code underlying GUI change to select new test candidates.

Thus, we observe a schema emerging. Using heuristics such as minimal deviation from the original graph\cite{Gao2016} or exercising previously executed methods \cite{Zhang2013}, all these techniques suggest new tests by generating new event sequences using different strategies (\emph{e.g.} genetic algorithms \cite{Huang2010}, exhaustively \cite{Memon2008} or randomly \cite{Zhang2013}) to explore the search space. One major limitation to this kind of technique comes from the oracle problem. Once the test has been modified that way, even though heuristics are put in place to limit this effect, there is no guarantee that the intent of the test is preserved \cite{Li2019}.

%Besides model-based approaches, white box approaches have also been studied for GUI test script repair. Daniel et al. [15] propose to record GUI code refactorings as they are conducted in an IDE and use them to repair test scripts. Grechanik et al. [14] propose a tool to extract information about GUI changes by analyzing the source code and test scripts and generate repair candidates for GUI test scripts to be selected by testers. Fu et al. [56] develop a type-inference technique for GUI test scripts based on static analysis, which can

In Section~\ref{sec:related-robust} we observe that many strategies to reduce test fragility target locator specifically. In a similar fashion, when generating repair candidates, many studies reduce the search space by only targeting one type of breakage, locator breakage. As such, different tools are proposed in the literature to leverage different properties of the page such as attribute properties in the passing tests\cite{Choudhary2011} constants in the GUI representation\cite{Kirinuki2019} or relying on computer vision\cite{Stocco2018} to regenerate broken locators while preventing any oracle violation.


%read the related work of Zhang2013 and read more Pan2020, include Hammoudi2016 (Waterfall), Eladawy2019, Daniel2011, Stocco2018 (Vista), Imtiaz2019

%aware of the high maintenance cost concentrate on technique to automatically maintain GUI test scripts and compared their performance to manual maintenance \cite{Grechanik2009, Grechanik2009b}.

%\textcite{Pinto2012} analyze the evolution of test suites and extract actions performed on the test suite to see how the test suite is evolving. They conclude that test repair occurs in practice and that it is not due to only assertion fixes and suggest further research of automatic repair tools.