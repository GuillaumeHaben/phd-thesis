\chapter{Related Work}
\label{chap:related-work}

\section{Test code evolution}
\label{sec:related-evolution}

In the literature, we find a large amount of work tackling the problem of test code evolution. Most studies present in the literature address the issue by analyzing each version of a test suite and extracting change patterns between two consecutive versions. These patterns are extracted with the help of fine-grain change extraction tools such as \emph{ChangeDistiller} \cite{Fluri2007} or by performing  a coarser grain level such as in the work of \textcite{Zaidman2011} which restrict their analysis at the file level. The rules that are used to classify the changes to a particular pattern either originate from prior (expert) knowledge \cite{Marsavina2014}, are inferred from the analysis of changes occurring in the test code base \cite{Negara2014, Labuschagne2017} or reflect the influence of patterns observed in the production code on the evolution of test code \cite{VanRompaey2008}. Thus, the analysis of the evolution of test suites takes the form of mining changes between version and categorizing them into patterns with the aim of aiding testers to reduce the maintenance cost of their test suites either by automatic maintenance \cite{Hurdugaci2012} or by exposing sub-optimal processes \cite{Labuschagne2017}.

In particular, a line of research has been dedicated to understanding the relationship (co-evolution) between the evolution and the maintenance of test code and production code \cite{Lamkanfi2010, Zaidman2011, Marsavina2014, Levin2017, Vidacs2018, Alenezi2019}. The goal of this body of work is to perceive which are the type of production code evolution that trigger changes in test code. Other studies, take the test suite in isolation and perform systematic analysis of each of the patterns to evaluate their root causes \cite{Pinto2012}. To this end, the analysis of bad practices present in the test code, \emph{i.e.} test smells, is an active field of research with the detection of those anti-patterns \cite{VanDeursen2001, Bowes2017, Tufano2016}, the analysis of their impact and diffusion \cite{Bavota2015, Tufano2016, Kim2020} and their automatic identification and removal \cite{VanRompaey2007, Reichhart2007, Peruma2020}.

Interestingly, most studies focus on the detection of indication of low quality of the test suite resulting in low bug detection capabilities or higher maintenance cost. However, historically, fewer studies focus on evolution caused by incorrect behavior in the test code leading to potentially wrong signal which can originate from bugs in the test code \cite{Vahabzadeh2015} or obsolete test cases \cite{Hao2013, Tang2015}. To a certain extent, this trend could change with the work of \textcite{Luo2014} which pointed out the prevalence of test which only occasionally provide wrong signal (flaky tests). While not directly in line with test code evolution, discoveries in the root causes of test failure might shed light on the issues causing test obsolescence or help explain test code evolution.

All the work presented so far focuses on unit tests, which may have very different characteristics than SUITs, one key difference being the way to interact with the SUT which is considered as a black box making their maintenance difficult \cite{Berner2005}. To account for these differences, researchers need investigate the specific evolution of SUITs. For example, \textcite{Skoglund2004} conduct an exploratory study on the evolution of SUITs showing that their maintenance is more costly than at the unit level. This high maintenance cost associated to SUIT arising with each new version of the SUT obliterates the benefits offered by test automation. To alleviate this problem work needs to be conducted on understanding to propose mitigation strategies.

To answer this question, not unlike the work presented earlier, studies investigate the co-evolution between production code and SUITs \cite{Shewchuk2010, Christophe2014}. Further work focuses on external factors influencing the maintenance of test suite \cite{Alegroth2013, Kan2013, Alegroth2016, Lavoie2017} providing insights on how to create and maintain SUIT suites. Such findings include the analysis of the frequency at which tests should be updated \cite{Alegroth2013, Alegroth2016}, which type of bad practices appear and should be avoided \cite{Chen2012, Lavoie2017} or on the contrary adopted \cite{Kan2013}.

Although much work has been conducted on test evolution, its root causes and its impact, work targeting SUITs evolution is still scarce. Because of the particularities in term of interaction with the code, structure of the test, portion of the code exercised, SUIT evolution can rapidly lead to a higher maintenance cost. Thus, focusing on these peculiarities of SUIT and how they affect their evolution is necessary in order to address the problem of their high maintenance cost.

\section{The Fragility Problem}
\label{sec:related-fragility}

As described in Section~\ref{sec:related-evolution}, the literature on co-evolution, suggests that there is a strong impact from evolution in production code on its associated test code, thus, requiring its maintenance. The fragility of the test code is measured by its propensity to break following minor changes in the production code not affecting its functionalities \cite{Garousi2016, Coppola2019}. Note that here, by minor changes, we understand changes not presenting any trouble to a human. For example, a test testing the login behavior of a system should not be impacted by the color of the \texttt{Login} button. The fragility problem contrast with the promise of test automation that once test scripts are created, they can be used for efficient regression testing cycles \cite{Yandrapally2014}.  It as been reported that up to 75\% of SUITs become unusable during GUI regression testing \cite{Memon2003a, Grechanik2009} following minor changes in the SUT. This major limitation constitutes a major obstacle for developer adoption where the diffusion of SUITs remains low (\emph{e.g.}, only 8\% \cite{Coppola2017, Coppola2019b} in Android Application) when compared to unit tests (\emph{e.g.}, only 14\% \cite{Kochhar2015} to 20\% \cite{Coppola2017, Coppola2019b} in Android Application) while some consider SUITs as a cost-ineffective investment and advocate for its replacement \cite{Vliegendhart2012, Chen2020}.

The literature proposes to classify changes originating from the SUT into three categories \cite{Choudhary2011, Yandrapally2014, Coppola2016}: (1) structural changes that change the representation of the web element that is being targeted, \emph{e.g.}, DOM node or node attribute are changed in a webpage (2) content changes affecting the content of a particular GUI element the SUIT is interacting with, such as the label of a field, the text from a button or the content of an image (3) build changes which alter the behavior of the application as the apparition of modal boxes requiring user consent for the usage of cookies in all websites to remain compliant to the General Data Protection Regulation (GDPR) imposed by the European Union.

Moreover, studies have shown that different approaches may lead to different performances when accounting for fragility. \textcite{Leotta2013, Leotta2014} perform a comparison between Capture \& Replay and GUI Test Scripting showing that even though Capture \& Replay is cheaper to produce, because of the high fragility of the tests that are generated, the cumulative cost of maintaining GUI Test Scripts becomes lower. This results show that more than the nature of the changes in the SUT, the structure of the SUITs may condition their fragility. 

Indeed, the root cause of SUIT fragility often originates from their interaction with internal information of the SUT instead of solely relying on rendered elements \textcite{Yandrapally2014}. As such, in a survey on smells in software test code conducted by \textcite{Garousi2018}, the authors show that fragility can be exacerbated by bad practices in test code, such as the failure to use proper synchronization point when interacting with the GUI. Other studies focused on the representation of the identifiers, one of the major challenges being to identify GUI elements through the use of locators \cite{Hammoudi2016} which allow to uniquely identify GUI elements. For example, in the domain of web testing, \textcite{Leotta2015} propose a fragility coefficient to qualify an XPath expression that uniquely describes a web element. Thus, the more specialized the XPath, the higher its fragility coefficient.

Thus, to reduce the impact of the SUIT fragility, the research community started investigating mitigation strategies. These strategies can be classified in two families: (1) strategies aiming at avoiding test breakage altogether, thus yielding tests more robust to changes and (2) repairing the test whenever it breaks. We discuss these two strategies in the remaining of this section.

\section{Robust Test Code}
\label{sec:related-robust}

To limit maintenance induced by the fragility of SUITs a straightforward approach is to render the test more robust to SUT evolution. To that end,\cite{Pirzadeh2014} propose a framework for creating \emph{resilient} SUITs which are resistant to specific SUT refactoring by avoiding to rely on unnecessary details. 

In the literature, we find a large amount of work focusing specifically on the generation of robust locators uniquely identifying a GUI element. For example, some studies propose to generate more robust locators relying on the presentation layer (structural) of the SUT \cite{Montoto2011, Leotta2014, Leotta2015, Leotta2016, Zheng2018}. However, these structural locators all suffer from the leakage of the internal representation of the presentation layer. To overcome this limitation, different strategies have been developed to minimise the impact of structural changes, such as the use of multiple locators \cite{Leotta2015, Zheng2018} or the use of short and expressive representation for the GUI element \cite{Montoto2011, Leotta2014, Leotta2015}.

To alleviate this limitation, Visual GUI Testing (VGT) \cite{Bosch2014}, uses computer vision to locate elements through the bitmap graphics shown to the user at runtime. Indeed, relying on computer vision allows third-generation locators to be more flexible under structural changes, thus, leading to more robust locators \cite{Leotta2014b}. Tools like COLOR \cite{Kirinuki2019}, Automating Test Automation (ATA) \cite{Thummalapenta2013} or the approach proposed by \textcite{Yandrapally2014} propose to extract contextual clues from the GUI to augment the information contained in the locator and avoid to rely too heavily on the structure of the DOM. However, while this technique is more adaptive to underlying structural changes than structural locators, minor changes in the GUI representation (\emph{e.g} minor changes in the layout or in the color of GUI components) might break tests relying on the exact representation of GUI elements \cite{Aldalur2017, Alegroth2018}.

\section{Repairing Test Code}
\label{sec:related-repair}

Further studies, aware of the high maintenance cost concentrate on technique to automatically maintain GUI test scripts and compared their performance to manual maintenance \cite{Grechanik2009, Grechanik2009b}.

\textcite{Pinto2012} analyze the evolution of test suites and extract actions performed on the test suite to see how the test suite is evolving. They conclude that test repair occurs in practice and that it is not due to only assertion fixes and suggest further research of automatic repair tools.

Finally, an alternative to trying to build more robust locators is to automatically repair tests suffering from locator breakages. Different tools are proposed in the literature trying to leverage different properties of the page such as attribute properties in the passing tests\cite{Choudhary2011} constants in the GUI representation\cite{Kirinuki2019} or relying on computer vision\cite{Stocco2018} to regenerate broken locators. Because these techniques focus on the root causes of locator breakages and the ways to fix them, lessons can be learned on how to generate less fragile locators.