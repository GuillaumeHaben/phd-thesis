\chapter{Related Work}
\label{chap:related-work}

\section{Test code evolution}
\label{sec:related-evolution}

In the literature, we find a large amount of work tackling the problem of test code evolution. Most paper present in the literature tackles the issue by analyzing each version of a test suite and extracting change patterns between two consecutive versions. These patterns are extracted with the help of fine-grain change extraction tools such as \emph{ChangeDistiller} \cite{Fluri2007}, the analysis can be performed at a coarser grain level such as in the work of \textcite{Zaidman2011} or the influence of patterns observed in the production code on the evolution of test code \cite{VanRompaey2008}. As for the patterns, where they are typically extracted from prior knowledge \cite{Marsavina2014}, some work focused some attention on automatically deriving such patterns from changes occurring in the code base \cite{Negara2014}. Thus, the analysis of the evolution of test suites takes the form of mining changes between version and categorizing them into patterns with the aim of aiding testers to reduce the maintenance cost of their test suites \cite{Hurdugaci2012}.

In particular, some work has been dedicated to understanding the relationship (co-evolution) between the evolution and the maintenance of test code and production code \cite{Lamkanfi2010, Zaidman2011, Marsavina2014, Levin2017, Vidacs2018, Alenezi2019}. The goal of this body of work is to perceive which are the type of production code evolution that trigger changes in test code. Other studies, take the test suite in isolation and perform systematic analysis of each of the patterns to evaluate their root causes \cite{Pinto2012}. Interestingly, where most studies focused on the detection of indication of low quality of the test suite resulting in low bug detection capabilities or higher maintenance cost, historically fewer focus on evolution caused by incorrect behavior in the test code leading to potentially wrong signal which can originate from bugs in the test code \cite{Vahabzadeh2015} or obsolete test cases \cite{Hao2013, Tang2015}. Moreover, \textcite{Luo2014} pointed out the prevalence of test which only occasionally provide wrong signal. Where this line of research is not directly in line with test code evolution, discoveries in the root causes of test failure might shed light on the issues causing test obsolescence.

All the work presented so far focuses on unit tests, which may have very different characteristics than SUITs. Thus, to account for these differences, research investigated the specific evolution of SUITs. \textcite{Skoglund2004} were the first to conduct an exploratory study on the evolution of SUITs showing that their maintenance is more costly than at the unit level. Similar to the work presented earlier, further work investigated the co-evolution between production code and test code \cite{Shewchuk2010}. Similarly, \textcite{Christophe2014} propose an analysis of the co-evolution of test code and SUITs relying on a coarse-grain change algorithm. Further work focuses on external factors influencing the maintenance of test suite \cite{Alegroth2013, Kan2013, Alegroth2016, Lavoie2017} providing insights on how to create and maintain SUIT suites. Such findings include the analysis of the frequency at which tests should be updated \cite{Alegroth2013, Alegroth2016} which type of bad practices appear and should be avoided \cite{Lavoie2017} or on the contrary adopted \cite{Kan2013}.

Although much work has been conducted on test evolution, its root causes and its impact, work targeting SUITs evolution is still scarce. Because of the particularities in term of interaction with the code, structure of the test, portion of the code exercised, SUIT evolution can rapidly lead to a higher maintenance cost. Thus, we encourage more research in the field where a better understanding of the root causes can lead to better mitigation strategies.

%In their work, \textcite{Labuschagne2017} explore the cost and benefits of automated regression testing in practice. To do so, they select 61 projects and analyze their test execution reports. They show that in some cases tests break because of invalid assumption and that maintenance cost could be reduced via the use of better development processes.

\section{The Fragility Problem}
\label{sec:related-fragility}

As described in Section~\ref{sec:related-evolution}, the literature on co-evolution, suggests that there is a strong impact from evolution in production code on its associated test code, thus, requiring its maintenance. The fragility of the test code is measured by its propensity to break following minor changes in the production code not affecting its functionalities \cite{Garousi2016, Coppola2019}. Note that here, by minor changes, we understand changes not presenting any trouble to a human. For example, a test testing the login behavior of a system should not be impacted by the color of the \texttt{Login} button. The fragility problem contrast with the promise of test automation that once test scripts are created, they can be used for efficient regression testing cycles \cite{Yandrapally2014}. This major limitation constitutes a major obstacle for developer adoption where the diffusion of SUITs remains low (\emph{e.g.}, only 8\% \cite{Coppola2017, Coppola2019b} in Android Application) when compared to unit tests (\emph{e.g.}, only 14\% \cite{Kochhar2015} to 20\% \cite{Coppola2017, Coppola2019b} in Android Application).

The literature proposes to classify changes originating from the SUT into three categories \cite{Choudhary2011, Yandrapally2014, Coppola2016}: (1) structural changes that change the representation of the web element that is being targeted, \emph{e.g.}, DOM node or node attribute are changed in a webpage (2) content changes affecting the content of a particular GUI element the SUIT is interacting with, such as the label of a field, the text from a button or the content of an image (3) build changes which alter the behavior of the application as the apparition of modal boxes requiring user consent for the usage of cookies in all websites to remain compliant to the General Data Protection Regulation (GDPR) imposed by the European Union.

Moreover, studies have shown that different approaches may lead to different performances when accounting for fragility. \textcite{Leotta2013, Leotta2014} perform a comparison between Capture \& Replay and GUI Test Scripting showing that even though Capture \& Replay is cheaper to produce, because of the high fragility of the tests that are generated, the cumulative cost of maintaining GUI Test Scripts becomes lower. This results show that more than the nature of the changes in the SUT, the structure of the SUITs may condition their fragility. 

Indeed, the root cause of SUIT fragility often originates from their interaction with internal information of the SUT instead of solely relying on rendered elements \textcite{Yandrapally2014}. As such, in a survey on smells in software test code conducted by \textcite{Garousi2018}, the authors show that fragility can be exacerbated by bad practices in test code, such as the failure to use proper synchronization point when interacting with the GUI. Other studies focused on the representation of the identifiers, one of the major challenges being to identify GUI elements through the use of locators \cite{Hammoudi2016} which allow to uniquely identify GUI elements. For example, in the domain of web testing, \textcite{Leotta2015} propose a fragility coefficient to qualify an XPath expression that uniquely describes a web element. Thus, the more specialized the XPath, the higher its fragility coefficient.

\section{Repairing Test Code}
\label{sec:related-repair}

Further studies, aware of the high maintenance cost concentrate on technique to automatically maintain GUI test scripts and compared their performance to manual maintenance \cite{Grechanik2009, Grechanik2009b}.

\textcite{Pinto2012} analyze the evolution of test suites and extract actions performed on the test suite to see how the test suite is evolving. They conclude that test repair occurs in practice and that it is not due to only assertion fixes and suggest further research of automatic repair tools.

Finally, an alternative to trying to build more robust locators is to automatically repair tests suffering from locator breakages. Different tools are proposed in the literature trying to leverage different properties of the page such as attribute properties in the passing tests\cite{Choudhary2011} constants in the GUI representation\cite{Kirinuki2019} or relying on computer vision\cite{Stocco2018} to regenerate broken locators. Because these techniques focus on the root causes of locator breakages and the ways to fix them, lessons can be learned on how to generate less fragile locators.

\section{Robust Test Code}
\label{sec:related-robust}

\cite{Pirzadeh2014} propose a novel framework for creating SUITs resistant to refactoring of web applications.

The problem of the impact of DOM evolution on locators is not new in the research community. First attempts at solving this problem come from the information retrieval community\cite{Anton2005, Dalvi2009, Cohen2015}. However, they target different elements in the DOM (wrappers) presenting significant differences to the ones used for automation testing.

Other approaches to generate more robust second-generation locators for GUI-base testing can be found in the literature. \textcite{Leotta2014, Leotta2016} and \textcite{Montoto2011} propose algorithms to generate more robust XPath. The resulting locators are a fully compliant XPath and thus while algorithms are very efficient at generating robust XPath, they still suffer from the same drawbacks in terms of lack of flexibility. 

Other approaches propose to enrich the locators with additional information. Some authors\cite{Leotta2015, Zheng2018} propose a multi-locator approach which combines different XPath in order to generate more robust locators. Their results suggest that the combination of multiple location paths yields more robust locators. Closer to our work, \textcite{Thummalapenta2013} and later \textcite{Yandrapally2014} propose approaches to extract contextual clues from the GUI to augment the information contained in the locator and avoid to rely too heavily on the structure of the DOM. However, those approaches focuses only on the proximity of labels and if it cannot find any falls back to XPath. We argue that the HTML document contains more contextual information that can be leverage during the construction of the locator.