\section{The Original Study}
\label{sec:replication-pinto}

This work is a replication of the study by Pinto \etal \cite{Pinto2020}. 
In this section, we briefly summarise the approach they presented for flakiness prediction. 
We first present the dataset of existing flaky tests which they used in their study. Then, we explain their source code representation and prediction model. 
Finally, we recall their evaluation methodology and results. 

\subsection{Dataset}
In their original study, Pinto \etal relied on the DeFlaker dataset, which was proposed by Bell \etal~\cite{Bell2018}. 
This dataset includes 1,874 flaky tests identified using the DeFlaker tool on multiple revisions of 24 open-source Java projects.
Pinto \etal selected 1,403 flaky tests from this dataset to build their set of flaky tests.
They also randomly selected tests that were not flagged as flaky by DeFlaker to form a set of \textit{a priori} non-flaky tests. 
To mitigate the problem of class imbalance, both sets had the same size. 

\subsection{Prediction Model}
In order to prepare the classification inputs, Pinto \etal extracted identifiers that represent the test vocabulary and complexity. 
This extraction takes several steps. First, they localise the file where the test is defined. Then, they select all identifiers contained in this test, pre-process them by splitting them according to their camel-case syntax and converting them into lower-case. Finally, they remove stop words from the obtained set. Each flaky and non-flaky test is represented as follows:
\begin{itemize}
    \item A vector of booleans indicating for each token if it is present in the test or not;
    \item The number of lines of code;
    \item The number of Java keywords contained in the test.
\end{itemize}  
The last two features are used as a proxy for code complexity.
The authors used these vectors as inputs for their prediction models.
In particular, they evaluated the performance of five machine learning classifiers: Random Forest, Decision Tree, Naive Bayes, Support Vector Machine, and Nearest Neighbour. 
\subsection{Evaluation}
\subsubsection{Evaluation methodology}
The authors follow a standard methodology to train and evaluate the five classifiers. That is, they split the whole set of test cases into a training set containing 80\% of the tests and a validation (``test'') set containing the remaining 20\%.

\begin{table*}[t]
\vspace{1.0em}
\centering
    \caption{Model performance of the Pinto \etal study~\cite{Pinto2020}}
\label{table:resultsPinto}
\resizebox{\textwidth}{!}{\begin{tabular}{l l|r r r r r r} 
 \toprule
  & \textbf{Algorithm ~~~~~~~~~~~~} & \textbf{~~~~~~ Precision} & \textbf{~~~~~~ Recall} & \textbf{~~~~~~ F1} & \textbf{~~~~~~ MCC} & \textbf{~~~~~~ AUC} & \\ %[0.25ex]
 \midrule
  & Random Forest & \textbf{0.99} & 0.91 & \textbf{0.95} & \textbf{0.90} & \textbf{0.98}  &  \\
  & Decision Tree & 0.89 & 0.88 & 0.89 & 0.77 & 0.91  &  \\
  & Naive Bayes & 0.93 & 0.80 & 0.86 & 0.74 & 0.93  &  \\ 
  & Support Vector & 0.93 & \textbf{0.92} & 0.93 & 0.85 & 0.93  &  \\
  & Nearest Neighbour & 0.97 & 0.88 & 0.92 & 0.85 & 0.93  &  \\
 \bottomrule
\end{tabular}}
\vspace{1.0em}
\end{table*}

They report the standard precision, recall and F1-score metrics. The precision shows the proportion of correctly classified flaky tests. The recall shows the proportion of flaky tests found out of all existing ones. 
They focus their analysis on the F1-score, which combines precision and recall to assess the model performance. Detailed results for their different models are listed in Table ~\ref{table:resultsPinto}.

\subsubsection{Results}
Among the five trained models, the most promising one was Random Forest, having a performance as high as 0.95 for the F1-score. Altogether, the five models showed great performance on their dataset.  
