\section{Data Collection}
\label{sec:sherloc-data}

The objective of our study is to assess the effectiveness of FL techniques in identifying flaky classes.
To achieve this, we need a set of flaky tests for which the responsible classes are already known. 
For this, we rely on flakiness-fixing commits as they provide information about classes that were modified as part of the fix.
Our assumption is that such classes are, at least, part of the root cause.
To collect flaky classes, we followed a four-step process.

\paragraph{Search} 
This step aims to identify Java projects containing the highest number of flakiness-fixing commits. For this, we relied on two sets of projects to consider. 
We built the first set by using the SEART GitHub Search Engine~\cite{githubsearch}. Out of the 81,180 available Java projects, we selected the top 200 projects for each of those criteria: number of commits, contributors, stars, releases, issues, and files. This sorting was made with the aim of finding the bigger and more complex projects, thus maximising our chance to find flakiness-fixing commits. 
Keeping only unique projects in those sets, we ended up with a first list of 902 projects. As a second set, we use the 187 projects available in the \textsc{iDFlakies} dataset~\cite{Lam2019iDFlakies}. 
For each of the 1,089 projects, 902 from the first and 187 from the second set, we query the GitHub API looking for commits with messages containing the keyword \textit{flaky}. This led to the identification of 16,501 commits.
We look further into whether these commits are truly suitable for our purpose through the following processes. 

\newcommand\mch[2]{\multicolumn{1}{>{\centering\arraybackslash}b{#1}}{#2}}


\begin{table}[t]
\caption{Collected Data. \textit{ffc:} number of flakiness-fixing commits. \textit{all:} number of commits in the project.
\centering}
\centering
\label{tab:SD-info}
\scalebox{0.8}{
\begin{tabular}{l|cc|cc|cc}
\toprule
\textbf{Proj. } & \multicolumn{2}{c|}{\textbf{\#Commits}} & \multicolumn{2}{c|}{\textbf{\#Tests}} & \multicolumn{2}{c}{\textbf{\#Classes}} \\
 & ffc & all &  min - max & avg & min -- max & avg \\
\midrule
Hbase & 8 & 18,990 & 138 - 2,089 & 1,113 & 734 -- 1366 & 1053.4 \\
Ignite & 14 & 27,903 & 15 - 1,018 & 174 & 72 -- 1767 & 1262.3 \\
Pulsar & 10 & 8,516 & 194 - 1,326 & 626 & 171 -- 422 & 259.7 \\
Alluxio & 3 & 32,560 & 315 - 694 & 473 & 131 -- 817 & 360.3 \\
Neo4j & 3 & 71,824 &  21 - 5,782 & 2,139 & 40 -- 1663 & 581.3 \\
\midrule
Total & 38 &  &  15 - 5,782 & 905 & 40 -- 1767 & 820.2 \\
\bottomrule
\end{tabular}}
\end{table}


\paragraph{Inspection}
The objective of this step is to filter commits that do not provide a clear indication about the flaky class. Hence, we look for flakiness-fixing commits containing any of the following keywords: \textit{fix, repair, solve, correct, patch, prevent}. Then, we analyse each commit and keep the ones that:
\begin{itemize}[wide=0pt,noitemsep,topsep=0pt]
    \item The fix affects the code under test 
    (not only the test itself);
    \item The changes are atomic enough (\ie containing only relevant changes) allowing us to discern the flaky class(es). 
\end{itemize}

This led to the selection of 85 commits from five projects. We further discarded 22 commits for which the flaky tests or commits were not retrievable (\eg rejected pull request), leaving 63 commits in the end. 


\paragraph{Test execution}
This step aims to select commits that are usable in our evaluation.
Our first question inspects the effectiveness of SBFL, a technique that requires a coverage matrix indicating the classes covered by each test.
Hence, for a commit to be usable in our analysis, its test suite should be runnable allowing us to extract the coverage matrix.
To ensure this, we used  \textsc{GZoltar}\footnote{\url{https://github.com/GZoltar/gzoltar/blob/master/com.gzoltar.ant/}}, a Maven plugin that allows collecting coverage information for each commit.

For 11 commits, we were unable to run \textsc{GZoltar} due to an incompatible Java version. We also found that the flakiness patches were irrelevant in 10 commits.
For instance, some commits were fixing modules in other programming languages or modifying non-source code files.
Lastly, we filtered out four additional commits since the reported flaky failures were not \textit{flaky test failures}.
Consequently, we dropped 25 commits in addition. Table~\ref{tab:SD-info} summarises the retained projects.
The complete list of flakiness-fixing commits is available in our replication package. 

\paragraph{Extraction}
For each collected flakiness-fixing commit, we retrieve the source code, the test suite, the fixed flaky test, and the flaky class. To retrieve the flaky classes, two authors manually analysed the commit diff and message to identify them.
Overall, the identification was obvious since we selected atomic commits beforehand.
Hence, there were no disagreements between the authors at this step.
The identified classes are considered the ground truth of our study. 

