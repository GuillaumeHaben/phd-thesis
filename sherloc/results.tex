\section{Results}
\label{sec:sherloc-results}


\subsection{RQ1: Effectiveness}

Table~\ref{tab:RQ1} shows the localisation results of SBFL \formulas.
Among the four SBFL \formulas, Dstar yields the worst results both in accuracy and wasted effort, while the other three perform similarly. Out of 38 analysed flaky classes, Dstar ranks 18 (47\%) in the top 10. Ochiai, which performs the best, places 53\% of flaky classes (\ie 21) within the top 10 and 16\% (6) at the top. Nevertheless, regardless of which formula we use, our SBFL-based approach outperforms the baseline of inspecting classes covered by flaky tests: for all four SBFL \formulas, $R_{wef}$ is always smaller than 50 in total, especially in its median. It is worth noting that since the total number of classes covered by flaky tests differs in each flaky commit, $R_{wef}$ does not always concur with \textit{wef}. For Ochiai, $R_{wef}$ reduces to 6, meaning we only need to inspect 6\% of the classes covered by flaky tests.  

\begin{table*}[ht]
\caption{RQ1: Effectiveness of SBFL \formulas. 
(\#) denotes the total number of flaky commits for each project\centering.
The row \textit{Perc} contains the percentage of flaky commits whose triggering flaky classes are ranked in the top $n$; these values are computed only for \textit{acc@n}. 
\label{tab:RQ1}}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|p{0.26cm}p{0.26cm}p{0.26cm}p{0.45cm}|rr|p{0.26cm}p{0.26cm}p{0.26cm}p{0.45cm}|rr|p{0.26cm}p{0.26cm}p{0.26cm}p{0.45cm}|rr|p{0.26cm}p{0.26cm}p{0.26cm}p{0.45cm}|rr}
\toprule
& \multicolumn{6}{c|}{\textbf{Dstar}} & \multicolumn{6}{c|}{\textbf{Ochiai}} & \multicolumn{6}{c|}{\textbf{Tarantula}} & \multicolumn{6}{c}{\textbf{Barinel}} \\
\textbf{Proj. (\#)} & \multicolumn{4}{c}{acc} & \multicolumn{2}{c|}{wef (R$_{wef}$)} & \multicolumn{4}{c}{acc} & \multicolumn{2}{c|}{wef (R$_{wef}$)} & \multicolumn{4}{c}{acc} & \multicolumn{2}{c|}{wef (R$_{wef}$)} & \multicolumn{4}{c}{acc} & \multicolumn{2}{c}{wef (R$_{wef}$)} \\
& @1 & @3 & @5 & @10 & mean & med & @1 & @3 & @5 & @10 & mean & med & @1 & @3 & @5 & @10 & mean & med & @1 & @3 & @5 & @10 & mean & med \\
\midrule

Hbase (8) & 0 & 3 & 4 & 4 & 33.0 (17) & 7 (5) & 2 & 5 & 5 & 5 & 14.9 (13) & 1 (4) & 1 & 4 & 4 & 5 & 11.9 (12) & 4 (4) & 1 & 4 & 4 & 5 & 11.6 (12) & 4 (4) \\
ignite (14) & 0 & 2 & 2 & 2 & 214.7 (21) & 31 (4) & 0 & 3 & 3 & 4 & 212.0 (20) & 20 (4) & 0 & 3 & 3 & 4 & 177.1 (17) & 20 (4) & 0 & 3 & 3 & 4 & 175.1 (17) & 20 (4) \\
Pulsar (10) & 1 & 3 & 6 & 9 & 9.9 (21) & 4 (6) & 3 & 5 & 6 & 9 & 9.2 (13) & 3 (6) & 3 & 5 & 6 & 9 & 9.2 (13) & 3 (6) & 3 & 5 & 6 & 9 & 9.2 (13) & 3 (6)\\
Alluxio (3) & 0 & 0 & 0 & 1 & 60.7 (43) & 72 (31) & 0 & 0 & 0 & 1 & 71.0 (46) & 72 (41) & 0 & 0 & 0 & 0 & 92.7 (59) & 73 (58) & 0 & 0 & 0 & 0 & 105.3 (66) & 87 (65) \\
Neo4j (3) & 1 & 2 & 2 & 2 & 12.0 (41) & 1 (18) & 1 & 2 & 2 & 2 & 12.0 (41) & 1 (18) & 1 & 2 & 2 & 2 & 23.0 (43) & 1 (18) & 1 & 2 & 2 & 2 & 23.7 (43) & 1 (18) \\

\midrule
Total  (38) & 2 & 10 & 14 & 18 & 94.4 (24) & \textbf{11 (17)} & 6 & 15 & 16 & 21 & 90.2 (21) & 7 (6) & 5 & 14 & 15 & 20 & 79.3 (21) & 8 (7) & 5 & 14 & 15 & 20 & 79.6 (21) & 8 (7) \\
Perc (\%) & 5 & 26 & 37 & \textbf{47} & - & - & \textbf{16} & \textbf{39} & 42 & \textbf{55} & - & - & 13 & 37 & 39 & \textbf{53} & - & - & 13 & 37 & 39 & \textbf{53} & - & - \\
\bottomrule
\end{tabular}
}
\end{table*}


% updated by JJ
Table~\ref{tab:RQ1-GP} presents the evaluation results of our GP model evolved to combine the four SBFL \formulas.
As explained in Section \ref{sub:rq1_effectiveness}, we report only the results of the model with the median fitness among 30 models. 
In contrast to what we expected from combining the four SBFL \formulas using GP, we fail to observe any meaningful improvement compared to the results of Ochiai, the best of the four \formulas: 
the \textit{acc@10} and the median wasted effort improve only marginally, and $R_{wef}$ degrades. 


\begin{table}[t!]
\caption{RQ1: The effectiveness of GP evolved \formulas using Ochiai, Barinel, Tarantula, and DStar. \label{tab:RQ1-GP}\centering}
\centering
\scalebox{0.85}{
\begin{tabular}{l|r|rrrr|rr}
\toprule
\textbf{Project} & \textbf{Total} & \multicolumn{4}{c|}{\textbf{acc}} & \multicolumn{2}{c}{\textbf{wef (R$_{wef}$)}} \\
& & @1 & @3 & @5 & @10 & mean & med \\
\midrule

Hbase & 8 & 1 & 4 & 5 & 5 & 13.12 (16) & 2.5 (5) \\
Ignite & 14 & 0 & 3 & 3 & 5 & 214.93 (21) & 20.0 (4) \\
Pulsar & 10 & 3 & 5 & 6 & 9 & 9.20 (23) & 3.0 (9) \\
Alluxio & 3 & 0 & 0 & 0 & 1 & 101.67 (65) & 86.0 (83) \\
Neo4j & 3 & 1 & 2 & 2 & 2 & 23.33 (43) & 1.0 (18) \\

\midrule
Total & 38 & 5 & 14 & 16 & 22 & 94.24 (26) & 6.5 (8) \\
Percentage (\%) & 100 & \textbf{13} & \textbf{37} & 42 & \textbf{58} & - & - \\
\bottomrule
\end{tabular}
}
\end{table}

To understand these observations, we inspect the intersection between the sets of classes ranked in the top 5 by these four SBFL \formulas. Figure~\ref{fig:sbfl_venn} presents this intersection in a Venn diagram.
Out of 14,16,15,15 flaky classes ranked within the top 5 by Dstar, Ochiai, Tarantula, and Barinel, 13 of them are the same flaky classes. There are two additional classes that are ranked in the top 5 by all except Dstar and one extra class by only Ochiai and Dstar.
Overall, the diagram demonstrates that there are large overlaps between the results of these four SBFL \formulas. 
Thus, we can conclude that the GP-evolved formula did not lead to substantial improvements because there was no space for improvement as all four input \formulas provided similar signals. 
This conclusion brings out the need for introducing external signals from other code and change metrics, which will be discussed in the following research question.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth, trim=40mm 0mm 40mm 30mm, clip]{figures/sherloc/venn_sbfls.pdf}
\caption{Venn-diagram of flaky classes ranked in the top 5 by the four SBFL \formulas.} 
\label{fig:sbfl_venn}
\end{figure}

\begin{tcolorbox}[
    left=2pt,right=2pt,top=2pt,bottom=2pt,
    arc=0pt,
    boxrule=1.2pt
]
\textbf{RQ1:} 
Using SBFL, we were able to localise flaky classes by inspecting only 21-24\% (6-7\%) of classes covered by flaky tests on average (median). With Ochiai, flaky classes are ranked at the top and in the top 10 for 16\% and 55\% of total flaky commits. 
\end{tcolorbox}

\subsection{RQ2: Code and Change Metrics}

Table~\ref{tab:RQ2} shows the evaluation results for GP-evolved models using SBFL scores with change and code metrics.
The table shows that the addition of signals from change and size metrics leads to an improvement in the identification of flaky classes.
In particular, by adding change metrics, the percentage of classes ranked at the top reaches 24\%. 
This percentage is much higher than the maximum percentage achieved with SBFL alone, which is 16\% with Ochiai.
On the contrary, we do not observe any significant improvements in the number of flaky tests ranked in the top 10 or top 5. 
Combined, these results imply that these change and size metrics can give additional signals that break ties between the classes located near the top, allowing developers to identify the exact cause of flakiness more precisely.
The comparison with the results of GP with only SBFL \formulas in Table~\ref{tab:RQ1-GP} further supports the usefulness of change and size metrics.
Specifically, by adding change and size metrics, the percentage of flaky classes ranked at the top ($acc@1$) goes from 13\% to 24\% and 18\%, respectively. In addition, average $R_{wef}$ improves 5\% with change metrics and 4\% with size metrics. 

With regard to flakiness metrics, their combination with SBFL scores does not lead to any notable improvements in the ranking of classes at the top. 
The percentage of classes at the top is 11\% and the percentage of classes in the top 10 is 53\%.
One possible explanation for this is that our flakiness metrics are derived from a flakiness taxonomy that focuses on the test instead of the CUT.
Hence, using metrics derived from such categories may not be helpful in the identification of CUT components that are responsible for flakiness.
To alleviate this, future studies should consider categories and metrics that are derived from the CUT, and existing flakiness taxonomies should be updated accordingly.

\begin{table*}[ht]
\caption{RQ2: The contribution of flakiness, change, and size metrics to the identification of flaky classes. 
\label{tab:RQ2}\centering}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{c|rrrr|rr|rrrr|rr|rrrr|rr}
\toprule
& \multicolumn{6}{c|}{\textbf{SBFL \& flakiness}} & \multicolumn{6}{c|}{\textbf{SBFL \& change}} & \multicolumn{6}{c}{\textbf{SBFL \& size}}  \\
\textbf{Proj. (\#)} & \multicolumn{4}{c}{acc} & \multicolumn{2}{c|}{wef (R$_{wef}$)} & \multicolumn{4}{c}{acc} & \multicolumn{2}{c|}{wef (R$_{wef}$)} & \multicolumn{4}{c}{acc} & \multicolumn{2}{c}{wef (R$_{wef}$)}  \\
& @1 & @3 & @5 & @10 & mean & med & @1 & @3 & @5 & @10 & mean & med & @1 & @3 & @5 & @10 & mean & med \\
\midrule
%

Hbase (8) & 1 & 4 & 5 & 5 & 11.9 (12) & 3 (4) & 2 & 4 & 4 & 5 & 16.9 (13) & 4 (4) & 2 & 4 & 5 & 5 & 11.4 (12) & 3 (3) \\
Ignite (14) & 0 & 2 & 2 & 4 & 230.9 (26) & 63 (4) & 2 & 4 & 4 & 4 & 222.3 (24) & 18 (4) & 1 & 3 & 3 & 5 & 220.1 (24) & 43 (4) \\
Pulsar (10) & 2 & 5 & 6 & 8 & 10.2 (15) & 3 (8) & 3 & 5 & 7 & 9 & 8.0 (12) & 2 (5) & 2 & 5 & 7 & 9 & 6.9 (13) & 2 (6)  \\
Alluxio (3) & 0 & 0 & 1 & 1 & 97.7 (51) & 73 (65) & 0 & 0 & 1 & 1 & 75.7 (49) & 94 (39) & 0 & 0 & 1 & 1 & 90.7 (49) & 77 (58) \\
Neo4j (3) & 1 & 2 & 2 & 2 & 19.3 (42) & 1 (18) & 2 & 2 & 2 & 2 & 6.7 (37) & 0 (9) & 2 & 2 & 2 & 2 & 23.0 (40) & 0 (10) \\

\midrule
Total  (38) & 4 & 13 & 16 & 20 & 99.5 (24) & 8 (8) & 9 & 15 & 18 & 21 & 94.1 (21) & 5 (6) & 7 & 14 & 18 & 22 & 94.3 (22) & 5 (7) \\
Percentage (\%) & \textbf{11} & 34 & 42 & \textbf{53} & - & - & \textbf{24} & 39 & 47 & 55 & - & - & \textbf{18} & 37 & 47 & 58 & - & - \\

\bottomrule
\end{tabular}}
\end{table*}

To further investigate the impact of change and size metrics on the identification performance, we analyse the involvement of each metric in our GP-evolved \formulas. 
Table~\ref{tab:RQ2-distr} shows the frequency of change and size metrics in the GP evolved \formulas generated under the configuration of using SBFL and change metrics (\ie SBFL \& Change) and the configuration of using SBFL and size metrics (\ie SBFL \& Size). 
As shown in this table, both change and size metrics are frequently involved in the final \formulas, confirming that our observed improvement did not come only from using GP.
Based on these results, we posit that change and size metrics can contribute positively to the identification of flaky classes.

\begin{table}[ht]
\caption{Frequency of metrics in GP-evolved \formulas (from 0 to 1). `Changes' and `Dev' denote \textit{`Unique Changes'} and \textit{`Developers'}, respectively. The column `SBFL' contains the average frequency of the four SBFL metrics. 
\label{tab:RQ2-distr}}
\centering
\scalebox{0.85}{
\begin{tabular}{l|c|ccc|ccc}
\toprule
 & \textbf{SBFL} & \textbf{Changes} & \textbf{Dev} & \textbf{Age} & \textbf{LOC} & \textbf{DOI} & \textbf{CC} \\
\midrule
SBFL \& Change & 0.45 & 0.50 & 0.37 & 0.53  & - & - & - \\
SBFL \& Size & 0.50 & - & - & - & 0.71 & 0.37 & 0.73 \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{tcolorbox}[
    left=2pt,right=2pt,top=2pt,bottom=2pt,
    arc=0pt,
    boxrule=1.2pt
]
\textbf{RQ2:} 
The augmentation of Spectrum-Based Fault Localisation with change or size metrics lets more flaky classes be ranked near the top; by adding change metrics, we can rank 24\% flaky classes at the top. In contrast, metrics specific to flakiness categories do not provide any beneficial signals to the identification approach. 
\end{tcolorbox}

\subsection{RQ3: Ensemble Method}

Table~\ref{tab:RQ3-voting} presents the evaluation results for the voting method with 60 GP-evolved models, half from using SBFL and change metrics and the other half from using SBFL and size metrics.
We decided to exclude the models that build on flakiness metrics since their usage did not improve the performance any further.
As explained in Section~\ref{sec:rq3_ensemble_method}, there can be a case where none of the participating models succeeds to vote for the true candidate since individual models vote only for those ranked within the top n. For this case, we report the median of all rankings of the models as an alternative. 

The results show that the voting step further improves the ranking results.
The most notable improvement is the accuracy at the top 3, which reaches 47\%. Although the improvements in the other accuracy metrics are not as noticeable as what we have seen in the accuracy at the top 3, there are constant improvements over the results without voting. 
The average of wasted effort remains almost the same while the median improves from the voting, dropping to 3.5. These results imply that the voting allows those near the top to shift further to higher ranks based on the agreement among the models that exploit and capture different features of flaky classes.
Nonetheless, the constant improvements in $R_{we}$, both per project and in total, suggest that through the voting, we can rank flaky classes further near to the top; for example, in Alluxio, where $R_{wef}$ is always near 50, average $R_{wef}$ reduces to 22 and its median to 10. 
These results imply that voting can leverage the complementarity between different models, further improving the localisation of flakiness.

\begin{table}[ht]
\caption{RQ3: The effectiveness of the voting between 60 different GP-evolved models, 30 from SBFL with change metrics, and 30 from using SBFL with size metrics. `Perc' denotes Percentage \label{tab:RQ3-voting}\centering}
\centering
\scalebox{0.9}{
\begin{tabular}{l|r|rrrr|rr}
\toprule
\textbf{Project} & \textbf{Total} & \multicolumn{4}{c|}{\textbf{acc}} & \multicolumn{2}{c}{\textbf{wef (R$_{wef}$)}} \\
& & @1 & @3 & @5 & @10 & mean & med \\
\midrule

Hbase & 8 & 3 & 5 & 6 & 6 & 9.62 (12) & 1.5 (2) \\
Ignite & 14 & 2 & 4 & 4 & 4 & 228.61 (24) & 17.5 (4) \\
Pulsar & 10 & 3 & 6 & 7 & 9 & 7.30 (12) & 2.0 (5) \\
Alluxio & 3 & 1 & 1 & 1 & 2 & 61.83 (22) & 9.0 (10) \\
Neo4j & 3 & 1 & 2 & 2 & 2 & 19.67 (42) & 1.0 (18) \\

\midrule
Total & 38 & 10 & 18 & 20 & 23 & 94.61 (19) & 3.5 (5) \\
Perc (\%) & 100 & 26 & \textbf{47} & 53 & 61 & - & - \\

\bottomrule
\end{tabular}
}
\end{table}

\begin{tcolorbox}[
    left=2pt,right=2pt,top=2pt,bottom=2pt,
    arc=0pt,
    boxrule=1.2pt
]
\textbf{RQ3:} 
A voting between models based on SBFL, change, and size metrics, provides the best ranking for flaky classes.
47\% of flaky classes are ranked in the top 3 and 26\% of them are ranked at the top. The average $R_{wef}$ further reduces to 19, highlighting the practical usefulness of our approach.  
\end{tcolorbox}

\subsection{RQ4: Flakiness Categories}
Table~\ref{tab:RQ4} presents the performances of the voting method on the different flakiness categories encountered in our dataset.
The \textit{``Ambiguous"} category represents cases where the flaky tests could not be assigned to any of the known flakiness categories.
First, we observe that the most common categories are Concurrency and Asynchronous Waits.
This aligns with observations from previous studies~\cite{Luo2014,Lam2020a,Eck2019} and confirms that the taxonomy adopted for our metrics is adequate for our distribution.
Furthermore, we observe a discrepancy between the performances in different categories.
Classes responsible for Async Waits are well identified with 80\% of the classes in the top 10, and 30\% of them at the top.
Classes responsible for Concurrency also show good performances with 50\% of them in the top 10, and 38\% of them at the top; the average $R_{wef}$ is below ten, eight precisely, meaning we can locate flaky classes by inspecting less than 10\% of the total number of the classes covered by flaky tests. 

Categories such as Time and I/O show much lower performances, with 33\% and 0\% of flaky classes in the top 10, respectively.
Nevertheless, given the low number of instances for these categories, it is hard to discuss or generalise their results.
With only two instances, the category Unordered Collections shows curious results as one class is ranked second and the other one ranked 663. 
To understand the reasons behind the bad ranking, we manually inspected this case\footnote{\url{https://github.com/apache/ignite/commit/188e4d52c2}}.  
We found that the concerned test, \texttt{testUnstableTopology}, was executed twice due to a retry mechanism. Both executions led to failure, but interestingly, we found that the two failures have different causes. One of them is due to a lack of context initialisation and is likely to be the reason behind flakiness. 
As the two failure causes are different, the coverage is also different in them.
Specifically, one of the failures  did not cover the flaky class, and as the coverage of this failure was leveraged in the SBFL, the flaky class was not considered suspicious.
We discuss other reasons responsible for poor ranking in Section~\ref{sec:sherloc-discussion}.

\begin{table}[ht]
\caption{RQ4: The effectiveness per flakiness category \label{tab:RQ4}\centering}
\centering
\scalebox{0.8}{
\resizebox{\textwidth}{!}{\begin{tabular}{l|rrrr|rr}
\toprule
\textbf{Flakiness} & \multicolumn{4}{c|}{\textbf{acc}} & \multicolumn{2}{c}{\textbf{wef (R$_{wef}$)}} \\
\textbf{Category} & @1 & @3 & @5 & @10 & mean & med \\
\midrule

Concurrency (16) & 6 (\textbf{38}) & 7 (44) & 7(44) & 8 (\textbf{50})& 147.53 (27) & 9.5 (9) \\
Async wait (10) & 3 (\textbf{30}) & 6 (60) & 8 (80) & 8 (\textbf{80})& 21.05 (8) & 1.5 (3) \\ 

Ambiguous (4) & 1 (25) & 2 (50) & 2 (50) & 3 (75)& 18.88 (5) & 3.5 (5) \\
Time (3) & 0 (0) & 0 (0) & 0 (0) & 1 (\textbf{33})& 88.33 (16) & 14.0 (10) \\
Network (2) & 0 (0) & 2 (100) & 2 (100) & 2 (100)& 1.00 (10) & 1.0 (10) \\
Unordered  &  &  &  &  & &  \\
collections (2) & 0 (0) & 1 (50) & 1(50) & 1 (50) & 331.5 (33) & 331.5 (33) \\
I/O (1) & 0 (0) & 0 (0) & 0(0) & 0 (\textbf{0})& 12.50 (3) & 12.5 (3) \\
Random (1) & 0 (0) & 1 (100) & 1 (100) & 1 (100)& 2.00 (75) & 2.0 (75) \\

\midrule
Total (39\tablefootnote{One flaky class belongs to two categories: Network and Unordered Collections.}) & 10 & 18 & 20 & 23 & 94.47 (19) & 3.5 (5) \\
Perc (\%) & 26 & 47 & 53 & 61 & - & - \\

\bottomrule
\end{tabular}}
}
\end{table}

\begin{tcolorbox}[
    left=2pt,right=2pt,top=2pt,bottom=2pt,
    arc=0pt,
    boxrule=1.2pt
]
\textbf{RQ4:} 
The most prominent flakiness categories, Concurrency and Asynchronous Waits, are identified effectively, with 38\%
and 30\% of their flaky classes ranked at the top, respectively.
In the Concurrency category, flaky classes are identified by examining 8\% of classes covered by flaky tests on average. 
\end{tcolorbox}