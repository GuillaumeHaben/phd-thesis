\section{Conclusion}

The goal of this chapter is to shed light on the smell occurring in System User Interactive Tests. To do so, we have defined a catalogue of SUIT smells based on the knowledge providing from both practitioners and and academia through a multivocal literature review. This process lead to a catalog of 35 SUIT smells. For 16 of these smells we derive metrics to characterize the presence of symptoms and their refactoring in the test code. Our results show that three symptoms \emph{Hardcoded Values} (90\% of the tests), \emph{Over Checking} (between 75\% and 80\% of the tests), and \emph{Sneaky Checking} (70\% of the tests) are prevalent in both industrial and open-source projects. Furthermore, \emph{Hardcoded Values} not only appear in most tests but when they are present tend to be in a large quantity with median values ranging between 50 and 70 occurrences per test. While these symptoms are largely present in the test code base, their refactoring on the other hand remain low where for half of the categories less than 5\% of the symptomatic tests are ever subjects to refactoring. \emph{Missing assertion} is a unique exception with between 70\% (open-source) and 90\% (industrial) of the symptomatic tests being refactored. Interestingly, while refactoring actions are rare, test smells like \emph{Narcissistic} and \emph{Middle Man} still disappear from the test code base as a side effect of the replacement of symptomatic tests by new tests not exhibiting the symptom.

Though we observe general trends common to both industrial and open-source projects, when performing a more rigorous comparison between the two sets of projects, we observe significant differences. Indeed, both in terms of diffusion of the symptoms and refactoring operations both types of projects differ. This results can be explain by the difference in scope, actors and lifecycle present in each context. Consequently, these results suggest that when conducting studies researchers should be aware of these fundamental differences which might limit the generalization of their results.

In light of the results from this exploratory analysis, we believe that extending the catalog of known SUIT smells will have an impact on our partner development process which already started using our tooling to address some of the bad designs they observe in their test code base. Moreover, with our new catalogue and these first observations we open perspective for future research on awareness of bad testing practices and the pitfalls to avoid when evolving test code. 