\section{Threats to Validity}

Threat to construct validity result from the non suitability of the metrics used to evaluate the results. To detect test smells we rely on heuristics based on code metrics. While we focus on metric that offer good precision, we cannot ignore the fact that some of the smells are not detected by the metrics devised in this work.

Threats to the internal validity are due to the design of the study, potentially impacting our conclusions. Such threats typically do not affect exploratory studies like the one in this work. A caveat can be raised on how the changes are extracted. Indeed, changes are recorded when developers check-in their changes to the control version system. Thus, refactoring actions might be lost if developers do not check-in often their changes or on the contrary, we might flag artifacts of the development process (\emph{e.g.} Assertions added only at the end of the test) as refactoring actions. To account for this phenomenon, we analyze manually a subset of the results to ensure the soundness of the process.

Finally, the threats to external validity, regarding the generalization of the results, concerns mainly the choice of the projects analyzed. Indeed, conducting our analysis on a limited sample of projects, our results might not generalize to other projects. However, we try to control for this limitation by selecting projects of different sizes, from different domains and in different development cultures. Furthermore, working with Robot Framework, there is no guarantee that the results presented in this work are transferable to other languages or technologies.