\section{Introduction}
\label{sec:survey-introduction}

Software Testing is critical for modern software development as it allows the concurrent implementation and integration of features.
At Google, more than 50 million test cases are executed every day to ensure the quality of their products~\cite{Welcomet41:online}.
Though, test automation faces major problems with the emergence of  flaky tests
~\cite{FlakinessGoogle,Mozilla,FlakinessSpotify}. Flaky tests are tests that, for the same versions of code and test, can pass and fail on different runs.
Such non-determinism sends confusing signals to developers who struggle to interpret the test results.
As a result, developers lose trust in test suites, disregard their signals and integrate features containing real failures, thereby nullifying the purpose of testing.

Flaky tests are prevalent in large software systems and they incur significant costs.
Google reports indicate that 16\% of their tests exhibit some flakiness whereas 84\% of the transitions from pass to fail involve a flaky test~\cite{FlakinessGoogle}.

In response to this challenge, researchers dedicate their efforts in understanding the nature of flaky tests and the way they manifest.
Empirical studies examined the root causes of flaky tests in open-source software~\cite{Luo2014,Eck2019,Thorve2018,Dutta2020} and industrial systems~\cite{Lam2019RootCausing}, showing that concurrency and order-dependency are among the main categories of test flakiness. 
Notably, the study of Eck~\etal~\cite{Eck2019} showed that flakiness can stem from the code under test and highlighted its potential impact on organisational aspects like resource allocation.

Other studies investigated tools and techniques that could help developers to cope with test flakiness.
Automated tools, such as DeFlaker~\cite{Bell2018}, iDFlakies~\cite{Lam2019iDFlakies}, and FlakeFlagger~\cite{FlakeFlagger} have been developed in order to detect flaky tests with a minimum number of test runs or re-runs. Unfortunately, these advances offer only partial solutions to the problem and may not fit well within the development systems and organisation constraints. For instance, DeFlaker relies on coverage and reruns of tests that do not execute changed code, which are not possible in specific development environments that use regression test selection or when coverage cannot be obtained.     
Furthermore, the fixing of flaky tests gained traction as studies investigated the fixing effort devoted for flaky tests and tools like \cite{Shi2019iFix} are designed to fix flaky tests.
Nonetheless, in order to devise flakiness solutions, we need to understand how developers deal with flaky tests in practice.
In particular, it is necessary to identify the typical measures taken by practitioners when dealing with flaky tests, and reflect on how research solutions could assist and improve them.

To shed some light on these questions, we conduct an empirical study focused on the industrial context in which flakiness manifests. 
Specifically, we perform a qualitative analysis on data collected from 14 practitioner interviews to answer the following research questions:


\begin{itemize}[label={}]
\item \textbf{\textsc{RQ1:}} \emph{Where can we locate flakiness?}\\
    \textbf{Goal:} Differently from previous studies~\cite{Luo2014,Eck2019,Thorve2018,Dutta2020,Lam2019RootCausing}, which focused on the root causes of flakiness, \eg concurrency and timeouts, we aim to identify where flakiness stems within the different components of the development ecosystem, \eg test, code under test, and infrastructure. This localisation is necessary to guide both detection and fixing approaches.\\
    \textbf{Results:} In addition to tests, flakiness stems from the poor orchestration between the system components, the testing infrastructure, and external factors, \eg OS and firmware. Studies should consider and leverage these factors when addressing flaky tests and not focus solely on the test and code under test.
    
\item \textbf{\textsc{RQ2:}} \emph{How do practitioners perceive the impact of flakiness?}\\
    \textbf{Goal:} This question is commonly discussed in industrial reports and research studies. In this paper, 
    we examine it through direct discussions with practitioners. The aim is to understand the impact of flakiness on the development workflow and practices. \\
    \textbf{Results:} Besides dissipating development time and hindering the continuous integration (CI), flakiness affects the testing practices and leads to a degradation of the system quality. We also shed light on the pernicious consequences of system flakiness, \ie buggy or non-deterministic features that are falsely labelled as flaky tests.
    
\item \textbf{\textsc{RQ3:}} \emph{ How do practitioners address flaky tests?}\\
    \textbf{Goal:} This question aims at identifying and understanding the measures taken by practitioners to address flakiness before and after it manifests in the CI.\\
    \textbf{Results:}
    The prevention of test flakiness is performed by building stable infrastructures and enforcing guidelines, whereas the detection still relies mainly on reruns and manual inspection. Our results also highlight monitoring and logging tasks, which are commonly dismissed in research, yet they are key to most of the mitigation measures taken by practitioners.
    
\item \textbf{\textsc{RQ4:}} \emph{How could mitigation measures be improved with automation tools?}\\
    \textbf{Goal:} This question aims to identify specific needs to be addressed by future research. \\
    \textbf{Results:} We accentuate the need for techniques that monitor and analyse the system states to assist the prediction, debugging, and fine-grained evaluation of flaky tests.
    Our participants also expressed the need for automating the quality assessment of software tests through static analysis and variability-aware reruns, \ie reruns under diverse system configurations. 
\end{itemize}

We believe that the qualitative results of this study are necessary to complement the current understanding of flakiness and advise future work.  
